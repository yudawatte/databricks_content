{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c446c157-0fb1-4eda-b4ea-116a287a8cc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Python interpreter will be restarted.\n",
       "Python interpreter will be restarted.\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Python interpreter will be restarted.\nPython interpreter will be restarted.\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pip install /dbfs/FileStore/python-wheels/dev/phgml-1.4.0-py3-none-any.whl -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cb391e2-6ce0-4381-b9bc-5018209a129f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.widgets.dropdown(\"save_model\", \"True\", [\"True\", \"False\"], \"Save Model\")\n",
    "dbutils.widgets.text(\"model_tags\",\"model_stage:CA3_develop\", \"Model Tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "837838a9-f969-4daf-a5fb-e421b99eebec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List\n",
    "import re\n",
    "import logging\n",
    "from mlflow import MlflowException\n",
    "from mlflow.client import MlflowClient\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e65a84f-344f-4c56-81b1-14effce69228",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def str_to_bool(value):\n",
    "  FALSE_VALUES = ['false', 'no', '0']\n",
    "  TRUE_VALUES = ['true', 'yes', '1']\n",
    "  lvalue = str(value).lower()\n",
    "  if lvalue in (FALSE_VALUES): return False\n",
    "  if lvalue in (TRUE_VALUES):  return True\n",
    "  raise Exception(\"String value should be one of {}, but got '{}'.\".format(FALSE_VALUES + TRUE_VALUES, value))\n",
    "\n",
    "def extract_param_values(value: str)-> List[str]:\n",
    "    \"\"\"\n",
    "    The function takes comma seperated strings and return list of strings\n",
    "\n",
    "    input params:\n",
    "        value (str) : comma seperated strings\n",
    "\n",
    "    output:\n",
    "        (List) : list of strings\n",
    "    \"\"\"\n",
    "    if value == \"\":\n",
    "        return []\n",
    "    elif \",\" in value:\n",
    "        val_lst = value.split(\",\")\n",
    "        return val_lst\n",
    "    else:\n",
    "        return [value]\n",
    "    \n",
    "def get_model_tags(model_tags_str):\n",
    "    ''' A Validation for the model tag text through databricks utility'''\n",
    "    valid_pattern = r'(\\w+:\\w+)'\n",
    "    invalid_pattern = r'[^:,\\w\\s]'\n",
    "    str_cpy=model_tags_str.replace(\" \", '')\n",
    "\n",
    "    not_allowed_symbols = re.findall(pattern=invalid_pattern, string=str_cpy)\n",
    "    if len(not_allowed_symbols)>0:\n",
    "        raise ValueError('''Unwanted characters detected. Allowed characters are colon(:), comma(,), word characters and white space characters\n",
    "                            Please specify key values pairs as key1:value1,key2:value2\n",
    "                        ''')\n",
    "    else:\n",
    "        matching_pairs = re.findall(pattern=valid_pattern, string=str_cpy)\n",
    "\n",
    "        for matching_str in matching_pairs:\n",
    "            str_cpy = str_cpy.replace(matching_str,'')\n",
    "        str_cpy = str_cpy.replace(',','')\n",
    "        \n",
    "        if len(str_cpy)>0:\n",
    "            raise ValueError('''unmatched string components detected. please check the specified string\n",
    "                             Please specify key values pairs as key1:value1,key2:value2\n",
    "                             ''')\n",
    "        \n",
    "    return {key:value for key,value in map(lambda x: x.split(':'),matching_pairs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b9f0326-4d28-481f-a273-3380e440123e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "params = {'WITHOUT_PMS': False,\n",
    " 'SELECTED_HOTELS': ['63662','26834','1406','10443','71999','64942','26532','55810','56217','76630','27226','79051','79908','36063','27275'], \n",
    " #,'26834','1406','10443','71999','64942','26532','55810'\n",
    " 'ENV': 'dev',\n",
    " 'CACHE_MODELS': False,\n",
    " 'TARGET_TYPE': 'REVENUE',\n",
    " 'MODEL_TYPE': 'FARFIELD',\n",
    " 'REPOPATH': '/Workspace/Repos/manik@surge.global/phg-data-mlsys/src',\n",
    " 'FORECAST_POINTS': [91, 84, 77, 70, 63, 56, 49, 42, 35],\n",
    " 'CA_AWARE': True,\n",
    " 'IS_USD_CURRENCY': True,\n",
    " 'DAYS_AHEAD': 7,\n",
    " 'MAX_FORECAST_POINT': 91,\n",
    " 'MIN_FORECAST_POINT': 35,\n",
    " 'MAX_TARGET_LEAD': 100,\n",
    " 'MAX_LEAD': 151,\n",
    " 'LAG_NUMBERS': [7,14,21,28,35,42,49,56,63,70,77,84,91,98,105,112,119,126,133,140,147],\n",
    " 'MODEL_START_DATE': pd.to_datetime('2018-10-01 00:00:00'),\n",
    " 'COVID_START_DATE': pd.to_datetime('2020-03-01 00:00:00'),\n",
    " 'COVID_END_DATE': pd.to_datetime('2021-08-01 00:00:00'),\n",
    " 'REVENUE_COL': '_reservationRevenuePerRoomUSD',\n",
    " 'ROOMS_COL': '_rooms',\n",
    " 'TARGET_COLUMN': '_reservationRevenuePerRoomUSD',\n",
    " 'PREPROCESSED_TABLE': 'testing_data.pp_ff_preprocess_rv',\n",
    " 'PARTITION_DATE': datetime(2024, 2, 1, 3, 59, 6),\n",
    " 'CORRECTED_HOTEL_IDS': ['63662','26834','1406','10443','71999','64942','26532','55810','56217','76630','27226','79051','79908','36063','27275']} \n",
    " # ,'10443','71999','64942','26532','55810'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf29c8fb-e132-4a46-9140-38270119b34d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# params = {}\n",
    "# params['WITHOUT_PMS'] = dbutils.jobs.taskValues.get(taskKey = \"capture_params\", key = \"WITHOUT_PMS\")\n",
    "# params['SELECTED_HOTELS'] = dbutils.jobs.taskValues.get(taskKey = \"capture_params\", key = \"SELECTED_HOTELS\")\n",
    "# params['ENV'] = dbutils.jobs.taskValues.get(taskKey = \"capture_params\", key = \"ENV\")\n",
    "# params['CACHE_MODELS'] = dbutils.jobs.taskValues.get(taskKey = \"capture_params\", key = \"CACHE_MODELS\")\n",
    "# params['TARGET_TYPE'] = dbutils.jobs.taskValues.get(taskKey = \"capture_params\", key = \"TARGET_TYPE\")\n",
    "# params['MODEL_TYPE'] = dbutils.jobs.taskValues.get(taskKey = \"capture_params\", key = \"MODEL_TYPE\")\n",
    "# params['REPOPATH'] = dbutils.jobs.taskValues.get(taskKey = \"capture_params\", key = \"REPOPATH\")\n",
    "# params[\"FORECAST_POINTS\"] = dbutils.jobs.taskValues.get(taskKey = \"dev-ff-forecast-data-processing\", key = \"FORECAST_POINTS\")\n",
    "# params['CA_AWARE'] = dbutils.jobs.taskValues.get(taskKey = \"dev-ff-forecast-data-processing\", key = \"CA_AWARE\")\n",
    "# params['IS_USD_CURRENCY'] = dbutils.jobs.taskValues.get(taskKey = \"dev-ff-forecast-data-processing\", key = \"IS_USD_CURRENCY\")\n",
    "# params['DAYS_AHEAD'] = dbutils.jobs.taskValues.get(taskKey = \"dev-ff-forecast-data-processing\", key = \"DAYS_AHEAD\")\n",
    "# params['MAX_FORECAST_POINT'] = dbutils.jobs.taskValues.get(taskKey = \"dev-ff-forecast-data-processing\", key = \"MAX_FORECAST_POINT\")\n",
    "# params['MIN_FORECAST_POINT'] = dbutils.jobs.taskValues.get(taskKey = \"dev-ff-forecast-data-processing\", key = \"MIN_FORECAST_POINT\")\n",
    "# params['MAX_TARGET_LEAD'] = dbutils.jobs.taskValues.get(taskKey = \"dev-ff-forecast-data-processing\", key = \"MAX_TARGET_LEAD\")\n",
    "# params['MAX_LEAD'] = dbutils.jobs.taskValues.get(taskKey = \"dev-ff-forecast-data-processing\", key = \"MAX_LEAD\")\n",
    "# params['LAG_NUMBERS'] = dbutils.jobs.taskValues.get(taskKey = \"dev-ff-forecast-data-processing\", key = \"LAG_NUMBERS\")\n",
    "# params['MODEL_START_DATE'] = pd.to_datetime(dbutils.jobs.taskValues.get(taskKey = \"dev-ff-forecast-data-processing\", key = \"MODEL_START_DATE\"))\n",
    "# params['COVID_START_DATE'] = pd.to_datetime(dbutils.jobs.taskValues.get(taskKey = \"dev-ff-forecast-data-processing\", key = \"COVID_START_DATE\"))\n",
    "# params['COVID_END_DATE'] = pd.to_datetime(dbutils.jobs.taskValues.get(taskKey = \"dev-ff-forecast-data-processing\", key = \"COVID_END_DATE\"))\n",
    "# params['REVENUE_COL'] = dbutils.jobs.taskValues.get(taskKey = \"dev-ff-forecast-data-processing\", key = \"REVENUE_COL\")\n",
    "# params['ROOMS_COL'] = dbutils.jobs.taskValues.get(taskKey = \"dev-ff-forecast-data-processing\", key = \"ROOMS_COL\")\n",
    "# params['TARGET_COLUMN'] = dbutils.jobs.taskValues.get(taskKey = \"dev-ff-forecast-data-processing\", key = \"TARGET_COLUMN\")\n",
    "# params['PREPROCESSED_TABLE'] = dbutils.jobs.taskValues.get(taskKey = \"dev-ff-forecast-data-processing\", key = \"PREPROCESSED_TABLE\")\n",
    "# params['PARTITION_DATE'] = pd.to_datetime(dbutils.jobs.taskValues.get(taskKey = \"dev-ff-forecast-data-processing\", key = \"PARTITION_DATE\"))\n",
    "# params['CORRECTED_HOTEL_IDS'] = dbutils.jobs.taskValues.get(taskKey = \"dev-ff-forecast-data-processing\", key = \"CORRECTED_HOTEL_IDS\")\n",
    "\n",
    "params[\"ML_EXPERIMENT_ID\"] = 609933091443417\n",
    "params[\"CALC_UNCERTAINTY\"] = True\n",
    "params[\"SAVE_MODEL\"] = False\n",
    "params[\"SAVE_METRICS\"] = False\n",
    "params[\"MODEL_TAGS_DCT\"] = get_model_tags(getArgument(\"model_tags\"))\n",
    "params[\"MAX_FORECAST_POINT\"] = max(params[\"FORECAST_POINTS\"]) \n",
    "params[\"MIN_FORECAST_POINT\"] = min(params[\"FORECAST_POINTS\"]) \n",
    "params[\"MODEL_TAGS_DCT\"][\"last_updated_date\"] = str(datetime.now())\n",
    "params[\"MODEL_TAGS_DCT\"][\"days_ahead\"] = params[\"DAYS_AHEAD\"]\n",
    "params[\"MODEL_TAGS_DCT\"][\"max_forecast_point\"] = params[\"MAX_FORECAST_POINT\"]\n",
    "params[\"MODEL_TAGS_DCT\"][\"min_forecast_point\"] = params[\"MIN_FORECAST_POINT\"]\n",
    "params[\"MODEL_NAME_PREFIX\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d940cdb-65cf-4697-b409-7bfe2cb18efc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# if params[\"ENV\"] == \"dev\":\n",
    "#     print(f\"Loading phgml package from repo {params['REPOPATH']}\")\n",
    "#     sys.path.append(os.path.abspath(params[\"REPOPATH\"]))\n",
    "\n",
    "# sys.path.append(os.path.abspath(\"/Workspace/Repos/yasith.udawatte@henrymwuamica.onmicrosoft.com/phg-data-mlsys\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f679631-a740-4a24-9ffc-bc1bd14ddceb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from phgml.data.config import FarfieldForecastingHotelConfigProvider,FarfieldEnvironmentConfig\n",
    "from phgml.data.data_types import (\n",
    "    training_output_schema,\n",
    ")\n",
    "# from phgml.pipeline.training import train_wrapper_farfield\n",
    "from phgml.reporting.logging import get_dbx_logger\n",
    "schema = training_output_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd378dc0-c29e-4148-8741-1fc4a20c362b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env_config = FarfieldEnvironmentConfig(\n",
    "    env=params[\"ENV\"], \n",
    "    without_pms=params[\"WITHOUT_PMS\"], \n",
    "    target=params[\"TARGET_TYPE\"],\n",
    "    spark=spark,\n",
    "    is_usd_currency=params[\"IS_USD_CURRENCY\"]\n",
    ")\n",
    "forecasting_config_provider = FarfieldForecastingHotelConfigProvider(spark=spark,env=params[\"ENV\"])\n",
    "params[\"TARGET_COLUMN\"] = env_config.target_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6355528b-533b-4fef-a474-17d5e248d91a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">24/09/12/ 16:10:24 UTC:PHGML-dev-REVENUE-PMS-INFO-Processing data for target type: REVENUE : _reservationRevenuePerRoomUSD\n",
       "24/09/12/ 16:10:24 UTC:PHGML-dev-REVENUE-PMS-INFO-Excluding PMS data? False\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">24/09/12/ 16:10:24 UTC:PHGML-dev-REVENUE-PMS-INFO-Processing data for target type: REVENUE : _reservationRevenuePerRoomUSD\n24/09/12/ 16:10:24 UTC:PHGML-dev-REVENUE-PMS-INFO-Excluding PMS data? False\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger = get_dbx_logger(pipeline=params[\"ENV\"],\n",
    "                        task_type=params[\"TARGET_TYPE\"],\n",
    "                        exclude_pms=params[\"WITHOUT_PMS\"])\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "logger.info(f\"Processing data for target type: {params['TARGET_TYPE']} : {params['TARGET_COLUMN']}\")\n",
    "logger.info(f\"Excluding PMS data? {params['WITHOUT_PMS']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8309eae9-04cb-4c6b-a421-9deb4852afae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">24/09/12/ 16:10:24 UTC:PHGML-dev-REVENUE-PMS-INFO-Loading data from testing_data.pp_ff_preprocess_rv\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">24/09/12/ 16:10:24 UTC:PHGML-dev-REVENUE-PMS-INFO-Loading data from testing_data.pp_ff_preprocess_rv\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger.info(f\"Loading data from {params['PREPROCESSED_TABLE']}\")\n",
    "df = spark.sql(f\"select * from {params['PREPROCESSED_TABLE']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9110ce41-ca5d-4e94-b259-bbcf2c64c0d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if df.count() <= 0:\n",
    "    logger.error(\"The loaded training dataset is empty.\")\n",
    "    logger.info(\"Terminting the pipeline execution\")\n",
    "    raise Exception(\"The loaded training dataset is empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cfe8dd2-311d-4870-9d20-0c0babfce0cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sys import version_info\n",
    "import cloudpickle\n",
    "from phgml.models.model_strategy import BaseStrategy\n",
    "from phgml.models.base_model import BaseModel\n",
    "from typing import Optional, Tuple, Union, List, Dict, Any, Callable\n",
    "import mlflow\n",
    "import numpy.typing as npt\n",
    "# from phgml.models.model_strategy import StrategyLGBM, StrategyAG, StrategyLGBMFarField\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n",
    "from phgml.reporting.output_metrics import (\n",
    "    log_metrics_stays,\n",
    "    report_metrics_stays,\n",
    "    log_metrics_stays_farfield,\n",
    "    report_metrics_stays_farfield,\n",
    ")\n",
    "\n",
    "PYTHON_VERSION = \"{major}.{minor}.{micro}\".format(\n",
    "    major=version_info.major, minor=version_info.minor, micro=version_info.micro\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "48f7be51-cc66-4d2c-ae8d-96323c569709",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from abc import ABC, abstractmethod\n",
    "import numpy.typing as npt\n",
    "from typing import Optional, Tuple, Union, List, Dict, Any, Callable\n",
    "\n",
    "class BaseStrategy(ABC):\n",
    "    model_type = \"base_strategy\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        day_ahead: int,\n",
    "        quantile_levels: list,\n",
    "        cd_axis_targets: list,\n",
    "        path: str,\n",
    "        is_auto_reg: bool,\n",
    "        # verbose: int,\n",
    "    ):\n",
    "        self.quantile_levels = quantile_levels\n",
    "        self.day_ahead = day_ahead\n",
    "        self.cd_axis_targets = cd_axis_targets\n",
    "        self.target_prefix = self.cd_axis_targets[0][:2]\n",
    "        self.path = path\n",
    "        self.autoregressive_predictions = is_auto_reg\n",
    "        # self.verbose = verbose\n",
    "\n",
    "        self.objective = \"quantile\"\n",
    "        self.sub_predictors: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "    @abstractmethod\n",
    "    def _fit(self, train_data: pd.DataFrame) -> None:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _predict(self, test_data: pd.Series) -> Dict[float, List[npt.NDArray]]:\n",
    "        pass\n",
    "\n",
    "\n",
    "class StrategyLGBMFarField(BaseStrategy):\n",
    "    model_type = \"LGBM_FARFIELD\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        day_ahead: int,\n",
    "        quantile_levels: list,\n",
    "        cd_axis_targets: list,\n",
    "        path: str,\n",
    "        is_auto_reg: bool,\n",
    "        # verbose: int = -1,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            day_ahead=day_ahead,\n",
    "            quantile_levels=quantile_levels,\n",
    "            cd_axis_targets=cd_axis_targets,\n",
    "            path=path,\n",
    "            is_auto_reg=is_auto_reg,\n",
    "            # verbose=verbose,\n",
    "        )\n",
    "\n",
    "    def _fit(self, train_data: pd.DataFrame) -> None:\n",
    "\n",
    "        x_data = train_data.drop([self.cd_axis_targets], axis=1)\n",
    "        self.target_feature_dtypes = dict(x_data.dtypes)\n",
    "        y_data = train_data[self.cd_axis_targets]\n",
    "        print(x_data)\n",
    "\n",
    "        # if self.verbose != -1:\n",
    "        print(\n",
    "            \"\\t\\ttarget: \",\n",
    "            self.cd_axis_targets,\n",
    "            \" x_data_cols:\",\n",
    "            x_data.columns.tolist(),\n",
    "            \" y_data_cols:\",\n",
    "            self.cd_axis_targets,\n",
    "        )\n",
    "\n",
    "        reg_objs = {}\n",
    "        for qtile in self.quantile_levels:\n",
    "            sub_predictor = LGBMRegressor(\n",
    "                objective=self.objective, alpha=qtile, verbose=-1, n_jobs=-1\n",
    "            )\n",
    "            sub_predictor.fit(x_data, y_data)\n",
    "            reg_objs[qtile] = sub_predictor\n",
    "\n",
    "        self.sub_predictor = {\n",
    "            \"predictors\": reg_objs,\n",
    "            \"targets\": self.cd_axis_targets,\n",
    "            \"features\": x_data.columns.tolist(),\n",
    "        }\n",
    "\n",
    "    def _predict(self, test_data: pd.Series) -> Dict[float, List[npt.NDArray]]:\n",
    "\n",
    "        # making sure dtypes are the same as in training, and filtering out the target columns from the dtypes dict since in test data its not there.\n",
    "        needed_dtypes = {\n",
    "            col: col_dtype\n",
    "            for col, col_dtype in self.target_feature_dtypes.items()\n",
    "            if \"_tgt\" not in col\n",
    "        }\n",
    "\n",
    "        test_data_cpy = (\n",
    "            test_data.to_frame().T.reset_index(drop=True).copy().astype(needed_dtypes)\n",
    "        )\n",
    "\n",
    "        data = {}\n",
    "\n",
    "        for qtile in self.quantile_levels:\n",
    "            pred = self.sub_predictor[\"predictors\"][qtile].predict(test_data_cpy)\n",
    "            data[qtile] = pred\n",
    "\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74a9181d-3e18-43ef-8985-918c873bbc2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "conda_env = {\n",
    "    \"channels\": [\"defaults\"],\n",
    "    \"dependencies\": [\n",
    "        \"python={}\".format(PYTHON_VERSION),\n",
    "        \"pip\",\n",
    "        {\n",
    "            \"pip\": [\n",
    "                \"mlflow\",\n",
    "                \"lightgbm\",\n",
    "                \"cloudpickle=={}\".format(cloudpickle.__version__),\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "    \"name\": \"model_wrapper_env\",\n",
    "}\n",
    "\n",
    "class ModelWrapperMlflowModel(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"Custom Pyfunc model since the main model is not of native format and hence,\n",
    "    doesn't belong to predefined MLflow flavors\n",
    "    \"\"\"\n",
    "\n",
    "    def load_context(self, context):\n",
    "        with open(f\"{context.artifacts['model_dir']}/model.pkl\", \"rb\") as pkl_file:\n",
    "            self.model_wrapper_model = pickle.load(pkl_file)\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        return self.model_wrapper_model.predict(model_input)\n",
    "    \n",
    "class ModelWrapperFarField(BaseModel):\n",
    "    \"\"\"Custom class which wraps a model type to generate\n",
    "    predictions in a timeseries format.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cd_axis_max_lags: int,\n",
    "        static_cols: List[str],\n",
    "        model_strategy: BaseStrategy,\n",
    "        is_auto_reg: bool = False,\n",
    "        is_ca3_training: bool = True,\n",
    "        prediction_horizon: int = 7,\n",
    "        lag_numbers: List[int] = [],\n",
    "        quantiles: List[float] = [0.5],\n",
    "        mlflow_run_id: Optional[str] = None,\n",
    "        hotel_id: Optional[str] = None,\n",
    "        version: Optional[Union[str, int]] = None,\n",
    "        stage: Optional[str] = None,\n",
    "        target_type: str = \"REVENUE\",\n",
    "        exclude_pms: bool = False,\n",
    "        save_models: bool = True,\n",
    "        local_root_dir: Optional[str] = None,\n",
    "        model_type: str = \"MODELWRAPPER_FARFIELD\",\n",
    "        model_name_prefix: Optional[str] = None,\n",
    "        meta_data: Dict[str, Any] = {},\n",
    "        n_cd_lags: Optional[int] = None,\n",
    "        forecast_points=[91, 84, 77, 70, 63, 56, 49, 42, 35],\n",
    "        is_parallel=True,\n",
    "        executor = \"processpool\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            model_type=model_strategy.model_type,\n",
    "            prediction_horizon=prediction_horizon,\n",
    "            lag_numbers=lag_numbers,\n",
    "            quantiles=quantiles,\n",
    "            mlflow_run_id=mlflow_run_id,\n",
    "            hotel_id=hotel_id,\n",
    "            version=version,\n",
    "            stage=stage,\n",
    "            target_type=target_type,\n",
    "            exclude_pms=exclude_pms,\n",
    "            save_models=save_models,\n",
    "            local_root_dir=local_root_dir,\n",
    "            model_name_prefix=model_name_prefix,\n",
    "            meta_data=meta_data,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.quantile_levels.sort()\n",
    "        self.cd_axis_max_lags = cd_axis_max_lags\n",
    "        self.sd_axis_lag_prefix = \"lag\"\n",
    "        self.static_cols = static_cols\n",
    "        self.n_cd_lags = n_cd_lags\n",
    "        self.target_suffix = \"_tgt\"\n",
    "        self.is_auto_reg = is_auto_reg\n",
    "        self.forecast_points = forecast_points\n",
    "        self.model_strategy = model_strategy\n",
    "        self.model_type = self.model_strategy.model_type\n",
    "        self.all_cd_cols = [\n",
    "            f\"{self.target_prefix}{i}\" for i in range(self.cd_axis_max_lags + 1)\n",
    "        ]\n",
    "        self.lag_numbers = [\n",
    "            x for x in range(0, self.cd_axis_max_lags + 1, self.prediction_horizon)\n",
    "        ]\n",
    "        self.is_ca3_training = is_ca3_training\n",
    "        self.executor = executor\n",
    "\n",
    "        # initializing targets variables\n",
    "        self.target_cols: Dict[int, str] = {}\n",
    "\n",
    "        # initializing feature variables\n",
    "        self.feature_cols: Dict[int, List[str]] = {}\n",
    "\n",
    "        self.is_parallel = is_parallel\n",
    "\n",
    "        if 0.5 not in self.quantile_levels:\n",
    "            raise ValueError(\n",
    "                \"median quantile (0.5) is not included in the quantile_levels. please ensure that its included\"\n",
    "            )\n",
    "\n",
    "    def save_model(self) -> None:\n",
    "        \"\"\"Saves the models in the local directory, which will then be logged as artifacts in MLflow\"\"\"\n",
    "        os.makedirs(self.local_dir)\n",
    "        with open(self.local_path, \"wb\") as pkl_file:\n",
    "            pickle.dump(obj=self, file=pkl_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def log_models(self) -> None:\n",
    "        \"\"\"Carries out the mlflow model registry procedures\"\"\"\n",
    "        print(\"Starting model logging\")\n",
    "        self.save_model()\n",
    "\n",
    "        modelpath = self.get_model_log_path()\n",
    "        print(\"Logging model\")\n",
    "        mlflow.pyfunc.log_model(\n",
    "            artifact_path=self.get_model_log_path(),\n",
    "            python_model=ModelWrapperMlflowModel(),\n",
    "            artifacts=self.artifacts,\n",
    "            conda_env=conda_env,\n",
    "        )\n",
    "\n",
    "        print(\"Registering model\")\n",
    "        result = mlflow.register_model(\n",
    "            self.get_model_register_path(),\n",
    "            self.get_model_name(),\n",
    "            tags=self.meta_data,\n",
    "        )\n",
    "\n",
    "    def clean(self) -> None:\n",
    "        if os.path.exists(self.local_root):\n",
    "            shutil.rmtree(self.local_root)\n",
    "\n",
    "    def load_pyfunc_model(\n",
    "        self, dst_path: Optional[str] = None, tag: Optional[Union[str, int]] = None\n",
    "    ) -> mlflow.pyfunc.PyFuncModel:\n",
    "        \"\"\"Load and return the pyfunc model from the MLFlow model repository\n",
    "\n",
    "        Args:\n",
    "            dst_path (str, optional): Destination path to save the loaded model.\n",
    "                                      If not provided the files will be saved in the local_root path.\n",
    "                                      Defaults to None.\n",
    "            tag (str, optional): Tag to specify the version or model stage to be loaded.\n",
    "                                 If not provided the latest model version will be loaded.\n",
    "                                Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            mlflow.pyfunc.model : pyfunc model\n",
    "        \"\"\"\n",
    "        # self.local_dir = dst_path\n",
    "        print(f\"Loading model {self.get_model_uri()}\")\n",
    "\n",
    "        if dst_path is not None:\n",
    "            self.local_root = dst_path\n",
    "\n",
    "        if os.path.exists(self.local_root):\n",
    "            self.clean()\n",
    "\n",
    "        os.mkdir(self.local_root)\n",
    "\n",
    "        model = mlflow.pyfunc.load_model(\n",
    "            self.get_model_uri(tag=tag), dst_path=self.local_root\n",
    "        )\n",
    "\n",
    "        self.run_id = model._model_meta.run_id\n",
    "\n",
    "        # following is a bit of a round about way to set local_dir\n",
    "        # having the run id in the directory name is a bit troublesome as the run id is not available to us when we create the autogluon object\n",
    "        # TODO make sure to remove the run id from the local_dir and include either or both task_type/exclude_pms\n",
    "        # TODO make sure to set the local_dir consistently for both training and inference tasks\n",
    "        # self.local_dir = \"/ag_models/\"\n",
    "\n",
    "        # if self.exclude_pms:\n",
    "        #     self.local_dir = f\"ag_models_{self.hotel_id}_{self.run_id}/\"\n",
    "\n",
    "        # os.rename(\"artifacts\",self.local_dir)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def get_filtered_data(\n",
    "        self, data: pd.DataFrame, forecast_point: int\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "\n",
    "        if self.is_ca3_training:\n",
    "            # Condition helps us get the specific entry for the cancellation day index\n",
    "            target_column = (\n",
    "                f\"{self.target_prefix}{forecast_point-self.prediction_horizon}_tgt\"\n",
    "            )\n",
    "            condition = data[\"forecast_index\"] == forecast_point\n",
    "            filt_data = data[condition].copy()\n",
    "        else:\n",
    "            target_column = (\n",
    "                f\"{self.target_prefix}{forecast_point-self.prediction_horizon}\"\n",
    "            )\n",
    "            filt_data = data.copy()\n",
    "\n",
    "        cd_axis_lag_columns = [\n",
    "            f\"{self.target_prefix}{x}\"\n",
    "            for x in range(forecast_point, self.cd_axis_max_lags + 1)\n",
    "        ]\n",
    "\n",
    "        sd_axis_lag_columns = [\n",
    "            f\"{self.sd_axis_lag_prefix}{SD_lag}\"\n",
    "            for SD_lag in self.lag_numbers\n",
    "            if SD_lag > forecast_point\n",
    "        ]\n",
    "\n",
    "        # assigning target and feature variables corresponding to the particular day ahead. This will be retrieved through the attributes in the inference phase.\n",
    "        self.target_cols[forecast_point] = target_column\n",
    "        self.feature_cols[forecast_point] = (\n",
    "            cd_axis_lag_columns + sd_axis_lag_columns + self.static_cols\n",
    "        )\n",
    "\n",
    "        x_data = filt_data[self.feature_cols[forecast_point]]\n",
    "        y_data = filt_data[self.target_cols[forecast_point]]\n",
    "\n",
    "        return (\n",
    "            x_data,\n",
    "            y_data,\n",
    "            filt_data[[target_column] + self.feature_cols[forecast_point]],\n",
    "        )\n",
    "\n",
    "    def train_inner_model(self, train_data: pd.DataFrame, forecast_point: int) -> None:\n",
    "        print(\"\\tForecast point: \", forecast_point)\n",
    "        x_train, y_train, xy_train = self.get_filtered_data(\n",
    "            data=train_data, forecast_point=forecast_point\n",
    "        )\n",
    "\n",
    "        reg_obj = self.model_strategy(\n",
    "            quantile_levels=self.quantile_levels,\n",
    "            day_ahead=self.prediction_horizon,\n",
    "            cd_axis_targets=self.target_cols[forecast_point],\n",
    "            path=self.local_dir,\n",
    "            is_auto_reg=self.is_auto_reg,\n",
    "        )  # type: ignore\n",
    "        reg_obj._fit(xy_train)\n",
    "\n",
    "        return reg_obj\n",
    "\n",
    "        # return (forecast_point,reg_obj)       \n",
    "\n",
    "    def get_pool_executer(self):\n",
    "        if self.executor == \"threadpool\":\n",
    "            return ThreadPoolExecutor\n",
    "        elif self.executor == \"processpool\":\n",
    "            return ProcessPoolExecutor\n",
    "\n",
    "\n",
    "    def train(self, train_data: pd.DataFrame, n_threads=3): # -> None\n",
    "        \"\"\"\n",
    "        trains models for each day ahead quantile predictions and  relevant\n",
    "        to the specified prediction_horizon value and the specific quantile\n",
    "        levels.\n",
    "\n",
    "        parameters:\n",
    "            train_data = training data with the booking pace lags, stay date lags or\n",
    "                    other features such as date features.\n",
    "\n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        # Using ProcessPoolExecutor\n",
    "        if self.is_parallel:\n",
    "            models = {}\n",
    "            with self.get_pool_executer()(max_workers=n_threads) as executor:\n",
    "                future_to_target = {executor.submit(\n",
    "                    self.train_inner_model, \n",
    "                    train_data, \n",
    "                    forecast_point): forecast_point for forecast_point in self.forecast_points}\n",
    "                \n",
    "                for future in as_completed(future_to_target):\n",
    "                    forecast_point = future_to_target[future]                    \n",
    "                    try:\n",
    "                        model = future.result()\n",
    "                        # index_, model = future.result()\n",
    "                    except Exception as exc:\n",
    "                        print(exc)\n",
    "                    else:\n",
    "                        # self.models[forecast_point] = model\n",
    "                        # self.models[index_] = model\n",
    "                        models[forecast_point] = model\n",
    "\n",
    "            self.models = models\n",
    "        else:\n",
    "            for forecast_point in self.forecast_points:\n",
    "                self.models[forecast_point] = self.train_inner_model(train_data, forecast_point)\n",
    "\n",
    "        model_list = self.models\n",
    "        return model_list\n",
    "    \n",
    "        # with ProcessPoolExecutor(max_workers=3) as executor:\n",
    "        #     futures = [executor.submit(\n",
    "        #         self.train_forecast_point,\n",
    "        #         train_data, \n",
    "        #         forecast_point) for forecast_point in self.forecast_points\n",
    "        #     ]\n",
    "\n",
    "        #     for future in futures:\n",
    "        #         self.models[future.result()[0]] = future.result()[1]\n",
    "            \n",
    "\n",
    "        # for forecast_point in self.forecast_points:\n",
    "            # print(\"\\tForecast point: \", forecast_point)\n",
    "\n",
    "            # x_train, y_train, xy_train = self.get_filtered_data(\n",
    "            #     data=train_data, forecast_point=forecast_point\n",
    "            # )\n",
    "            # reg_obj = self.model_strategy(\n",
    "            #     quantile_levels=self.quantile_levels,\n",
    "            #     day_ahead=self.prediction_horizon,\n",
    "            #     cd_axis_targets=self.target_cols[forecast_point],\n",
    "            #     path=self.local_dir,\n",
    "            #     is_auto_reg=self.is_auto_reg,\n",
    "            # )  # type: ignore\n",
    "            # reg_obj._fit(xy_train)\n",
    "\n",
    "            # self.models[forecast_point] = reg_obj\n",
    "\n",
    "        if self.do_save_models:\n",
    "            self.log_models()\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        test_data: pd.DataFrame,\n",
    "    ) -> Tuple[\n",
    "        List[npt.NDArray], List[pd.Series], List[npt.NDArray], List[npt.NDArray]\n",
    "    ]:\n",
    "        \"\"\"generating quantile predictions for the test data provided. test_data\n",
    "        should be provided which aligns with the prediction_horizon. If\n",
    "        test_data has less rows than the prediction_horizon, then the length\n",
    "        of the test_data will be considered as the prediction horizon.\n",
    "\n",
    "        eg: if prediction_horizon= 28, ideally test_data should have 28 rows\n",
    "            which are relevant for 28 stay dates.\n",
    "\n",
    "        parameters:\n",
    "            test_data = test data which aligns with the prediction horizon.\n",
    "                        rows of test_data <= prediction_horizon.\n",
    "\n",
    "        Returns: Lists with actual values and corresponding predicted values along the booking axis leading upto the\n",
    "          relevant stay date ahead\n",
    "        \"\"\"\n",
    "        y_test_lst = []\n",
    "        y_pred_lst = []\n",
    "        y_upper_lst = []\n",
    "        y_lower_lst = []\n",
    "        forecast_point = test_data[\"forecast_point\"].iloc[0]\n",
    "\n",
    "        # for forecast_point in self.forecast_points:\n",
    "        target_column = f\"{self.target_prefix}{forecast_point-self.prediction_horizon}\"\n",
    "        predictor = self.models[forecast_point]\n",
    "\n",
    "        x_test = test_data[predictor.sub_predictor[\"features\"]]\n",
    "        y_test_lst.append(test_data[target_column].iloc[0])\n",
    "\n",
    "        y_pred_dct = predictor._predict(x_test.squeeze())\n",
    "\n",
    "        if 0.1 in self.quantile_levels:\n",
    "            y_lower_lst.append(y_pred_dct[0.1][0])\n",
    "        else:\n",
    "            y_lower_lst.append(y_pred_dct[0.5][0])\n",
    "\n",
    "        if 0.5 in self.quantile_levels:\n",
    "            y_pred_lst.append(y_pred_dct[0.5][0])\n",
    "\n",
    "        if 0.9 in self.quantile_levels:\n",
    "            y_upper_lst.append(y_pred_dct[0.9][0])\n",
    "        else:\n",
    "            y_upper_lst.append(y_pred_dct[0.5][0])\n",
    "\n",
    "        return y_pred_lst, y_test_lst, y_upper_lst, y_lower_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21bfffcb-1516-485a-883f-82fd9d6a3677",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import Callable, List, Optional\n",
    "from phgml.models.model_wrapper import ModelWrapper #, ModelWrapperFarField\n",
    "# from phgml.models.model_strategy import StrategyLGBM, StrategyAG, StrategyLGBMFarField\n",
    "\n",
    "def train_wrapper_farfield(\n",
    "    target_type: str,\n",
    "    ml_experiment_id: str,\n",
    "    exclude_pms: bool,\n",
    "    calc_uncertainty: bool,\n",
    "    hotel_config_provider: FarfieldForecastingHotelConfigProvider,\n",
    "    processing_timestamp: datetime,\n",
    "    save_models: bool,\n",
    "    save_metrics: bool,\n",
    "    lag_numbers: List[int],\n",
    "    forecast_points: List[int],\n",
    "    model_name_prefix: Optional[str] = None,\n",
    "    model_tags: dict = None,\n",
    "    is_parallel: bool = True,\n",
    "    executor: str = \"processpool\"\n",
    ") -> Callable:\n",
    "\n",
    "    def train_data_models(df):\n",
    "        static_cols_ = [\n",
    "            \"year\",\n",
    "            \"quarter_of_year\",\n",
    "            \"month_of_year\",\n",
    "            \"week_of_year\",\n",
    "            \"day_of_year\",\n",
    "            \"month_of_quarter\",\n",
    "            \"week_of_quarter\",\n",
    "            \"day_of_quarter\",\n",
    "            \"week_of_month\",\n",
    "            \"day_of_month\",\n",
    "            \"holiday\",\n",
    "            \"day_of_week_0\",\n",
    "            \"day_of_week_1\",\n",
    "            \"day_of_week_2\",\n",
    "            \"day_of_week_3\",\n",
    "            \"day_of_week_4\",\n",
    "            \"day_of_week_5\",\n",
    "            \"day_of_week_6\",\n",
    "        ]\n",
    "\n",
    "        logger = get_dbx_logger(\"PHGML\")\n",
    "\n",
    "        trainer = None\n",
    "        hotel_id = df[\"HotelID\"].iloc[0]\n",
    "        hotel_config = hotel_config_provider.get_config(hotel_id)\n",
    "        model_type = hotel_config.training_model_name\n",
    "\n",
    "        max_lead = max(forecast_points)\n",
    "\n",
    "        if target_type == \"REVENUE\":\n",
    "            col_prefix = \"RV\"\n",
    "        elif target_type == \"ROOMS\":\n",
    "            col_prefix = \"RM\"\n",
    "\n",
    "        test_partition_end = df[\"_StayDates\"].max()\n",
    "        test_partition_start = test_partition_end - pd.Timedelta(max_lead, \"D\")\n",
    "\n",
    "        metadata_dict = {\n",
    "            \"last_trained_date\": str(test_partition_start),\n",
    "            \"booking_pace_start\": hotel_config.booking_pace_start,\n",
    "            \"booking_pace_end\": hotel_config.booking_pace_end,\n",
    "        }\n",
    "        metadata_dict.update(model_tags)\n",
    "\n",
    "        logger.debug(f\"{hotel_id}:Filter train data\")\n",
    "        dftest = df[\n",
    "            (df.HotelID == hotel_id)\n",
    "            & (df[\"_StayDates\"] >= test_partition_start)\n",
    "            & (df[\"_StayDates\"] <= test_partition_end)\n",
    "        ]\n",
    "\n",
    "        logger.debug(f\"{hotel_id}:Filter test data\")\n",
    "        dftrain = df[\n",
    "            (df.HotelID == hotel_id) & (df[\"_StayDates\"] < test_partition_start)\n",
    "        ]\n",
    "\n",
    "        model_version = 1\n",
    "        model_stage = \"Staging\"\n",
    "        model_name = None\n",
    "\n",
    "        pms = \"PMS\"\n",
    "        if exclude_pms:\n",
    "            pms = \"NOPMS\"\n",
    "\n",
    "        with mlflow.start_run(\n",
    "            experiment_id=ml_experiment_id,\n",
    "            run_name=f\"farfield-{model_type}-{target_type}-{pms}-{hotel_id}-{hotel_config.hotel_name}\",\n",
    "        ) as run:\n",
    "            run_id = run.info.run_id\n",
    "\n",
    "            if model_type == \"LIGHTGBM\":\n",
    "                trainer = ModelWrapperFarField(\n",
    "                    model_strategy=StrategyLGBMFarField,\n",
    "                    is_auto_reg=False,\n",
    "                    prediction_horizon=hotel_config.prediction_horizon,\n",
    "                    hotel_id=hotel_config.hotel_id,\n",
    "                    target_type=target_type,\n",
    "                    exclude_pms=exclude_pms,\n",
    "                    cd_axis_max_lags=hotel_config.booking_pace_end,\n",
    "                    static_cols=static_cols_,\n",
    "                    forecast_points=forecast_points,\n",
    "                    save_models=save_models,\n",
    "                    quantiles=[0.1, 0.5, 0.9],\n",
    "                    mlflow_run_id=run_id,\n",
    "                    model_name_prefix=model_name_prefix,\n",
    "                    meta_data=metadata_dict,\n",
    "                    is_parallel=is_parallel,\n",
    "                    executor = executor\n",
    "                )\n",
    "\n",
    "            model_name = trainer.get_model_name()\n",
    "            output_df = pd.DataFrame()\n",
    "            try:\n",
    "                logger.info(f\"{hotel_id}:Training model\")\n",
    "                model_list = trainer.train(dftrain)\n",
    "                logger.info(f\"{hotel_id}:Completed training\")\n",
    "\n",
    "                logger.info(f\"{hotel_id}:Start prediction\")\n",
    "                start_date = dftest[\"_StayDates\"].min()\n",
    "                end_date = dftest[\"_StayDates\"].max()\n",
    "                observed_stays = [\n",
    "                    start_date + timedelta(days=delta)\n",
    "                    for delta in range((end_date - start_date).days + 1)\n",
    "                ]\n",
    "\n",
    "                result_df_list = []\n",
    "                test_list = []\n",
    "                for forecast_point in forecast_points:\n",
    "                    print(f\"Training for {forecast_point} days\")\n",
    "                    stay_date_ = test_partition_start + timedelta(days=forecast_point)\n",
    "\n",
    "                    temp_df = dftest[\n",
    "                        (dftest[\"HotelID\"] == hotel_id)\n",
    "                        & (dftest[\"_StayDates\"] == stay_date_)\n",
    "                        & (dftest[\"forecast_index\"] == forecast_point)\n",
    "                    ]\n",
    "                    temp_df[\"forecast_point\"] = forecast_point\n",
    "\n",
    "                    y_pred_lst, y_test_lst, y_upper_lst, y_lower_lst = (\n",
    "                        [0],\n",
    "                        [0],\n",
    "                        [0],\n",
    "                        [0],\n",
    "                    )\n",
    "                    if not temp_df.empty:  \n",
    "                        y_pred_lst, y_test_lst, y_upper_lst, y_lower_lst = (\n",
    "                            trainer.predict(temp_df)\n",
    "                        )\n",
    "\n",
    "                    temp_result = pd.DataFrame(\n",
    "                        {\n",
    "                            \"prediction_points\": [forecast_point],\n",
    "                            \"y_true\": y_test_lst,\n",
    "                            \"y_pred\": y_pred_lst,\n",
    "                            \"y_lower\": y_lower_lst,\n",
    "                            \"y_upper\": y_upper_lst,\n",
    "                        }\n",
    "                    )\n",
    "                    temp_result[\"_StayDates\"] = stay_date_\n",
    "                    temp_result[\"HotelID\"] = hotel_id\n",
    "\n",
    "                    result_df_list.append(temp_result)\n",
    "\n",
    "                if save_metrics:\n",
    "                    log_metrics_stays_farfield(result_df_list, trainer)\n",
    "\n",
    "                report_metrics_stays_farfield(result_df_list)\n",
    "\n",
    "                del dftrain\n",
    "                del dftest\n",
    "\n",
    "                output_df = pd.DataFrame(\n",
    "                    {\n",
    "                        \"HotelID\": [hotel_id],\n",
    "                        \"run_id\": [run_id],\n",
    "                        \"model_version\": [model_version],\n",
    "                        \"timestamp\": [processing_timestamp],\n",
    "                        \"pms_sync_off\": [exclude_pms],\n",
    "                        \"model_name\": [model_name],\n",
    "                        \"status\": \"complete\",\n",
    "                        \"message\": f\"Successfully trained {hotel_id}\",\n",
    "                    }\n",
    "                )\n",
    "            except Exception as e:\n",
    "                empty = pd.DataFrame(\n",
    "                    {\n",
    "                        \"HotelID\": [hotel_id],\n",
    "                        \"run_id\": [run_id],\n",
    "                        \"model_version\": [model_version],\n",
    "                        \"timestamp\": [pd.Timestamp(\"1900-01-01\")],\n",
    "                        \"pms_sync_off\": [exclude_pms],\n",
    "                        \"model_name\": [model_name],\n",
    "                        \"status\": \"incomplete\",\n",
    "                        \"message\": str(e),\n",
    "                    }\n",
    "                )\n",
    "                return empty\n",
    "\n",
    "            finally:\n",
    "                if (model_type == \"AUTOGLUON\") and (trainer is not None):\n",
    "                    trainer.clean()\n",
    "\n",
    "        return output_df\n",
    "\n",
    "    return train_data_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa83a588-d841-46af-981a-aa885a301b04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Group the data by hotel id and execute the trainings in parallel\n",
    "# logger.info(\"Starting parallel training old way\")\n",
    "\n",
    "# output_df_old = df.groupby(\"HotelID\").applyInPandas(\n",
    "#     train_wrapper_farfield(\n",
    "#         target_type=params[\"TARGET_TYPE\"],\n",
    "#         ml_experiment_id=params[\"ML_EXPERIMENT_ID\"],\n",
    "#         exclude_pms=params[\"WITHOUT_PMS\"],\n",
    "#         calc_uncertainty=params[\"CALC_UNCERTAINTY\"],\n",
    "#         hotel_config_provider=forecasting_config_provider,\n",
    "#         processing_timestamp=datetime.now(),\n",
    "#         save_models=params[\"SAVE_MODEL\"],\n",
    "#         save_metrics=params[\"SAVE_METRICS\"],\n",
    "#         lag_numbers=[],\n",
    "#         forecast_points=params[\"FORECAST_POINTS\"],\n",
    "#         # model_name_prefix=params[\"MODEL_NAME_PREFIX\"],\n",
    "#         model_tags=params[\"MODEL_TAGS_DCT\"],\n",
    "#         is_parallel=False,        \n",
    "#     ),\n",
    "#     schema,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29f20e7d-1c26-4538-8ee0-a68d184def38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display(output_df_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fa0fc2b-3067-4edc-a0a2-bbd92d48afee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">24/09/12/ 16:10:27 UTC:PHGML-dev-REVENUE-PMS-INFO-Starting parallel training new\n",
       "Starting parallel training new\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">24/09/12/ 16:10:27 UTC:PHGML-dev-REVENUE-PMS-INFO-Starting parallel training new\nStarting parallel training new\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger.info(\"Starting parallel training new\")\n",
    "\n",
    "output_df_new = df.groupby(\"HotelID\").applyInPandas(\n",
    "    train_wrapper_farfield(\n",
    "        target_type=params[\"TARGET_TYPE\"],\n",
    "        ml_experiment_id=params[\"ML_EXPERIMENT_ID\"],\n",
    "        exclude_pms=params[\"WITHOUT_PMS\"],\n",
    "        calc_uncertainty=params[\"CALC_UNCERTAINTY\"],\n",
    "        hotel_config_provider=forecasting_config_provider,\n",
    "        processing_timestamp=datetime.now(),\n",
    "        save_models=params[\"SAVE_MODEL\"],\n",
    "        save_metrics=params[\"SAVE_METRICS\"],\n",
    "        lag_numbers=[],\n",
    "        forecast_points=params[\"FORECAST_POINTS\"],\n",
    "        # model_name_prefix=params[\"MODEL_NAME_PREFIX\"],\n",
    "        model_tags=params[\"MODEL_TAGS_DCT\"],\n",
    "        is_parallel=True,\n",
    "        executor=\"threadpool\"\n",
    "    ),\n",
    "    schema,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b36bb101-71d1-45cb-af28-c965988fa049",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>HotelID</th><th>run_id</th><th>model_version</th><th>timestamp</th><th>pms_sync_off</th><th>model_name</th><th>status</th><th>message</th></tr></thead><tbody><tr><td>71999</td><td>ad2fe266fd7e4541817567802589f2d7</td><td>1</td><td>2024-09-12T16:10:27.882+0000</td><td>false</td><td>71999_REVENUE_PMS_LGBM_FARFIELD_model</td><td>complete</td><td>Successfully trained 71999</td></tr><tr><td>55810</td><td>e46e123585aa44b8a236754f0f795998</td><td>1</td><td>2024-09-12T16:10:27.882+0000</td><td>false</td><td>55810_REVENUE_PMS_LGBM_FARFIELD_model</td><td>complete</td><td>Successfully trained 55810</td></tr><tr><td>10443</td><td>8784a59c09364c498e6c91eaaa970c2c</td><td>1</td><td>2024-09-12T16:10:27.882+0000</td><td>false</td><td>10443_REVENUE_PMS_LGBM_FARFIELD_model</td><td>complete</td><td>Successfully trained 10443</td></tr><tr><td>63662</td><td>ec3b890e8da64899b7213002be5abac4</td><td>1</td><td>2024-09-12T16:10:27.882+0000</td><td>false</td><td>63662_REVENUE_PMS_LGBM_FARFIELD_model</td><td>complete</td><td>Successfully trained 63662</td></tr><tr><td>1406</td><td>fb6a1d380e3347c180027240d6a8ca52</td><td>1</td><td>2024-09-12T16:10:27.882+0000</td><td>false</td><td>1406_REVENUE_PMS_LGBM_FARFIELD_model</td><td>complete</td><td>Successfully trained 1406</td></tr><tr><td>64942</td><td>9fce8b41db2e44e0bb48bf1967fc1226</td><td>1</td><td>2024-09-12T16:10:27.882+0000</td><td>false</td><td>64942_REVENUE_PMS_LGBM_FARFIELD_model</td><td>complete</td><td>Successfully trained 64942</td></tr><tr><td>26532</td><td>f56b5f8cd5724aa485f07f7081754849</td><td>1</td><td>2024-09-12T16:10:27.882+0000</td><td>false</td><td>26532_REVENUE_PMS_LGBM_FARFIELD_model</td><td>complete</td><td>Successfully trained 26532</td></tr><tr><td>26834</td><td>da624252db7c44d3a2fd4ba94c1f9ca1</td><td>1</td><td>2024-09-12T16:10:27.882+0000</td><td>false</td><td>26834_REVENUE_PMS_LGBM_FARFIELD_model</td><td>complete</td><td>Successfully trained 26834</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "71999",
         "ad2fe266fd7e4541817567802589f2d7",
         1,
         "2024-09-12T16:10:27.882+0000",
         false,
         "71999_REVENUE_PMS_LGBM_FARFIELD_model",
         "complete",
         "Successfully trained 71999"
        ],
        [
         "55810",
         "e46e123585aa44b8a236754f0f795998",
         1,
         "2024-09-12T16:10:27.882+0000",
         false,
         "55810_REVENUE_PMS_LGBM_FARFIELD_model",
         "complete",
         "Successfully trained 55810"
        ],
        [
         "10443",
         "8784a59c09364c498e6c91eaaa970c2c",
         1,
         "2024-09-12T16:10:27.882+0000",
         false,
         "10443_REVENUE_PMS_LGBM_FARFIELD_model",
         "complete",
         "Successfully trained 10443"
        ],
        [
         "63662",
         "ec3b890e8da64899b7213002be5abac4",
         1,
         "2024-09-12T16:10:27.882+0000",
         false,
         "63662_REVENUE_PMS_LGBM_FARFIELD_model",
         "complete",
         "Successfully trained 63662"
        ],
        [
         "1406",
         "fb6a1d380e3347c180027240d6a8ca52",
         1,
         "2024-09-12T16:10:27.882+0000",
         false,
         "1406_REVENUE_PMS_LGBM_FARFIELD_model",
         "complete",
         "Successfully trained 1406"
        ],
        [
         "64942",
         "9fce8b41db2e44e0bb48bf1967fc1226",
         1,
         "2024-09-12T16:10:27.882+0000",
         false,
         "64942_REVENUE_PMS_LGBM_FARFIELD_model",
         "complete",
         "Successfully trained 64942"
        ],
        [
         "26532",
         "f56b5f8cd5724aa485f07f7081754849",
         1,
         "2024-09-12T16:10:27.882+0000",
         false,
         "26532_REVENUE_PMS_LGBM_FARFIELD_model",
         "complete",
         "Successfully trained 26532"
        ],
        [
         "26834",
         "da624252db7c44d3a2fd4ba94c1f9ca1",
         1,
         "2024-09-12T16:10:27.882+0000",
         false,
         "26834_REVENUE_PMS_LGBM_FARFIELD_model",
         "complete",
         "Successfully trained 26834"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "HotelID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "run_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "model_version",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "pms_sync_off",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "model_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "message",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(output_df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7d210d6-b6a2-41df-b79b-e356ff7e58cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# output_df = output_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d812e32-d539-427d-b685-2d1d87f6e537",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for index, row in output_df.iterrows():\n",
    "#     if row.status == \"complete\":\n",
    "#         logger.info(f\"{row.message}\")\n",
    "#     else:\n",
    "#         logger.error(\n",
    "#             f\"Error encountered when training hotel {row.HotelID}: {row.message}\"\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a003b057-af15-45d5-9289-3550a062e97a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# client = MlflowClient()\n",
    "# completed = output_df[output_df[\"status\"]==\"complete\"]\n",
    "\n",
    "# outputs_list = []\n",
    "# for n,g in completed.groupby([\"HotelID\",\"model_name\"]):\n",
    "#     hotel_id = n[0]\n",
    "#     model_name = n[1]\n",
    "#     hotel_config = forecasting_config_provider.get_config(hotel_id)\n",
    "\n",
    "#     mv = client.get_latest_versions(name=model_name)[0]\n",
    "#     print(mv)\n",
    "#     arts = client.list_artifacts(mv.run_id,path=f\"forecasting/{hotel_id}/models/{model_name}/artifacts\")\n",
    "    \n",
    "#     outputs_list.append({\"hotel_id\":hotel_id,\n",
    "#                          \"model_name\":model_name,\n",
    "#                          \"creation_time\":datetime.fromtimestamp(mv.creation_timestamp/1e3),\n",
    "#                          \"last_update\":datetime.fromtimestamp(mv.last_updated_timestamp/1e3),\n",
    "#                          \"version\":mv.version,\n",
    "#                          \"target\":params[\"TARGET_TYPE\"],\n",
    "#                          \"exclude_pms\":params[\"WITHOUT_PMS\"],\n",
    "#                          \"config_train_length\":hotel_config.booking_pace_start,\n",
    "#                          \"config_infer_length\":hotel_config.booking_pace_end,\n",
    "#                          \"num_model_steps\":len(arts)-1})\n",
    "    \n",
    "#     print(f\"Hotel: {hotel_id} target_type:{params['TARGET_TYPE']} exclude_pms:{params['WITHOUT_PMS']} : {len(arts)-1}\")\n",
    "\n",
    "# completed_df = pd.DataFrame(outputs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d17c47a-1b14-4324-a4c9-5dfc805906c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">1 10\n",
       "2 20\n",
       "3 30\n",
       "4 40\n",
       "5 50\n",
       "6 60\n",
       "7 70\n",
       "8 80\n",
       "9 90\n",
       "10 100\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">1 10\n2 20\n3 30\n4 40\n5 50\n6 60\n7 70\n8 80\n9 90\n10 100\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# with ProcessPoolExecutor(max_workers=5) as executor:\n",
    "#     futures = [executor.submit(\n",
    "#         testfunction,\n",
    "#         i) for i in [1,2,3,4,5,6,7,8,9,10]]\n",
    "\n",
    "#     # {future: forecast_point}\n",
    "#     for future in futures:\n",
    "#         print(future.result()[0],future.result()[1])\n",
    "#         # self.models[future.result().day_ahead] = future.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9900696f-9dd7-48a7-98e9-72ad2d6481fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">{3: 30, 1: 10, 5: 50, 4: 40, 2: 20, 6: 60, 7: 70, 9: 90, 10: 100, 8: 80}\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">{3: 30, 1: 10, 5: 50, 4: 40, 2: 20, 6: 60, 7: 70, 9: 90, 10: 100, 8: 80}\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import time\n",
    "\n",
    "# def testfunction(number:int):\n",
    "#     result_ = number*10\n",
    "#     time.sleep(6)\n",
    "#     return (number, result_)\n",
    "\n",
    "# with ProcessPoolExecutor(max_workers=5) as executor:\n",
    "#     future_to_target = {\n",
    "#         executor.submit(\n",
    "#             testfunction,\n",
    "#             i): i for i in [1,2,3,4,5,6,7,8,9,10]\n",
    "#     }\n",
    "\n",
    "#     # print(future_to_target)\n",
    "\n",
    "#     models = {}\n",
    "#     for task in as_completed(future_to_target): \n",
    "#         i = future_to_target[task]\n",
    "#         try:\n",
    "#             index, resutl = task.result()\n",
    "#             # resutl = task.result()[1]\n",
    "#         except Exception as exc:\n",
    "#             print(exc)\n",
    "#         else:\n",
    "#             models[index] = resutl\n",
    "\n",
    "#     print(models)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2538145163135129,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "pp_ff_forecast_training_proc_pool",
   "widgets": {
    "model_tags": {
     "currentValue": "model_stage:CA3_develop",
     "nuid": "d849094c-cfa5-4368-949c-3d62c3fa6fb5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "model_stage:CA3_develop",
      "label": "Model Tags",
      "name": "model_tags",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "model_stage:CA3_develop",
      "label": "Model Tags",
      "name": "model_tags",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "save_model": {
     "currentValue": "True",
     "nuid": "bf270aee-a16d-437a-b2e2-90a7d41c44a2",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "True",
      "label": "Save Model",
      "name": "save_model",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "True",
        "False"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "True",
      "label": "Save Model",
      "name": "save_model",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "True",
        "False"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}