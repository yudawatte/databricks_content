{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abf2ba98-3ac0-450f-9943-cfa36c66ec42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.dropdown(\"env_stage\", \"dev\", [\"dev\", \"prod\", \"qa\",\"dev_synxis_2_0\", \"prod_synxis_2_0\", \"qa_synxis_2_0\"], \"Pipeline stage\")\n",
    "dbutils.widgets.dropdown(\"is_usd_currency\", \"True\", [\"True\", \"False\"], \"Use USD currency\")\n",
    "dbutils.widgets.text(\"eval_days\", \"30\", \"Evaluation Window (Days)\")\n",
    "dbutils.widgets.dropdown(\"plot_hotels\", \"False\", [\"True\", \"False\"], \"Display Triangular Plots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "892aa65c-1844-4812-bb49-2a99154e7cb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "import os\n",
    "from datetime import date\n",
    "from datetime import datetime, timedelta,date\n",
    "from phgml.data.config import ModuleConfig\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "sns.set()\n",
    "\n",
    "from phgml.data.config import ForecastingHotelConfigProvider,EnvironmentConfig\n",
    "from phgml.data.processing_distr_spark import preprocess_data,get_running_total\n",
    "from phgml.utilities.task_utilities import str_to_lst, str_to_bool\n",
    "from sktime.performance_metrics.forecasting import mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8b6c7f4-2e70-45dd-b9f9-2980634e6329",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "params = {}\n",
    "params[\"ENV\"] = dbutils.widgets.get(\"env_stage\")\n",
    "params[\"IS_USD_CURRENCY\"] = str_to_bool(getArgument(\"is_usd_currency\"))\n",
    "params[\"PLOT_HOTELS\"] = str_to_bool(getArgument(\"plot_hotels\"))\n",
    "params[\"EVAL_DAYS\"] = int(dbutils.widgets.get(\"eval_days\"))\n",
    "params[\"REVENUE_COL\"] = \"_reservationRevenuePerRoomUSD\"\n",
    "params[\"ROOMS_COL\"] = \"_rooms\"\n",
    "\n",
    "env_config_rv = EnvironmentConfig(env=params[\"ENV\"], target=\"REVENUE\", spark=spark, is_usd_currency=params[\"IS_USD_CURRENCY\"])\n",
    "env_config_rm = EnvironmentConfig(env=params[\"ENV\"], target=\"ROOMS\", spark=spark, is_usd_currency=params[\"IS_USD_CURRENCY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73600cbe-13a4-4dca-aa5b-b4ec64249fff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "params[\"INFERENCE_SUMMARY_TABLE\"] = env_config_rv.inference_summary_table\n",
    "params[\"INFERENCE_TRIANGULAR_METRIC_TABLE\"] = env_config_rv.inference_triangular_metric_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "837af395-009f-4399-944f-e8bfa0a540a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "hotel_ids = spark.sql(\n",
    "    \"select distinct HotelID,HotelName,No_of_rooms from dev_data.dim_hotels_data\"\n",
    ").toPandas()\n",
    "hotel_ids = hotel_ids.sort_values(\"HotelID\")\n",
    "hotel_ids[\"HotelName\"] = hotel_ids[\"HotelName\"].apply(lambda x: x[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cce94eee-956f-45e8-b7d5-f49569e31cd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Pick evel period\n",
    "eval_period_end = datetime.now() - timedelta(days=2)\n",
    "eval_period_start = eval_period_end - timedelta(days=params[\"EVAL_DAYS\"])\n",
    "print(f\"Evaluate from {eval_period_start} to {eval_period_end}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee45590a-1eb5-4fcc-bfe4-75ee7221f065",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load actuals\n",
    "if \"synxis_2_0\" in params['ENV']:\n",
    "    dfsp_src = spark.sql(\n",
    "        f\"select * from {env_config_rv.source_data_table}\"\n",
    "        )\n",
    "    \n",
    "    dfsp_src = dfsp_src.withColumn(\n",
    "        'cancellationDate',\n",
    "        F.when(F.col('status') == 'No-show', F.col('_StayDates')).otherwise(F.col('cancellationDate'))\n",
    "    )\n",
    "else:\n",
    "    dfsp_src = spark.sql(\n",
    "        f\"select a.TransactionID,a.HotelID,a._StayDates,a.confirmationDate,a.departureDate,a.channel,a.status,a.cancellationNumber,a._reservationRevenuePerRoomUSD,a._rooms,b.cancellationDate from {env_config.source_data_table} as a left join {env_config.transaction_data_table} as b on a.TransactionID=b.TransactionID\"\n",
    "        )\n",
    "    \n",
    "# load booking statuses \n",
    "result = spark.sql(\"SELECT * FROM phg_data.bookings_status\")\n",
    "\n",
    "confirmed_status_list = [row['status'] for row in result.filter(result.scenario == 'confirmed').collect()]\n",
    "cancelled_status_list = [row['status'] for row in result.filter(result.scenario == 'cancelled').collect()]\n",
    "\n",
    "# Display the list\n",
    "print(f\"Confirmed Booking Status List: {confirmed_status_list}\")\n",
    "print(f\"Cancelled Booking Status List: {cancelled_status_list}\")\n",
    "\n",
    "columns = [\n",
    "    \"HotelID\",\n",
    "    \"_StayDates\",\n",
    "    \"confirmationDate\",\n",
    "    \"channel\",\n",
    "    \"status\",\n",
    "    params[\"REVENUE_COL\"],\n",
    "    params[\"ROOMS_COL\"],\n",
    "]\n",
    "\n",
    "dfsp = dfsp_src.filter(\n",
    "        (F.col('status').isin(confirmed_status_list)) & (dfsp_src.cancellationDate.isNull())\n",
    "    ).select(columns)\n",
    "\n",
    "actuals_inc = preprocess_data(\n",
    "    dfsp,\n",
    "    False,\n",
    "    params[\"REVENUE_COL\"],\n",
    "    params[\"ROOMS_COL\"],\n",
    "    pd.to_datetime(eval_period_start),\n",
    "    cancel_aware=False\n",
    ")\n",
    "\n",
    "actuals_exc = preprocess_data(\n",
    "    dfsp,\n",
    "    False,\n",
    "    params[\"REVENUE_COL\"],\n",
    "    params[\"ROOMS_COL\"],\n",
    "    pd.to_datetime(eval_period_start),\n",
    "    cancel_aware=False\n",
    ")\n",
    "\n",
    "window_spec = Window.partitionBy(\"HotelID\", \"_StayDates\", \"confirmationDate\")\n",
    "\n",
    "actuals_inc = actuals_inc\\\n",
    "    .withColumn(\"_reservationRevenuePerRoomUSD\", F.sum(\"_reservationRevenuePerRoomUSD\").over(window_spec))\\\n",
    "    .withColumn(\"_rooms\", F.sum(\"_rooms\").over(window_spec))\n",
    "actuals_inc = actuals_inc.dropDuplicates([\"HotelID\", \"_StayDates\", \"confirmationDate\"])\n",
    "\n",
    "actuals_exc = actuals_exc\\\n",
    "    .withColumn(\"_reservationRevenuePerRoomUSD\", F.sum(\"_reservationRevenuePerRoomUSD\").over(window_spec))\\\n",
    "    .withColumn(\"_rooms\", F.sum(\"_rooms\").over(window_spec))\n",
    "actuals_exc = actuals_exc.dropDuplicates([\"HotelID\", \"_StayDates\", \"confirmationDate\"])\n",
    "\n",
    "rv_actuals_inc = get_running_total(actuals_inc, params[\"REVENUE_COL\"], cancel_aware=False)\n",
    "rv_actuals_inc = rv_actuals_inc.withColumn(\"pms_sync_off\", F.lit(False))\n",
    "rv_actuals_exc = get_running_total(actuals_exc, params[\"REVENUE_COL\"], cancel_aware=False)\n",
    "rv_actuals_exc = rv_actuals_exc.withColumn(\"pms_sync_off\", F.lit(True))\n",
    "\n",
    "rm_actuals_inc = get_running_total(actuals_inc, params[\"ROOMS_COL\"], cancel_aware=False)\n",
    "rm_actuals_inc = rm_actuals_inc.withColumn(\"pms_sync_off\", F.lit(False))\n",
    "rm_actuals_exc = get_running_total(actuals_exc, params[\"ROOMS_COL\"], cancel_aware=False)\n",
    "rm_actuals_exc = rm_actuals_exc.withColumn(\"pms_sync_off\", F.lit(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61ea7928-f3f0-4693-a46f-d7e53a18482d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Concat pms_off and pms_on dataframes\n",
    "rv_actuals_df = rv_actuals_inc.union(rv_actuals_exc)\n",
    "rm_actuals_df = rm_actuals_inc.union(rm_actuals_exc)\n",
    "req_cols =['HotelID', '_StayDates', 'confirmationDate','booking_lead','cum_sum_value','pms_sync_off']\n",
    "rv_actuals_df = rv_actuals_df.select(req_cols).withColumnRenamed(\"cum_sum_value\", \"revenue\")\n",
    "rm_actuals_df = rm_actuals_df.select(req_cols).withColumnRenamed(\"cum_sum_value\", \"rooms\")\n",
    "\n",
    "del actuals_inc, actuals_exc, rv_actuals_inc, rv_actuals_exc, rm_actuals_inc, rm_actuals_exc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f391641-8a15-41b4-b236-6415551b675f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load inferences - Revenue\n",
    "cols = ['HotelID', 'stay_date', 'booking_date', 'timestamp', 'pms_sync_off', 'day_index', 'y_med']\n",
    "inference_rv = spark.sql(f\"SELECT * FROM {env_config_rv.inference_output_table}\")\n",
    "inference_rv = inference_rv.filter(F.col(\"stay_date\") >= F.lit(eval_period_start).cast(\"date\"))\n",
    "\n",
    "grouped_df = (\n",
    "    inference_rv.groupBy(\"pms_sync_off\", \"timestamp\", \"HotelID\", \"stay_date\")\n",
    "    .agg(F.min(\"booking_date\").alias(\"eval_start_date\"))\n",
    ")\n",
    "\n",
    "# Join the eval_start_date back to the original DataFrame\n",
    "inference_rv = inference_rv.join(\n",
    "    grouped_df,\n",
    "    on=[\"pms_sync_off\", \"timestamp\", \"HotelID\", \"stay_date\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "inference_rv = inference_rv\\\n",
    "    .filter(F.col(\"booking_date\") > F.col(\"eval_start_date\"))\\\n",
    "    .withColumn(\"day_ahead_index\",F.datediff(F.col(\"stay_date\"), F.col(\"eval_start_date\")))\\\n",
    "    .withColumn(\"booking_index\",F.col(\"day_index\"))\n",
    "\n",
    "inference_rv = inference_rv.withColumnRenamed(\"booking_date\", \"confirmationDate\")\n",
    "inference_rv = inference_rv.withColumnRenamed(\"y_med\", \"y_pred\")\n",
    "inference_rv = inference_rv.withColumnRenamed(\"stay_date\", \"_StayDates\")\n",
    "inference_rv = inference_rv.withColumn(\"_StayDates\", F.to_date(F.col(\"_StayDates\")))\n",
    "inference_rv = inference_rv.withColumn(\"confirmationDate\", F.to_date(F.col(\"confirmationDate\")))\n",
    "\n",
    "cols = ['HotelID', '_StayDates', 'confirmationDate', \"day_ahead_index\", \"booking_index\",'y_pred', 'pms_sync_off', 'eval_start_date', 'timestamp']\n",
    "inference_rv = inference_rv.select(cols)\n",
    "\n",
    "# Merge actuals - Revenue\n",
    "inference_rv=inference_rv.join(rv_actuals_df, on= [\"HotelID\", \"_StayDates\", \"confirmationDate\", \"pms_sync_off\"], how=\"left\")\n",
    "inference_rv = inference_rv.withColumnRenamed(\"revenue\", \"y_true\")\n",
    "inference_rv = inference_rv.filter((F.col(\"y_true\").isNotNull()) & (F.col(\"y_true\") != 0))\n",
    "\n",
    "req_cols = ['HotelID', '_StayDates', 'confirmationDate', \"day_ahead_index\", \"booking_index\",'y_pred', 'y_true', 'pms_sync_off','timestamp']\n",
    "inference_rv = inference_rv.select(req_cols)\n",
    "\n",
    "inference_rv = inference_rv.withColumn(\n",
    "        \"SAPE\",\n",
    "        F.abs(F.col(\"y_true\") - F.col(\"y_pred\"))\n",
    "        * 2\n",
    "        / (F.abs(F.col(\"y_true\")) + F.abs(F.col(\"y_pred\"))),\n",
    "    )\n",
    "inference_rv = inference_rv.withColumn(\"target_type\", F.lit(\"REVENUE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed0942d3-1627-4e2e-98ca-5602c63a7496",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load inferences - Rooms\n",
    "cols = ['HotelID', 'stay_date', 'booking_date', 'timestamp', 'pms_sync_off', 'day_index', 'y_med']\n",
    "inference_rm = spark.sql(f\"SELECT * FROM {env_config_rm.inference_output_table}\")\n",
    "inference_rm = inference_rm.filter(F.col(\"stay_date\") >= F.lit(eval_period_start).cast(\"date\"))\n",
    "\n",
    "grouped_df = (\n",
    "    inference_rm.groupBy(\"pms_sync_off\", \"timestamp\", \"HotelID\", \"stay_date\")\n",
    "    .agg(F.min(\"booking_date\").alias(\"eval_start_date\"))\n",
    ")\n",
    "\n",
    "# Join the eval_start_date back to the original DataFrame\n",
    "inference_rm = inference_rm.join(\n",
    "    grouped_df,\n",
    "    on=[\"pms_sync_off\", \"timestamp\", \"HotelID\", \"stay_date\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "inference_rm = inference_rm\\\n",
    "    .filter(F.col(\"booking_date\") > F.col(\"eval_start_date\"))\\\n",
    "    .withColumn(\"day_ahead_index\",F.datediff(F.col(\"stay_date\"), F.col(\"eval_start_date\")))\\\n",
    "    .withColumn(\"booking_index\",F.col(\"day_index\"))\n",
    "\n",
    "inference_rm = inference_rm.withColumnRenamed(\"booking_date\", \"confirmationDate\")\n",
    "inference_rm = inference_rm.withColumnRenamed(\"y_med\", \"y_pred\")\n",
    "inference_rm = inference_rm.withColumnRenamed(\"stay_date\", \"_StayDates\")\n",
    "inference_rm = inference_rm.withColumn(\"_StayDates\", F.to_date(F.col(\"_StayDates\")))\n",
    "inference_rm = inference_rm.withColumn(\"confirmationDate\", F.to_date(F.col(\"confirmationDate\")))\n",
    "\n",
    "cols = ['HotelID', '_StayDates', 'confirmationDate', \"day_ahead_index\", \"booking_index\",'y_pred', 'pms_sync_off', 'eval_start_date', 'timestamp']\n",
    "inference_rm = inference_rm.select(cols)\n",
    "\n",
    "# Merge actuals - Rooms\n",
    "inference_rm = inference_rm.join(rm_actuals_df, on= [\"HotelID\", \"_StayDates\", \"confirmationDate\", \"pms_sync_off\"], how=\"left\")\n",
    "inference_rm = inference_rm.withColumnRenamed(\"rooms\", \"y_true\")\n",
    "inference_rm = inference_rm.filter((F.col(\"y_true\").isNotNull()) & (F.col(\"y_true\") != 0))\n",
    "\n",
    "req_cols = ['HotelID', '_StayDates', 'confirmationDate', \"day_ahead_index\", \"booking_index\",'y_pred', 'y_true', 'pms_sync_off','timestamp']\n",
    "inference_rm = inference_rm.select(req_cols)\n",
    "\n",
    "inference_rm = inference_rm.withColumn(\n",
    "        \"SAPE\",\n",
    "        F.abs(F.col(\"y_true\") - F.col(\"y_pred\"))\n",
    "        * 2\n",
    "        / (F.abs(F.col(\"y_true\")) + F.abs(F.col(\"y_pred\"))),\n",
    "    )\n",
    "inference_rm = inference_rm.withColumn(\"target_type\", F.lit(\"ROOMS\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5788d4a2-f9f2-4362-9976-7024b7226b4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_df = inference_rv.union(inference_rm)\n",
    "output_df = output_df.withColumn(\"eval_start_date\", F.lit(eval_period_start))\n",
    "output_df = output_df.withColumn(\"eval_end_date\", F.lit(eval_period_end))\n",
    "output_df = output_df.withColumn(\"Timestamp\", F.current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3a5f84a-66af-4e17-bc4e-2ca871708f7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    output_df.write.mode(\"append\") # \"overwrite\", \"append\"\n",
    "    .option(\"overwriteSchema\", \"true\")    \n",
    "    .saveAsTable(params[\"INFERENCE_SUMMARY_TABLE\"])\n",
    ")\n",
    "del inference_rv, inference_rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98afd0c7-00b0-46a1-ad1b-58ce3f7e94e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "grouped_df = output_df.groupBy(\n",
    "    \"pms_sync_off\", \"target_type\", \"HotelID\", \"day_ahead_index\", \"booking_index\"\n",
    ").agg(*[F.avg(c).alias(c) for c in output_df.columns if 'SAPE' in c])\n",
    "\n",
    "mean_sapes_df = grouped_df.select(\n",
    "    \"pms_sync_off\", \"target_type\", \"HotelID\", \"day_ahead_index\", \"booking_index\",\n",
    "    *[c for c in grouped_df.columns if 'SAPE' in c]\n",
    ")\n",
    "\n",
    "melted_mean_sapes_df = mean_sapes_df.selectExpr(\n",
    "    \"pms_sync_off\", \n",
    "    \"target_type\", \n",
    "    \"HotelID\", \n",
    "    \"day_ahead_index\", \n",
    "    \"booking_index\", \n",
    "    \"stack(\" +\n",
    "        f\"{len([c for c in mean_sapes_df.columns if 'SAPE' in c])}, \" +\n",
    "        \", \".join([f\"'{c}', {c}\" for c in mean_sapes_df.columns if 'SAPE' in c]) +\n",
    "    \") as (variable, value)\"\n",
    ")\n",
    "\n",
    "melted_mean_sapes_df = melted_mean_sapes_df\\\n",
    "    .withColumn(\"eval_start_date\", F.lit(eval_period_start))\\\n",
    "    .withColumn(\"eval_end_date\", F.lit(eval_period_end))\\\n",
    "    .withColumn(\"Timestamp\", F.current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96fe7bcf-fc7c-43e1-86a2-ad68fdc838d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    melted_mean_sapes_df.write.mode(\"append\") # \"overwrite\", \"append\"\n",
    "    .option(\"overwriteSchema\", \"true\")    \n",
    "    .saveAsTable(params[\"INFERENCE_TRIANGULAR_METRIC_TABLE\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5406767d-1a89-48a9-88a8-9ff40e757a4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define a custom colormap with red (low), yellow (middle), and green (high)\n",
    "if params[\"PLOT_HOTELS\"]:\n",
    "    melted_mean_sapes_df_pd = melted_mean_sapes_df.toPandas()\n",
    "    colors = [(0, \"green\"), (0.40, \"yellow\"),(1, \"red\")]\n",
    "    custom_palette = LinearSegmentedColormap.from_list(\"custom_palette\", colors)\n",
    "\n",
    "    target_df = melted_mean_sapes_df_pd[(melted_mean_sapes_df_pd.pms_sync_off==False) & (melted_mean_sapes_df_pd.target_type=='REVENUE')]\n",
    "\n",
    "    for hid in target_df.HotelID.unique():\n",
    "        check_triangle = target_df[(target_df.HotelID==hid)]\n",
    "\n",
    "        VMAX=check_triangle.groupby(['HotelID']).agg({'value':['min','max']}).reset_index()[('value','max')][0]\n",
    "        VMIN=check_triangle.groupby(['HotelID']).agg({'value':['min','max']}).reset_index()[('value','min')][0]\n",
    "\n",
    "        different_plots = sorted(check_triangle.variable.unique())\n",
    "        total_plots = len(different_plots)#len([i for i in range(1,DAY_AHEAD+1)])\n",
    "        plots_per_row = target_df.variable.nunique()\n",
    "        rows_needed = int(np.ceil(total_plots/plots_per_row))\n",
    "\n",
    "        pointer = 0\n",
    "        for row in range(rows_needed):\n",
    "            fig,ax = plt.subplots(nrows=1,ncols=plots_per_row, figsize=(30,6))\n",
    "            for col in range(plots_per_row):\n",
    "                if pointer<total_plots:\n",
    "                    check_variant = check_triangle[check_triangle.variable==different_plots[pointer]]\n",
    "                    df_plot_inv = check_variant[['day_ahead_index','booking_index','value']].pivot(index='booking_index', columns='day_ahead_index', values='value')\n",
    "                    df_plot_inv = df_plot_inv.sort_index(ascending=False)\n",
    "                    sns.heatmap(df_plot_inv,cmap=custom_palette, vmin=VMIN, vmax=VMAX)\n",
    "                    ax.title.set_text(f' Hotel: {hid} | variant: {different_plots[pointer]}')\n",
    "                    pointer+=1"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "static_inference_metric_monitoring",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}