{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7591516-5b3c-48f8-925c-6badce361be3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d78aa31-8eb5-4f58-aae2-eed535ee91ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.dropdown(\"env_stage\", \"dev\", [\"dev\", \"prod\", \"qa\",\"dev_synxis_2_0\", \"prod_synxis_2_0\", \"qa_synxis_2_0\"], \"Pipeline stage\")\n",
    "dbutils.widgets.dropdown(\"source_catalog\", \"phg_data\", [\"dev_data\", \"qa_data\",\"phg_data\"], \"Source Catalog\")\n",
    "dbutils.widgets.dropdown(\"exclude_pms\", \"False\", [\"True\", \"False\"], \"Exclude PMS\")\n",
    "dbutils.widgets.dropdown(\"target_type\", \"REVENUE\", [\"REVENUE\", \"ROOMS\"], \"Target Type\")\n",
    "dbutils.widgets.dropdown(\"is_usd_currency\", \"True\", [\"True\", \"False\"], \"Use USD currency\")\n",
    "dbutils.widgets.text(\"selected_hotels\", \"\", \"Hotels\")\n",
    "dbutils.widgets.text(\"lag_numbers\",\"1,7,14,28\", \"Lag Numbers\")\n",
    "dbutils.widgets.text(\"model_start_date\", \"2018-10-01\", \"Model Start Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f087ce5-2b19-4ff1-8ca6-4ad8af37db44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import holidays\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import os\n",
    "from sys import version_info\n",
    "import cloudpickle\n",
    "from autogluon.core.utils.loaders import load_pkl\n",
    "import logging\n",
    "import shutil\n",
    "import mlflow\n",
    "from mlflow import MlflowException\n",
    "import mlflow.pyfunc\n",
    "import time\n",
    "import warnings\n",
    "import pyspark.sql.functions as F\n",
    "import sys\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "start_time = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92e12f90-2feb-4308-9595-af77d11ee5ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from phgml.data.processing_distr_ca import filter_hotels, filter_test_partition\n",
    "from phgml.data.processing_distr_spark import (\n",
    "    filter_data,\n",
    "    preprocess_data,\n",
    "    get_lags,\n",
    "    compile_hotel_tables,\n",
    ")\n",
    "from phgml.data.data_types import inference_output_schema\n",
    "from phgml.reporting.logging import get_logging_path,get_logging_filename\n",
    "from phgml.data.config import ForecastingHotelConfigProvider,EnvironmentConfig\n",
    "from phgml.utilities.task_utilities import str_to_lst, str_to_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a14bca8-16bd-4bf3-b8bb-9a16eb3a72e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Disable adaptive query optimization\n",
    "# Adaptive query optimization groups together smaller tasks into a larger tasks.\n",
    "# This may result in limited parallelism if the parallel inference tasks are deemed to be too small by the query optimizer\n",
    "# We are diableing AQE here to circumevent this limitation on parallelism\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4b86c0e-18dd-4e17-8573-c8cf2b9b7d0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "params = {}\n",
    "params[\"ENV\"] = getArgument(\"env_stage\")\n",
    "params[\"SOURCE_CATALOG\"] = getArgument(\"source_catalog\")\n",
    "params[\"REVENUE_COL\"] = \"_reservationRevenuePerRoomUSD\"\n",
    "params[\"ROOMS_COL\"] = \"_rooms\"\n",
    "params[\"PIPELINE\"] = \"INFERENCE\"\n",
    "params[\"WITHOUT_PMS\"] = str_to_bool(getArgument(\"exclude_pms\"))\n",
    "params[\"IS_USD_CURRENCY\"] = str_to_bool(getArgument(\"is_usd_currency\"))\n",
    "params[\"TARGET_TYPE\"] = getArgument(\"target_type\")\n",
    "params[\"SELECTED_HOTELS\"] = str_to_lst(getArgument(\"selected_hotels\"))\n",
    "params[\"LAG_NUMBERS\"] = list(map(int,str_to_lst(getArgument(\"lag_numbers\"))))\n",
    "params[\"COVID_START_DATE\"] = pd.to_datetime(\"2020-03-01\")\n",
    "params[\"COVID_END_DATE\"] = pd.to_datetime(\"2021-08-01\")\n",
    "params[\"MODEL_START_DATE\"] = pd.to_datetime(getArgument(\"model_start_date\"))\n",
    "params[\"CALC_UNCERTAINTY\"] = False\n",
    "params['LOG_ROOT'] = '/dbfs/mnt/extractionlogs/synxis'\n",
    "\n",
    "cluster_name = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterName\") \n",
    "\n",
    "if \"synxis_2_0\" in params[\"ENV\"]:\n",
    "    params['LOG_ROOT'] = '/dbfs/mnt/extractionlogs/synxis_2_0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f61154b8-ab9e-4004-8fc3-308c97ef3bee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "env_config = EnvironmentConfig(env=params[\"ENV\"], target=params[\"TARGET_TYPE\"], spark=spark, is_usd_currency=params[\"IS_USD_CURRENCY\"])\n",
    "forecasting_config_provider = ForecastingHotelConfigProvider(spark=spark,env=params[\"ENV\"])\n",
    "params[\"TARGET_COLUMN\"] = env_config.target_column\n",
    "\n",
    "for handler in logging.root.handlers[:]:\n",
    "        logging.root.removeHandler(handler)\n",
    "        \n",
    "logging.root.setLevel(logging.INFO)\n",
    "\n",
    "processing_timestamp = datetime.now()\n",
    "\n",
    "logfile_path = get_logging_path(params['LOG_ROOT'],processing_timestamp)\n",
    "if not os.path.exists(logfile_path):\n",
    "    os.makedirs(logfile_path)\n",
    "\n",
    "pms = \"PMS\"\n",
    "if params['WITHOUT_PMS']:\n",
    "    pms = \"NOPMS\"\n",
    "        \n",
    "log_file_name = get_logging_filename(\n",
    "    logfile_path,\n",
    "    \"PREPROCESS\",\n",
    "    params['TARGET_TYPE'],\n",
    "    pms,\n",
    "    processing_timestamp)\n",
    "\n",
    "logger = logging.getLogger(f\"preprocess-{params['TARGET_TYPE']}-{pms}\")\n",
    "\n",
    "file_handler = logging.FileHandler(log_file_name)\n",
    "file_format = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(file_format)\n",
    "\n",
    "logger.addHandler(file_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f719d1a-de8e-4563-831c-ce90a688da0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# As a workaround for the bug PHG-2157\n",
    "params[\"PARTITION_DATE\"] = spark.sql(\n",
    "    f\"select max(confirmationDate) from {env_config.source_data_table}\"\n",
    ").collect()[0][0]\n",
    "\n",
    "print(f\"Partition date: {params['PARTITION_DATE']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acc52ff7-7e61-422d-9b6b-9363c537323f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "max_inference_length = spark.sql(f'select max(inference_prediction_length) from {forecasting_config_provider.config_table_name}').collect()[0][0]\n",
    "max_lead_window = spark.sql(f'select max(lead_window) from {forecasting_config_provider.config_table_name}').collect()[0][0]\n",
    "params[\"TEST_PARTIITON_END\"] = params[\"PARTITION_DATE\"] + pd.Timedelta(max_inference_length, \"D\")\n",
    "\n",
    "print(f\"Partition end date: {params['TEST_PARTIITON_END']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "510ac03f-dfc0-4868-8cd5-2ba8fb42066a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Executing pipeline stage: {params['ENV']}\")\n",
    "print(f\"Processing data for target type: {params['TARGET_TYPE']} : {params['TARGET_COLUMN']}\")\n",
    "print(f\"Intermediate inference results table name: {env_config.inference_intermediate_table }\")\n",
    "print(f\"Writing inference results to table: {env_config.inference_output_table } with blob {env_config.inference_output_table_blob}\")\n",
    "print(f\"Excluding PMS data? {params['WITHOUT_PMS']}\")\n",
    "\n",
    "logger.info(f\"Executing pipeline stage: {params['ENV']}\")\n",
    "logger.info(f\"Processing data for target type: {params['TARGET_TYPE']} : {params['TARGET_COLUMN']}\")\n",
    "logger.info(f\"Intermediate inference results table name: {env_config.inference_intermediate_table }\")\n",
    "logger.info(f\"Writing inference results to table: {env_config.inference_output_table } with blob {env_config.inference_output_table_blob}\")\n",
    "logger.info(f\"Excluding PMS data? {params['WITHOUT_PMS']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b3aaeea4-71f5-473d-86d9-f6036f551005",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logger.info(\"Selecting hotels.\")\n",
    "\n",
    "hotel_details = spark.sql(\n",
    "    f\"select HotelID,HotelName,PMSStartDate,Country,State from {params['SOURCE_CATALOG']}.dim_hotels_data\"\n",
    ").toPandas()\n",
    "\n",
    "# Not considering state info other countries other than US and Canada for date features\n",
    "hotel_details.loc[~hotel_details.Country.isin(['US','CA']), \"State\"] = \"N/A\"\n",
    "hotel_details = hotel_details[~hotel_details.HotelID.isna()]\n",
    "\n",
    "#Filter hotels \n",
    "correct_hotel_ids = filter_hotels(\n",
    "    hotel_details,\n",
    "    params[\"SELECTED_HOTELS\"],\n",
    "    params[\"WITHOUT_PMS\"],\n",
    "    forecasting_config_provider\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49778d76-af25-4cb7-b18c-2b20b07be985",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logger.info(\"Loading data\")\n",
    "# Select transaction data along with the cancellation data from the raw dataset\n",
    "if \"synxis_2_0\" in params['ENV']:\n",
    "    dfsp_src = spark.sql(\n",
    "        f\"select * from {env_config.source_data_table}\"\n",
    "        )\n",
    "    \n",
    "    dfsp_src = dfsp_src.withColumn(\n",
    "        'cancellationDate',\n",
    "        F.when(F.col('status') == 'No-show', F.col('_StayDates')).otherwise(F.col('cancellationDate'))\n",
    "    )\n",
    "else:\n",
    "    dfsp_src = spark.sql(\n",
    "        f\"select a.TransactionID,a.HotelID,a._StayDates,a.confirmationDate,a.departureDate,a.channel,a.status,a.cancellationNumber,a._reservationRevenuePerRoomUSD,a._rooms,b.cancellationDate from {env_config.source_data_table} as a left join {env_config.transaction_data_table} as b on a.TransactionID=b.TransactionID\"\n",
    "        )\n",
    "\n",
    "dfsp_src = dfsp_src.filter(dfsp_src.HotelID.isin(correct_hotel_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f9b68d1-ee08-4209-b8d6-5b3228642036",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load booking statuses \n",
    "result = spark.sql(\"SELECT * FROM phg_data.bookings_status\")\n",
    "\n",
    "confirmed_status_list = [row['status'] for row in result.filter(result.scenario == 'confirmed').collect()]\n",
    "cancelled_status_list = [row['status'] for row in result.filter(result.scenario == 'cancelled').collect()]\n",
    "\n",
    "# Display the list\n",
    "print(f\"Confirmed Booking Status List: {confirmed_status_list}\")\n",
    "print(f\"Cancelled Booking Status List: {cancelled_status_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7aaef58-a7bd-4457-8cbc-f4eb985ee2a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"HotelID\",\n",
    "    \"_StayDates\",\n",
    "    \"confirmationDate\",\n",
    "    \"channel\",\n",
    "    \"status\",\n",
    "    params[\"REVENUE_COL\"],\n",
    "    params[\"ROOMS_COL\"],\n",
    "]\n",
    "\n",
    "dfsp = dfsp_src.filter(\n",
    "        (F.col('status').isin(confirmed_status_list)) & (dfsp_src.cancellationDate.isNull())\n",
    "    ).select(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ea398e6-b134-43f6-9ac0-382a4a179572",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logger.info(\"Preprocessing data\")\n",
    "df = preprocess_data(\n",
    "    dfsp,\n",
    "    params[\"WITHOUT_PMS\"],\n",
    "    params[\"REVENUE_COL\"],\n",
    "    params[\"ROOMS_COL\"],\n",
    "    params[\"MODEL_START_DATE\"],\n",
    "    cancel_aware=False\n",
    ")\n",
    "\n",
    "logger.info(\"Calculating date features\")\n",
    "partition_start_date = params[\"PARTITION_DATE\"]\n",
    "partition_end_date = params[\"TEST_PARTIITON_END\"]\n",
    "dates = spark.sql(f\"select * from phg_data.date_features where date >= '{partition_start_date}' and date <= '{partition_end_date}'\")\n",
    "dates = dates.withColumn('date', F.to_date('date'))\n",
    "dates = dates.withColumnRenamed(\"date\",\"_StayDates\")\n",
    "dates = dates.join(\n",
    "    spark.createDataFrame(hotel_details[[\"HotelID\",\"Country\",\"State\"]]), \n",
    "    on=['Country','State'],\n",
    "     how=\"inner\")\n",
    "\n",
    "logger.info(\"Calculating lag features\")\n",
    "df_lags = get_lags(\n",
    "    df,\n",
    "    lag_numbers=params[\"LAG_NUMBERS\"], \n",
    "    target_col=params[\"TARGET_COLUMN\"]\n",
    ")\n",
    "\n",
    "logger.info(\"Filtering test partition\")\n",
    "df = filter_test_partition(\n",
    "    df=df,\n",
    "    partition_start=params[\"PARTITION_DATE\"],\n",
    "    partition_end=params[\"TEST_PARTIITON_END\"],\n",
    "    revenue_col=params[\"REVENUE_COL\"],\n",
    "    rooms_col=params[\"ROOMS_COL\"],\n",
    "    cancel_aware=False\n",
    ").orderBy([\"HotelID\", \"_StayDates\",\"confirmationDate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4adb9329-812d-43b4-aef6-f093cb710f86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logger.info(\"Compiling test data set\")\n",
    "output_df = compile_hotel_tables(\n",
    "    df=df,\n",
    "    target_type=params[\"TARGET_TYPE\"],\n",
    "    target_column=params[\"TARGET_COLUMN\"],\n",
    "    prediction_horizon=max_inference_length,\n",
    "    lead_window=max_lead_window,\n",
    "    selected_hotels=correct_hotel_ids,\n",
    "    dates=dates,\n",
    "    df_lags=df_lags,\n",
    "    spark=spark,\n",
    "    cancel_aware=False,\n",
    "    suffix=\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4041497-f475-457a-8263-0220cf398d89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logger.info(\n",
    "    f\"Writing preprocess data to temporary table {env_config.preprocess_intermediate_table}\"\n",
    ")\n",
    "(\n",
    "    output_df.write.mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(env_config.preprocess_intermediate_table)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "156d6e5b-c102-46b4-b0b6-32d1c5888e47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "elapsed_time = time.perf_counter() - start_time\n",
    "logger.info(f\"Time elapsed {elapsed_time}\")\n",
    "logger.info(f\"Time elapsed in minutes {elapsed_time/60}\")\n",
    "print(f\"Time elapsed in minutes {elapsed_time/60}\")\n",
    "logger.info(\"Preprocessing completed.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "forecast_inference_preprocessing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}