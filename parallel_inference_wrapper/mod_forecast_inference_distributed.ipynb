{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d0cfc3e-dfae-47ed-bb91-70c147eec77e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Python interpreter will be restarted.\n",
       "Requirement already satisfied: mlflow in /databricks/python3/lib/python3.8/site-packages (2.2.2)\n",
       "Requirement already satisfied: scipy&lt;2 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.6.2)\n",
       "Requirement already satisfied: requests&lt;3,&gt;=2.17.3 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (2.32.3)\n",
       "Requirement already satisfied: entrypoints&lt;1 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (0.3)\n",
       "Requirement already satisfied: markdown&lt;4,&gt;=3.3 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (3.7)\n",
       "Requirement already satisfied: cloudpickle&lt;3 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (2.2.1)\n",
       "Requirement already satisfied: scikit-learn&lt;2 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.0.2)\n",
       "Requirement already satisfied: shap&lt;1,&gt;=0.40 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (0.44.1)\n",
       "Requirement already satisfied: pytz&lt;2023 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (2020.5)\n",
       "Requirement already satisfied: pyyaml&lt;7,&gt;=5.1 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (6.0.2)\n",
       "Requirement already satisfied: docker&lt;7,&gt;=4.0.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (6.1.3)\n",
       "Requirement already satisfied: pandas&lt;3 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.4.4)\n",
       "Requirement already satisfied: querystring-parser&lt;2 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.2.4)\n",
       "Requirement already satisfied: numpy&lt;2 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.22.4)\n",
       "Requirement already satisfied: importlib-metadata!=4.7.0,&lt;7,&gt;=3.7.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (6.11.0)\n",
       "Requirement already satisfied: protobuf&lt;5,&gt;=3.12.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (3.17.2)\n",
       "Requirement already satisfied: pyarrow&lt;12,&gt;=4.0.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (4.0.0)\n",
       "Requirement already satisfied: Flask&lt;3 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (2.3.3)\n",
       "Requirement already satisfied: packaging&lt;24 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (23.2)\n",
       "Requirement already satisfied: databricks-cli&lt;1,&gt;=0.8.7 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (0.18.0)\n",
       "Requirement already satisfied: alembic&lt;2 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.13.3)\n",
       "Requirement already satisfied: sqlparse&lt;1,&gt;=0.4.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (0.5.1)\n",
       "Requirement already satisfied: matplotlib&lt;4 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (3.4.2)\n",
       "Requirement already satisfied: gunicorn&lt;21 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (20.1.0)\n",
       "Requirement already satisfied: Jinja2&lt;4,&gt;=2.11 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (3.1.4)\n",
       "Requirement already satisfied: gitpython&lt;4,&gt;=2.1.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (3.1.43)\n",
       "Requirement already satisfied: click&lt;9,&gt;=7.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (8.1.7)\n",
       "Requirement already satisfied: sqlalchemy&lt;3,&gt;=1.4.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (2.0.35)\n",
       "Requirement already satisfied: importlib-resources in /databricks/python3/lib/python3.8/site-packages (from alembic&lt;2-&gt;mlflow) (6.4.5)\n",
       "Requirement already satisfied: typing-extensions&gt;=4 in /databricks/python3/lib/python3.8/site-packages (from alembic&lt;2-&gt;mlflow) (4.12.2)\n",
       "Requirement already satisfied: Mako in /databricks/python3/lib/python3.8/site-packages (from alembic&lt;2-&gt;mlflow) (1.3.5)\n",
       "Requirement already satisfied: oauthlib&gt;=3.1.0 in /databricks/python3/lib/python3.8/site-packages (from databricks-cli&lt;1,&gt;=0.8.7-&gt;mlflow) (3.2.2)\n",
       "Requirement already satisfied: pyjwt&gt;=1.7.0 in /databricks/python3/lib/python3.8/site-packages (from databricks-cli&lt;1,&gt;=0.8.7-&gt;mlflow) (2.9.0)\n",
       "Requirement already satisfied: urllib3&lt;3,&gt;=1.26.7 in /databricks/python3/lib/python3.8/site-packages (from databricks-cli&lt;1,&gt;=0.8.7-&gt;mlflow) (1.26.15)\n",
       "Requirement already satisfied: tabulate&gt;=0.7.7 in /databricks/python3/lib/python3.8/site-packages (from databricks-cli&lt;1,&gt;=0.8.7-&gt;mlflow) (0.9.0)\n",
       "Requirement already satisfied: six&gt;=1.10.0 in /databricks/python3/lib/python3.8/site-packages (from databricks-cli&lt;1,&gt;=0.8.7-&gt;mlflow) (1.15.0)\n",
       "Requirement already satisfied: websocket-client&gt;=0.32.0 in /databricks/python3/lib/python3.8/site-packages (from docker&lt;7,&gt;=4.0.0-&gt;mlflow) (1.8.0)\n",
       "Requirement already satisfied: blinker&gt;=1.6.2 in /databricks/python3/lib/python3.8/site-packages (from Flask&lt;3-&gt;mlflow) (1.8.2)\n",
       "Requirement already satisfied: Werkzeug&gt;=2.3.7 in /databricks/python3/lib/python3.8/site-packages (from Flask&lt;3-&gt;mlflow) (3.0.4)\n",
       "Requirement already satisfied: itsdangerous&gt;=2.1.2 in /databricks/python3/lib/python3.8/site-packages (from Flask&lt;3-&gt;mlflow) (2.2.0)\n",
       "Requirement already satisfied: gitdb&lt;5,&gt;=4.0.1 in /databricks/python3/lib/python3.8/site-packages (from gitpython&lt;4,&gt;=2.1.0-&gt;mlflow) (4.0.11)\n",
       "Requirement already satisfied: smmap&lt;6,&gt;=3.0.1 in /databricks/python3/lib/python3.8/site-packages (from gitdb&lt;5,&gt;=4.0.1-&gt;gitpython&lt;4,&gt;=2.1.0-&gt;mlflow) (5.0.1)\n",
       "Requirement already satisfied: setuptools&gt;=3.0 in /usr/local/lib/python3.8/dist-packages (from gunicorn&lt;21-&gt;mlflow) (52.0.0)\n",
       "Requirement already satisfied: zipp&gt;=0.5 in /databricks/python3/lib/python3.8/site-packages (from importlib-metadata!=4.7.0,&lt;7,&gt;=3.7.0-&gt;mlflow) (3.20.2)\n",
       "Requirement already satisfied: MarkupSafe&gt;=2.0 in /databricks/python3/lib/python3.8/site-packages (from Jinja2&lt;4,&gt;=2.11-&gt;mlflow) (2.1.5)\n",
       "Requirement already satisfied: cycler&gt;=0.10 in /databricks/python3/lib/python3.8/site-packages (from matplotlib&lt;4-&gt;mlflow) (0.10.0)\n",
       "Requirement already satisfied: pillow&gt;=6.2.0 in /databricks/python3/lib/python3.8/site-packages (from matplotlib&lt;4-&gt;mlflow) (8.2.0)\n",
       "Requirement already satisfied: pyparsing&gt;=2.2.1 in /databricks/python3/lib/python3.8/site-packages (from matplotlib&lt;4-&gt;mlflow) (2.4.7)\n",
       "Requirement already satisfied: kiwisolver&gt;=1.0.1 in /databricks/python3/lib/python3.8/site-packages (from matplotlib&lt;4-&gt;mlflow) (1.3.1)\n",
       "Requirement already satisfied: python-dateutil&gt;=2.7 in /databricks/python3/lib/python3.8/site-packages (from matplotlib&lt;4-&gt;mlflow) (2.8.1)\n",
       "Requirement already satisfied: certifi&gt;=2017.4.17 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.17.3-&gt;mlflow) (2020.12.5)\n",
       "Requirement already satisfied: idna&lt;4,&gt;=2.5 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.17.3-&gt;mlflow) (2.10)\n",
       "Requirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.17.3-&gt;mlflow) (3.4.0)\n",
       "Requirement already satisfied: joblib&gt;=0.11 in /databricks/python3/lib/python3.8/site-packages (from scikit-learn&lt;2-&gt;mlflow) (1.0.1)\n",
       "Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /databricks/python3/lib/python3.8/site-packages (from scikit-learn&lt;2-&gt;mlflow) (2.1.0)\n",
       "Requirement already satisfied: numba in /databricks/python3/lib/python3.8/site-packages (from shap&lt;1,&gt;=0.40-&gt;mlflow) (0.58.1)\n",
       "Requirement already satisfied: tqdm&gt;=4.27.0 in /databricks/python3/lib/python3.8/site-packages (from shap&lt;1,&gt;=0.40-&gt;mlflow) (4.66.5)\n",
       "Requirement already satisfied: slicer==0.0.7 in /databricks/python3/lib/python3.8/site-packages (from shap&lt;1,&gt;=0.40-&gt;mlflow) (0.0.7)\n",
       "Requirement already satisfied: greenlet!=0.4.17 in /databricks/python3/lib/python3.8/site-packages (from sqlalchemy&lt;3,&gt;=1.4.0-&gt;mlflow) (3.1.1)\n",
       "Requirement already satisfied: llvmlite&lt;0.42,&gt;=0.41.0dev0 in /databricks/python3/lib/python3.8/site-packages (from numba-&gt;shap&lt;1,&gt;=0.40-&gt;mlflow) (0.41.1)\n",
       "Python interpreter will be restarted.\n",
       "Python interpreter will be restarted.\n",
       "Collecting autogluon==0.6.0\n",
       "  Using cached autogluon-0.6.0-py3-none-any.whl (9.8 kB)\n",
       "Collecting autogluon.tabular[all]==0.6.0\n",
       "  Using cached autogluon.tabular-0.6.0-py3-none-any.whl (285 kB)\n",
       "Collecting autogluon.vision==0.6.0\n",
       "  Using cached autogluon.vision-0.6.0-py3-none-any.whl (49 kB)\n",
       "Collecting autogluon.timeseries[all]==0.6.0\n",
       "  Using cached autogluon.timeseries-0.6.0-py3-none-any.whl (101 kB)\n",
       "Collecting autogluon.text==0.6.0\n",
       "  Using cached autogluon.text-0.6.0-py3-none-any.whl (62 kB)\n",
       "Collecting autogluon.multimodal==0.6.0\n",
       "  Using cached autogluon.multimodal-0.6.0-py3-none-any.whl (279 kB)\n",
       "Collecting autogluon.core[all]==0.6.0\n",
       "  Using cached autogluon.core-0.6.0-py3-none-any.whl (224 kB)\n",
       "Collecting autogluon.features==0.6.0\n",
       "  Using cached autogluon.features-0.6.0-py3-none-any.whl (59 kB)\n",
       "Requirement already satisfied: dask&lt;=2021.11.2,&gt;=2021.09.1 in /databricks/python3/lib/python3.8/site-packages (from autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (2021.11.2)\n",
       "Requirement already satisfied: scikit-learn&lt;1.2,&gt;=1.0.0 in /databricks/python3/lib/python3.8/site-packages (from autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (1.0.2)\n",
       "Requirement already satisfied: distributed&lt;=2021.11.2,&gt;=2021.09.1 in /databricks/python3/lib/python3.8/site-packages (from autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (2021.11.2)\n",
       "Requirement already satisfied: pandas!=1.4.0,&lt;1.6,&gt;=1.2.5 in /databricks/python3/lib/python3.8/site-packages (from autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (1.4.4)\n",
       "Collecting autogluon.common==0.6.0\n",
       "  Using cached autogluon.common-0.6.0-py3-none-any.whl (41 kB)\n",
       "Requirement already satisfied: scipy&lt;1.10.0,&gt;=1.5.4 in /databricks/python3/lib/python3.8/site-packages (from autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (1.6.2)\n",
       "Requirement already satisfied: tqdm&gt;=4.38.0 in /databricks/python3/lib/python3.8/site-packages (from autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (4.66.5)\n",
       "Requirement already satisfied: requests in /databricks/python3/lib/python3.8/site-packages (from autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (2.32.3)\n",
       "Requirement already satisfied: numpy&lt;1.24,&gt;=1.21 in /databricks/python3/lib/python3.8/site-packages (from autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (1.22.4)\n",
       "Requirement already satisfied: matplotlib in /databricks/python3/lib/python3.8/site-packages (from autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (3.4.2)\n",
       "Requirement already satisfied: boto3 in /databricks/python3/lib/python3.8/site-packages (from autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (1.16.7)\n",
       "Collecting ray[tune]&lt;2.1,&gt;=2.0\n",
       "  Using cached ray-2.0.1-cp38-cp38-manylinux2014_x86_64.whl (60.2 MB)\n",
       "Collecting hyperopt&lt;0.2.8,&gt;=0.2.7\n",
       "  Using cached hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
       "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from autogluon.common==0.6.0-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (52.0.0)\n",
       "Requirement already satisfied: psutil&lt;6,&gt;=5.7.3 in /databricks/python3/lib/python3.8/site-packages (from autogluon.features==0.6.0-&gt;autogluon==0.6.0) (5.9.8)\n",
       "Collecting torch&lt;1.13,&gt;=1.9\n",
       "  Using cached torch-1.12.1-cp38-cp38-manylinux1_x86_64.whl (776.3 MB)\n",
       "Collecting sentencepiece&lt;0.2.0,&gt;=0.1.95\n",
       "  Using cached sentencepiece-0.1.99-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
       "Collecting evaluate&lt;=0.2.2\n",
       "  Using cached evaluate-0.2.2-py3-none-any.whl (69 kB)\n",
       "Collecting pycocotools&lt;2.0.7,&gt;=2.0.5\n",
       "  Using cached pycocotools-2.0.6-cp38-cp38-linux_x86_64.whl\n",
       "Collecting timm&lt;0.7.0\n",
       "  Using cached timm-0.6.13-py3-none-any.whl (549 kB)\n",
       "Collecting pytorch-metric-learning&lt;1.4.0,&gt;=1.3.0\n",
       "  Using cached pytorch_metric_learning-1.3.2-py3-none-any.whl (109 kB)\n",
       "Collecting text-unidecode&lt;=1.3\n",
       "  Using cached text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
       "Collecting torchmetrics&lt;0.9.0,&gt;=0.8.0\n",
       "  Using cached torchmetrics-0.8.2-py3-none-any.whl (409 kB)\n",
       "Collecting torchtext&lt;0.14.0\n",
       "  Using cached torchtext-0.13.1-cp38-cp38-manylinux1_x86_64.whl (1.9 MB)\n",
       "Collecting nlpaug&lt;=1.1.10,&gt;=1.1.10\n",
       "  Using cached nlpaug-1.1.10-py3-none-any.whl (410 kB)\n",
       "Collecting Pillow&lt;9.1.0,&gt;=9.0.1\n",
       "  Using cached Pillow-9.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
       "Collecting nltk&lt;4.0.0,&gt;=3.4.5\n",
       "  Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
       "Collecting accelerate&lt;0.14,&gt;=0.9\n",
       "  Using cached accelerate-0.13.2-py3-none-any.whl (148 kB)\n",
       "Collecting nptyping&lt;1.5.0,&gt;=1.4.4\n",
       "  Using cached nptyping-1.4.4-py3-none-any.whl (31 kB)\n",
       "Collecting fairscale&lt;=0.4.6,&gt;=0.4.5\n",
       "  Using cached fairscale-0.4.6-py3-none-any.whl\n",
       "Collecting scikit-image&lt;0.20.0,&gt;=0.19.1\n",
       "  Using cached scikit_image-0.19.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
       "Collecting seqeval&lt;=1.2.2\n",
       "  Using cached seqeval-1.2.2-py3-none-any.whl\n",
       "Collecting smart-open&lt;5.3.0,&gt;=5.2.1\n",
       "  Using cached smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
       "Collecting torchvision&lt;0.14.0\n",
       "  Using cached torchvision-0.13.1-cp38-cp38-manylinux1_x86_64.whl (19.1 MB)\n",
       "Collecting transformers&lt;4.24.0,&gt;=4.23.0\n",
       "  Using cached transformers-4.23.1-py3-none-any.whl (5.3 MB)\n",
       "Requirement already satisfied: defusedxml&lt;=0.7.1,&gt;=0.7.1 in /databricks/python3/lib/python3.8/site-packages (from autogluon.multimodal==0.6.0-&gt;autogluon==0.6.0) (0.7.1)\n",
       "Collecting albumentations&lt;=1.2.0,&gt;=1.1.0\n",
       "  Using cached albumentations-1.2.0-py3-none-any.whl (113 kB)\n",
       "Collecting omegaconf&lt;2.2.0,&gt;=2.1.1\n",
       "  Using cached omegaconf-2.1.2-py3-none-any.whl (74 kB)\n",
       "Collecting pytorch-lightning&lt;1.8.0,&gt;=1.7.4\n",
       "  Using cached pytorch_lightning-1.7.7-py3-none-any.whl (708 kB)\n",
       "Collecting openmim&lt;=0.2.1,&gt;0.1.5\n",
       "  Using cached openmim-0.2.1-py2.py3-none-any.whl (49 kB)\n",
       "Requirement already satisfied: jsonschema&lt;=4.8.0 in /databricks/python3/lib/python3.8/site-packages (from autogluon.multimodal==0.6.0-&gt;autogluon==0.6.0) (3.2.0)\n",
       "Requirement already satisfied: networkx&lt;3.0,&gt;=2.3 in /databricks/python3/lib/python3.8/site-packages (from autogluon.tabular[all]==0.6.0-&gt;autogluon==0.6.0) (2.8.8)\n",
       "Collecting lightgbm&lt;3.4,&gt;=3.3\n",
       "  Using cached lightgbm-3.3.5-py3-none-manylinux1_x86_64.whl (2.0 MB)\n",
       "Collecting fastai&lt;2.8,&gt;=2.3.1\n",
       "  Using cached fastai-2.7.17-py3-none-any.whl (234 kB)\n",
       "Collecting catboost&lt;1.2,&gt;=1.0\n",
       "  Using cached catboost-1.1.1-cp38-none-manylinux1_x86_64.whl (76.6 MB)\n",
       "Collecting xgboost&lt;1.8,&gt;=1.6\n",
       "  Using cached xgboost-1.7.6-py3-none-manylinux2014_x86_64.whl (200.3 MB)\n",
       "Collecting gluonts~=0.11.0\n",
       "  Using cached gluonts-0.11.12-py3-none-any.whl (1.0 MB)\n",
       "Collecting psutil&lt;6,&gt;=5.7.3\n",
       "  Using cached psutil-5.8.0-cp38-cp38-manylinux2010_x86_64.whl (296 kB)\n",
       "Collecting joblib~=1.1\n",
       "  Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
       "Collecting statsmodels~=0.13.0\n",
       "  Using cached statsmodels-0.13.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.9 MB)\n",
       "Collecting pmdarima~=1.8.2\n",
       "  Using cached pmdarima-1.8.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.5 MB)\n",
       "Requirement already satisfied: sktime&lt;0.14,&gt;=0.13.1 in /databricks/python3/lib/python3.8/site-packages (from autogluon.timeseries[all]==0.6.0-&gt;autogluon==0.6.0) (0.13.2)\n",
       "Collecting tbats~=1.1\n",
       "  Using cached tbats-1.1.3-py3-none-any.whl (44 kB)\n",
       "Collecting gluoncv&lt;0.10.6,&gt;=0.10.5\n",
       "  Using cached gluoncv-0.10.5.post0-py2.py3-none-any.whl (1.3 MB)\n",
       "Requirement already satisfied: pyyaml in /databricks/python3/lib/python3.8/site-packages (from accelerate&lt;0.14,&gt;=0.9-&gt;autogluon.multimodal==0.6.0-&gt;autogluon==0.6.0) (6.0.2)\n",
       "Requirement already satisfied: packaging&gt;=20.0 in /databricks/python3/lib/python3.8/site-packages (from accelerate&lt;0.14,&gt;=0.9-&gt;autogluon.multimodal==0.6.0-&gt;autogluon==0.6.0) (23.2)\n",
       "Collecting qudida&gt;=0.0.4\n",
       "  Using cached qudida-0.0.4-py3-none-any.whl (3.5 kB)\n",
       "Collecting opencv-python-headless&gt;=4.1.1\n",
       "  Using cached opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
       "Collecting albumentations&lt;=1.2.0,&gt;=1.1.0\n",
       "  Using cached albumentations-1.1.0-py3-none-any.whl (102 kB)\n",
       "Requirement already satisfied: plotly in /databricks/python3/lib/python3.8/site-packages (from catboost&lt;1.2,&gt;=1.0-&gt;autogluon.tabular[all]==0.6.0-&gt;autogluon==0.6.0) (5.9.0)\n",
       "Collecting graphviz\n",
       "  Using cached graphviz-0.20.3-py3-none-any.whl (47 kB)\n",
       "Requirement already satisfied: six in /databricks/python3/lib/python3.8/site-packages (from catboost&lt;1.2,&gt;=1.0-&gt;autogluon.tabular[all]==0.6.0-&gt;autogluon==0.6.0) (1.15.0)\n",
       "Requirement already satisfied: toolz&gt;=0.8.2 in /databricks/python3/lib/python3.8/site-packages (from dask&lt;=2021.11.2,&gt;=2021.09.1-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (1.0.0)\n",
       "Requirement already satisfied: cloudpickle&gt;=1.1.1 in /databricks/python3/lib/python3.8/site-packages (from dask&lt;=2021.11.2,&gt;=2021.09.1-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (2.2.1)\n",
       "Requirement already satisfied: fsspec&gt;=0.6.0 in /databricks/python3/lib/python3.8/site-packages (from dask&lt;=2021.11.2,&gt;=2021.09.1-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (2024.9.0)\n",
       "Requirement already satisfied: partd&gt;=0.3.10 in /databricks/python3/lib/python3.8/site-packages (from dask&lt;=2021.11.2,&gt;=2021.09.1-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (1.4.1)\n",
       "Requirement already satisfied: zict&gt;=0.1.3 in /databricks/python3/lib/python3.8/site-packages (from distributed&lt;=2021.11.2,&gt;=2021.09.1-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (3.0.0)\n",
       "Requirement already satisfied: tornado&gt;=6.0.3 in /databricks/python3/lib/python3.8/site-packages (from distributed&lt;=2021.11.2,&gt;=2021.09.1-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (6.1)\n",
       "Requirement already satisfied: tblib&gt;=1.6.0 in /databricks/python3/lib/python3.8/site-packages (from distributed&lt;=2021.11.2,&gt;=2021.09.1-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (3.0.0)\n",
       "Requirement already satisfied: jinja2 in /databricks/python3/lib/python3.8/site-packages (from distributed&lt;=2021.11.2,&gt;=2021.09.1-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (3.1.4)\n",
       "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /databricks/python3/lib/python3.8/site-packages (from distributed&lt;=2021.11.2,&gt;=2021.09.1-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (2.4.0)\n",
       "Requirement already satisfied: click&gt;=6.6 in /databricks/python3/lib/python3.8/site-packages (from distributed&lt;=2021.11.2,&gt;=2021.09.1-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (8.1.7)\n",
       "Requirement already satisfied: msgpack&gt;=0.6.0 in /databricks/python3/lib/python3.8/site-packages (from distributed&lt;=2021.11.2,&gt;=2021.09.1-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (1.1.0)\n",
       "Collecting responses&lt;0.19\n",
       "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
       "Collecting dill\n",
       "  Using cached dill-0.3.9-py3-none-any.whl (119 kB)\n",
       "Collecting datasets&gt;=2.0.0\n",
       "  Using cached datasets-3.0.1-py3-none-any.whl (471 kB)\n",
       "Collecting multiprocess\n",
       "  Using cached multiprocess-0.70.17-py38-none-any.whl (132 kB)\n",
       "Collecting huggingface-hub&gt;=0.7.0\n",
       "  Using cached huggingface_hub-0.25.2-py3-none-any.whl (436 kB)\n",
       "Collecting xxhash\n",
       "  Using cached xxhash-3.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
       "Collecting pyarrow&gt;=15.0.0\n",
       "  Using cached pyarrow-17.0.0-cp38-cp38-manylinux_2_28_x86_64.whl (40.0 MB)\n",
       "Collecting aiohttp\n",
       "  Using cached aiohttp-3.10.10-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
       "Collecting dill\n",
       "  Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
       "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from datasets&gt;=2.0.0-&gt;evaluate&lt;=0.2.2-&gt;autogluon.multimodal==0.6.0-&gt;autogluon==0.6.0) (3.6.0)\n",
       "Collecting fsspec[http]&gt;=2021.05.0\n",
       "  Using cached fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
       "Collecting fastcore&lt;1.8,&gt;=1.5.29\n",
       "  Using cached fastcore-1.7.13-py3-none-any.whl (80 kB)\n",
       "Collecting fastprogress&gt;=0.2.4\n",
       "  Using cached fastprogress-1.0.3-py3-none-any.whl (12 kB)\n",
       "Collecting fastdownload&lt;2,&gt;=0.0.5\n",
       "  Using cached fastdownload-0.0.7-py3-none-any.whl (12 kB)\n",
       "Requirement already satisfied: pip in /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/lib/python3.8/site-packages (from fastai&lt;2.8,&gt;=2.3.1-&gt;autogluon.tabular[all]==0.6.0-&gt;autogluon==0.6.0) (21.0.1)\n",
       "Collecting spacy&lt;4\n",
       "  Using cached spacy-3.8.2.tar.gz (1.3 MB)\n",
       "  Installing build dependencies: started\n",
       "  Installing build dependencies: finished with status &#39;error&#39;\n",
       "  ERROR: Command errored out with exit status 1:\n",
       "   command: /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/bin/python /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/lib/python3.8/site-packages/pip install --ignore-installed --no-user --prefix /tmp/pip-build-env-ux529r2t/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools &#39;cython&gt;=0.25,&lt;3.0&#39; &#39;cymem&gt;=2.0.2,&lt;2.1.0&#39; &#39;preshed&gt;=3.0.2,&lt;3.1.0&#39; &#39;murmurhash&gt;=0.28.0,&lt;1.1.0&#39; &#39;thinc&gt;=8.3.0,&lt;8.4.0&#39; &#39;numpy&gt;=2.0.0,&lt;2.1.0; python_version &lt; &#39;&#34;&#39;&#34;&#39;3.9&#39;&#34;&#39;&#34;&#39;&#39; &#39;numpy&gt;=2.0.0,&lt;2.1.0; python_version &gt;= &#39;&#34;&#39;&#34;&#39;3.9&#39;&#34;&#39;&#34;&#39;&#39;\n",
       "       cwd: None\n",
       "  Complete output (193 lines):\n",
       "  Ignoring numpy: markers &#39;python_version &gt;= &#34;3.9&#34;&#39; don&#39;t match your environment\n",
       "  Collecting setuptools\n",
       "    Using cached setuptools-75.1.0-py3-none-any.whl (1.2 MB)\n",
       "  Collecting cython&lt;3.0,&gt;=0.25\n",
       "    Using cached Cython-0.29.37-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
       "  Collecting cymem&lt;2.1.0,&gt;=2.0.2\n",
       "    Using cached cymem-2.0.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (46 kB)\n",
       "  Collecting preshed&lt;3.1.0,&gt;=3.0.2\n",
       "    Using cached preshed-3.0.9-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (154 kB)\n",
       "  Collecting murmurhash&lt;1.1.0,&gt;=0.28.0\n",
       "    Using cached murmurhash-1.0.10-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
       "  Collecting thinc&lt;8.4.0,&gt;=8.3.0\n",
       "    Using cached thinc-8.3.2.tar.gz (193 kB)\n",
       "    Installing build dependencies: started\n",
       "    Installing build dependencies: finished with status &#39;error&#39;\n",
       "    ERROR: Command errored out with exit status 1:\n",
       "     command: /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/bin/python /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/lib/python3.8/site-packages/pip install --ignore-installed --no-user --prefix /tmp/pip-build-env-sufoo46x/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools &#39;cython&gt;=0.25,&lt;3.0&#39; &#39;murmurhash&gt;=1.0.2,&lt;1.1.0&#39; &#39;cymem&gt;=2.0.2,&lt;2.1.0&#39; &#39;preshed&gt;=3.0.2,&lt;3.1.0&#39; &#39;blis&gt;=1.0.0,&lt;1.1.0&#39; &#39;numpy&gt;=2.0.0,&lt;2.1.0; python_version &lt; &#39;&#34;&#39;&#34;&#39;3.9&#39;&#34;&#39;&#34;&#39;&#39; &#39;numpy&gt;=2.0.0,&lt;2.1.0; python_version &gt;= &#39;&#34;&#39;&#34;&#39;3.9&#39;&#34;&#39;&#34;&#39;&#39;\n",
       "         cwd: None\n",
       "    Complete output (50 lines):\n",
       "    Ignoring numpy: markers &#39;python_version &gt;= &#34;3.9&#34;&#39; don&#39;t match your environment\n",
       "    Collecting setuptools\n",
       "      Using cached setuptools-75.1.0-py3-none-any.whl (1.2 MB)\n",
       "    Collecting cython&lt;3.0,&gt;=0.25\n",
       "      Using cached Cython-0.29.37-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
       "    Collecting murmurhash&lt;1.1.0,&gt;=1.0.2\n",
       "      Using cached murmurhash-1.0.10-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
       "    Collecting cymem&lt;2.1.0,&gt;=2.0.2\n",
       "      Using cached cymem-2.0.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (46 kB)\n",
       "    Collecting preshed&lt;3.1.0,&gt;=3.0.2\n",
       "      Using cached preshed-3.0.9-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (154 kB)\n",
       "    Collecting blis&lt;1.1.0,&gt;=1.0.0\n",
       "      Using cached blis-1.0.1.tar.gz (3.6 MB)\n",
       "      Installing build dependencies: started\n",
       "      Installing build dependencies: finished with status &#39;error&#39;\n",
       "      ERROR: Command errored out with exit status 1:\n",
       "       command: /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/bin/python /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/lib/python3.8/site-packages/pip install --ignore-installed --no-user --prefix /tmp/pip-build-env-8y60wzc5/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools &#39;cython&gt;=0.25&#39; &#39;numpy&gt;=2.0.0,&lt;3.0.0&#39;\n",
       "           cwd: None\n",
       "      Complete output (8 lines):\n",
       "      Collecting setuptools\n",
       "        Using cached setuptools-75.1.0-py3-none-any.whl (1.2 MB)\n",
       "      Collecting cython&gt;=0.25\n",
       "        Using cached Cython-3.0.11-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
       "      ERROR: Could not find a version that satisfies the requirement numpy&lt;3.0.0,&gt;=2.0.0\n",
       "      ERROR: No matching distribution found for numpy&lt;3.0.0,&gt;=2.0.0\n",
       "      WARNING: You are using pip version 21.0.1; however, version 24.2 is available.\n",
       "      You should consider upgrading via the &#39;/local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/bin/python -m pip install --upgrade pip&#39; command.\n",
       "      ----------------------------------------\n",
       "    WARNING: Discarding https://files.pythonhosted.org/packages/bd/e4/741f20c9b767330e2605d4c71a775303cb6a9c72764b8802232fe6c7afad/blis-1.0.1.tar.gz#sha256=91739cd850ca8100dcddbd8ad66942cab20c9473cdea9a35b165b11d7b8d91e4 (from https://pypi.org/simple/blis/). Command errored out with exit status 1: /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/bin/python /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/lib/python3.8/site-packages/pip install --ignore-installed --no-user --prefix /tmp/pip-build-env-8y60wzc5/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools &#39;cython&gt;=0.25&#39; &#39;numpy&gt;=2.0.0,&lt;3.0.0&#39; Check the logs for full command output.\n",
       "\n",
       "*** WARNING: skipped 19999 bytes of output ***\n",
       "\n",
       "  Using cached spacy-3.7.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n",
       "Collecting multidict&lt;7.0,&gt;=4.5\n",
       "  Using cached multidict-6.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
       "Collecting frozenlist&gt;=1.1.1\n",
       "  Using cached frozenlist-1.4.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (240 kB)\n",
       "Requirement already satisfied: attrs&gt;=17.3.0 in /databricks/python3/lib/python3.8/site-packages (from aiohttp-&gt;datasets&gt;=2.0.0-&gt;evaluate&lt;=0.2.2-&gt;autogluon.multimodal==0.6.0-&gt;autogluon==0.6.0) (20.3.0)\n",
       "Collecting aiosignal&gt;=1.1.2\n",
       "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
       "Collecting aiohappyeyeballs&gt;=2.3.0\n",
       "  Using cached aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
       "Collecting async-timeout&lt;5.0,&gt;=4.0\n",
       "  Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
       "Collecting yarl&lt;2.0,&gt;=1.12.0\n",
       "  Using cached yarl-1.14.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
       "Collecting opencv-python\n",
       "  Using cached opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.5 MB)\n",
       "Collecting yacs\n",
       "  Using cached yacs-0.1.8-py3-none-any.whl (14 kB)\n",
       "Collecting autocfg\n",
       "  Using cached autocfg-0.0.8-py3-none-any.whl (13 kB)\n",
       "Collecting portalocker\n",
       "  Using cached portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
       "Collecting pydantic~=1.7\n",
       "  Using cached pydantic-1.10.18-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
       "Collecting toolz&gt;=0.8.2\n",
       "  Using cached toolz-0.12.1-py3-none-any.whl (56 kB)\n",
       "Requirement already satisfied: typing-extensions~=4.0 in /databricks/python3/lib/python3.8/site-packages (from gluonts~=0.11.0-&gt;autogluon.timeseries[all]==0.6.0-&gt;autogluon==0.6.0) (4.12.2)\n",
       "Requirement already satisfied: py4j in /databricks/python3/lib/python3.8/site-packages (from hyperopt&lt;0.2.8,&gt;=0.2.7-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (0.10.9.7)\n",
       "Collecting future\n",
       "  Using cached future-1.0.0-py3-none-any.whl (491 kB)\n",
       "Requirement already satisfied: pyrsistent&gt;=0.14.0 in /databricks/python3/lib/python3.8/site-packages (from jsonschema&lt;=4.8.0-&gt;autogluon.multimodal==0.6.0-&gt;autogluon==0.6.0) (0.17.3)\n",
       "Requirement already satisfied: wheel in /databricks/python3/lib/python3.8/site-packages (from lightgbm&lt;3.4,&gt;=3.3-&gt;autogluon.tabular[all]==0.6.0-&gt;autogluon==0.6.0) (0.36.2)\n",
       "Collecting regex&gt;=2021.8.3\n",
       "  Using cached regex-2024.9.11-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (785 kB)\n",
       "Collecting typish&gt;=1.7.0\n",
       "  Using cached typish-1.9.3-py3-none-any.whl (45 kB)\n",
       "Collecting antlr4-python3-runtime==4.8\n",
       "  Using cached antlr4_python3_runtime-4.8-py3-none-any.whl\n",
       "Collecting model-index\n",
       "  Using cached model_index-0.1.11-py3-none-any.whl (34 kB)\n",
       "Collecting colorama\n",
       "  Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
       "Collecting rich\n",
       "  Using cached rich-13.9.2-py3-none-any.whl (242 kB)\n",
       "Requirement already satisfied: tabulate in /databricks/python3/lib/python3.8/site-packages (from openmim&lt;=0.2.1,&gt;0.1.5-&gt;autogluon.multimodal==0.6.0-&gt;autogluon==0.6.0) (0.9.0)\n",
       "Requirement already satisfied: pytz&gt;=2020.1 in /databricks/python3/lib/python3.8/site-packages (from pandas!=1.4.0,&lt;1.6,&gt;=1.2.5-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (2020.5)\n",
       "Requirement already satisfied: python-dateutil&gt;=2.8.1 in /databricks/python3/lib/python3.8/site-packages (from pandas!=1.4.0,&lt;1.6,&gt;=1.2.5-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (2.8.1)\n",
       "Requirement already satisfied: locket in /databricks/python3/lib/python3.8/site-packages (from partd&gt;=0.3.10-&gt;dask&lt;=2021.11.2,&gt;=2021.09.1-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (1.0.0)\n",
       "Requirement already satisfied: urllib3 in /databricks/python3/lib/python3.8/site-packages (from pmdarima~=1.8.2-&gt;autogluon.timeseries[all]==0.6.0-&gt;autogluon==0.6.0) (1.26.15)\n",
       "Requirement already satisfied: Cython!=0.29.18,&gt;=0.29 in /databricks/python3/lib/python3.8/site-packages (from pmdarima~=1.8.2-&gt;autogluon.timeseries[all]==0.6.0-&gt;autogluon==0.6.0) (0.29.23)\n",
       "Requirement already satisfied: cycler&gt;=0.10 in /databricks/python3/lib/python3.8/site-packages (from matplotlib-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (0.10.0)\n",
       "Requirement already satisfied: pyparsing&gt;=2.2.1 in /databricks/python3/lib/python3.8/site-packages (from matplotlib-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (2.4.7)\n",
       "Requirement already satisfied: kiwisolver&gt;=1.0.1 in /databricks/python3/lib/python3.8/site-packages (from matplotlib-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (1.3.1)\n",
       "Collecting tensorboard&gt;=2.9.1\n",
       "  Using cached tensorboard-2.14.0-py3-none-any.whl (5.5 MB)\n",
       "Collecting pyDeprecate&gt;=0.3.1\n",
       "  Using cached pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
       "Requirement already satisfied: virtualenv in /usr/local/lib/python3.8/dist-packages (from ray[tune]&lt;2.1,&gt;=2.0-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (20.4.1)\n",
       "Requirement already satisfied: protobuf&lt;4.0.0,&gt;=3.15.3 in /databricks/python3/lib/python3.8/site-packages (from ray[tune]&lt;2.1,&gt;=2.0-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (3.17.2)\n",
       "Collecting grpcio&lt;=1.43.0,&gt;=1.32.0\n",
       "  Using cached grpcio-1.43.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
       "Collecting click&gt;=6.6\n",
       "  Using cached click-8.0.4-py3-none-any.whl (97 kB)\n",
       "Collecting tensorboardX&gt;=1.9\n",
       "  Using cached tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
       "Requirement already satisfied: certifi&gt;=2017.4.17 in /databricks/python3/lib/python3.8/site-packages (from requests-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (2020.12.5)\n",
       "Requirement already satisfied: idna&lt;4,&gt;=2.5 in /databricks/python3/lib/python3.8/site-packages (from requests-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (2.10)\n",
       "Requirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /databricks/python3/lib/python3.8/site-packages (from requests-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (3.4.0)\n",
       "Collecting PyWavelets&gt;=1.1.1\n",
       "  Using cached PyWavelets-1.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.9 MB)\n",
       "Collecting imageio&gt;=2.4.1\n",
       "  Using cached imageio-2.35.1-py3-none-any.whl (315 kB)\n",
       "Collecting tifffile&gt;=2019.7.26\n",
       "  Using cached tifffile-2023.7.10-py3-none-any.whl (220 kB)\n",
       "Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /databricks/python3/lib/python3.8/site-packages (from scikit-learn&lt;1.2,&gt;=1.0.0-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (2.1.0)\n",
       "Requirement already satisfied: numba&gt;=0.53 in /databricks/python3/lib/python3.8/site-packages (from sktime&lt;0.14,&gt;=0.13.1-&gt;autogluon.timeseries[all]==0.6.0-&gt;autogluon==0.6.0) (0.58.1)\n",
       "Requirement already satisfied: deprecated&gt;=1.2.13 in /databricks/python3/lib/python3.8/site-packages (from sktime&lt;0.14,&gt;=0.13.1-&gt;autogluon.timeseries[all]==0.6.0-&gt;autogluon==0.6.0) (1.2.14)\n",
       "Requirement already satisfied: wrapt&lt;2,&gt;=1.10 in /databricks/python3/lib/python3.8/site-packages (from deprecated&gt;=1.2.13-&gt;sktime&lt;0.14,&gt;=0.13.1-&gt;autogluon.timeseries[all]==0.6.0-&gt;autogluon==0.6.0) (1.16.0)\n",
       "Requirement already satisfied: llvmlite&lt;0.42,&gt;=0.41.0dev0 in /databricks/python3/lib/python3.8/site-packages (from numba&gt;=0.53-&gt;sktime&lt;0.14,&gt;=0.13.1-&gt;autogluon.timeseries[all]==0.6.0-&gt;autogluon==0.6.0) (0.41.1)\n",
       "Requirement already satisfied: importlib-metadata in /databricks/python3/lib/python3.8/site-packages (from numba&gt;=0.53-&gt;sktime&lt;0.14,&gt;=0.13.1-&gt;autogluon.timeseries[all]==0.6.0-&gt;autogluon==0.6.0) (6.11.0)\n",
       "Collecting spacy-loggers&lt;2.0.0,&gt;=1.0.0\n",
       "  Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
       "Collecting preshed&lt;3.1.0,&gt;=3.0.2\n",
       "  Using cached preshed-3.0.9-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (154 kB)\n",
       "Collecting weasel&lt;0.5.0,&gt;=0.1.0\n",
       "  Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
       "Collecting spacy-legacy&lt;3.1.0,&gt;=3.0.11\n",
       "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
       "Collecting catalogue&lt;2.1.0,&gt;=2.0.6\n",
       "  Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
       "Collecting typer&lt;1.0.0,&gt;=0.3.0\n",
       "  Using cached typer-0.12.5-py3-none-any.whl (47 kB)\n",
       "Collecting srsly&lt;3.0.0,&gt;=2.4.3\n",
       "  Using cached srsly-2.4.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (494 kB)\n",
       "Collecting thinc&lt;8.3.0,&gt;=8.2.2\n",
       "  Using cached thinc-8.2.5-cp38-cp38-linux_x86_64.whl\n",
       "Collecting murmurhash&lt;1.1.0,&gt;=0.28.0\n",
       "  Using cached murmurhash-1.0.10-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
       "Collecting wasabi&lt;1.2.0,&gt;=0.9.1\n",
       "  Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
       "Collecting cymem&lt;2.1.0,&gt;=2.0.2\n",
       "  Using cached cymem-2.0.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (46 kB)\n",
       "Collecting langcodes&lt;4.0.0,&gt;=3.2.0\n",
       "  Using cached langcodes-3.4.1-py3-none-any.whl (182 kB)\n",
       "Collecting language-data&gt;=1.2\n",
       "  Using cached language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
       "Collecting marisa-trie&gt;=0.7.7\n",
       "  Using cached marisa_trie-1.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
       "Requirement already satisfied: patsy&gt;=0.5.2 in /databricks/python3/lib/python3.8/site-packages (from statsmodels~=0.13.0-&gt;autogluon.timeseries[all]==0.6.0-&gt;autogluon==0.6.0) (0.5.6)\n",
       "Collecting absl-py&gt;=0.4\n",
       "  Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
       "Collecting google-auth-oauthlib&lt;1.1,&gt;=0.5\n",
       "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
       "Collecting tensorboard-data-server&lt;0.8.0,&gt;=0.7.0\n",
       "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
       "Collecting google-auth&lt;3,&gt;=1.6.3\n",
       "  Using cached google_auth-2.35.0-py2.py3-none-any.whl (208 kB)\n",
       "Requirement already satisfied: werkzeug&gt;=1.0.1 in /databricks/python3/lib/python3.8/site-packages (from tensorboard&gt;=2.9.1-&gt;pytorch-lightning&lt;1.8.0,&gt;=1.7.4-&gt;autogluon.multimodal==0.6.0-&gt;autogluon==0.6.0) (3.0.4)\n",
       "Collecting protobuf&lt;4.0.0,&gt;=3.15.3\n",
       "  Using cached protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
       "Collecting tensorboard&gt;=2.9.1\n",
       "  Using cached tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
       "  Using cached tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
       "  Using cached tensorboard-2.12.2-py3-none-any.whl (5.6 MB)\n",
       "Collecting tensorboard-plugin-wit&gt;=1.6.0\n",
       "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
       "Collecting tensorboard&gt;=2.9.1\n",
       "  Using cached tensorboard-2.12.1-py3-none-any.whl (5.6 MB)\n",
       "  Using cached tensorboard-2.12.0-py3-none-any.whl (5.6 MB)\n",
       "Collecting google-auth-oauthlib&lt;0.5,&gt;=0.4.1\n",
       "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
       "Collecting tensorboard&gt;=2.9.1\n",
       "  Using cached tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
       "Collecting tensorboard-data-server&lt;0.7.0,&gt;=0.6.0\n",
       "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
       "Requirement already satisfied: markdown&gt;=2.6.8 in /databricks/python3/lib/python3.8/site-packages (from tensorboard&gt;=2.9.1-&gt;pytorch-lightning&lt;1.8.0,&gt;=1.7.4-&gt;autogluon.multimodal==0.6.0-&gt;autogluon==0.6.0) (3.7)\n",
       "Collecting pyasn1-modules&gt;=0.2.1\n",
       "  Using cached pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
       "Collecting cachetools&lt;6.0,&gt;=2.0.0\n",
       "  Using cached cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
       "Collecting rsa&lt;5,&gt;=3.1.4\n",
       "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
       "Collecting requests-oauthlib&gt;=0.7.0\n",
       "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
       "Requirement already satisfied: zipp&gt;=0.5 in /databricks/python3/lib/python3.8/site-packages (from importlib-metadata-&gt;numba&gt;=0.53-&gt;sktime&lt;0.14,&gt;=0.13.1-&gt;autogluon.timeseries[all]==0.6.0-&gt;autogluon==0.6.0) (3.20.2)\n",
       "Collecting pyasn1&lt;0.7.0,&gt;=0.4.6\n",
       "  Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
       "Requirement already satisfied: oauthlib&gt;=3.0.0 in /databricks/python3/lib/python3.8/site-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard&gt;=2.9.1-&gt;pytorch-lightning&lt;1.8.0,&gt;=1.7.4-&gt;autogluon.multimodal==0.6.0-&gt;autogluon==0.6.0) (3.2.2)\n",
       "Collecting confection&lt;1.0.0,&gt;=0.0.1\n",
       "  Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
       "Collecting blis&lt;0.8.0,&gt;=0.7.8\n",
       "  Using cached blis-0.7.11-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
       "Collecting tokenizers!=0.11.3,&lt;0.14,&gt;=0.11.1\n",
       "  Using cached tokenizers-0.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
       "Collecting shellingham&gt;=1.3.0\n",
       "  Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
       "Collecting pygments&lt;3.0.0,&gt;=2.13.0\n",
       "  Using cached pygments-2.18.0-py3-none-any.whl (1.2 MB)\n",
       "Collecting markdown-it-py&gt;=2.2.0\n",
       "  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
       "Collecting mdurl~=0.1\n",
       "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
       "Collecting cloudpathlib&lt;1.0.0,&gt;=0.7.0\n",
       "  Using cached cloudpathlib-0.19.0-py3-none-any.whl (49 kB)\n",
       "Requirement already satisfied: MarkupSafe&gt;=2.1.1 in /databricks/python3/lib/python3.8/site-packages (from werkzeug&gt;=1.0.1-&gt;tensorboard&gt;=2.9.1-&gt;pytorch-lightning&lt;1.8.0,&gt;=1.7.4-&gt;autogluon.multimodal==0.6.0-&gt;autogluon==0.6.0) (2.1.5)\n",
       "Collecting propcache&gt;=0.2.0\n",
       "  Using cached propcache-0.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
       "Requirement already satisfied: jmespath&lt;1.0.0,&gt;=0.7.1 in /databricks/python3/lib/python3.8/site-packages (from boto3-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (0.10.0)\n",
       "Requirement already satisfied: s3transfer&lt;0.4.0,&gt;=0.3.0 in /databricks/python3/lib/python3.8/site-packages (from boto3-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (0.3.7)\n",
       "Requirement already satisfied: botocore&lt;1.20.0,&gt;=1.19.7 in /databricks/python3/lib/python3.8/site-packages (from boto3-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (1.19.63)\n",
       "Collecting ordered-set\n",
       "  Using cached ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
       "Collecting multiprocess\n",
       "  Using cached multiprocess-0.70.16-py38-none-any.whl (132 kB)\n",
       "Requirement already satisfied: tenacity&gt;=6.2.0 in /databricks/python3/lib/python3.8/site-packages (from plotly-&gt;catboost&lt;1.2,&gt;=1.0-&gt;autogluon.tabular[all]==0.6.0-&gt;autogluon==0.6.0) (8.0.1)\n",
       "Requirement already satisfied: appdirs&lt;2,&gt;=1.4.3 in /usr/local/lib/python3.8/dist-packages (from virtualenv-&gt;ray[tune]&lt;2.1,&gt;=2.0-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (1.4.4)\n",
       "Requirement already satisfied: distlib&lt;1,&gt;=0.3.1 in /usr/local/lib/python3.8/dist-packages (from virtualenv-&gt;ray[tune]&lt;2.1,&gt;=2.0-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (0.3.4)\n",
       "Installing collected packages: toolz, pyasn1, propcache, multidict, mdurl, fsspec, frozenlist, yarl, rsa, pygments, pyasn1-modules, psutil, Pillow, markdown-it-py, joblib, click, catalogue, cachetools, async-timeout, aiosignal, aiohappyeyeballs, srsly, shellingham, rich, requests-oauthlib, pydantic, protobuf, murmurhash, marisa-trie, grpcio, google-auth, dill, cymem, autogluon.common, aiohttp, xxhash, wasabi, typer, torch, tifffile, tensorboardX, tensorboard-plugin-wit, tensorboard-data-server, smart-open, ray, PyWavelets, pyDeprecate, pyarrow, preshed, ordered-set, opencv-python-headless, multiprocess, language-data, imageio, huggingface-hub, graphviz, google-auth-oauthlib, future, confection, cloudpathlib, blis, autogluon.features, autogluon.core, absl-py, xgboost, weasel, typish, torchvision, torchmetrics, tokenizers, thinc, tensorboard, statsmodels, spacy-loggers, spacy-legacy, scikit-image, responses, regex, qudida, model-index, lightgbm, langcodes, hyperopt, fastprogress, fastcore, datasets, colorama, catboost, autogluon.tabular, antlr4-python3-runtime, yacs, transformers, torchtext, timm, text-unidecode, spacy, seqeval, sentencepiece, pytorch-metric-learning, pytorch-lightning, pycocotools, portalocker, pmdarima, openmim, opencv-python, omegaconf, nptyping, nltk, nlpaug, gluonts, fastdownload, fairscale, evaluate, autocfg, albumentations, accelerate, tbats, gluoncv, fastai, autogluon.timeseries, autogluon.multimodal, autogluon.vision, autogluon.text, autogluon\n",
       "  Attempting uninstall: toolz\n",
       "    Found existing installation: toolz 1.0.0\n",
       "    Not uninstalling toolz at /databricks/python3/lib/python3.8/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7\n",
       "    Can&#39;t uninstall &#39;toolz&#39;. No files were found to uninstall.\n",
       "  Attempting uninstall: fsspec\n",
       "    Found existing installation: fsspec 2024.9.0\n",
       "    Not uninstalling fsspec at /databricks/python3/lib/python3.8/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7\n",
       "    Can&#39;t uninstall &#39;fsspec&#39;. No files were found to uninstall.\n",
       "  Attempting uninstall: pygments\n",
       "    Found existing installation: Pygments 2.8.1\n",
       "    Not uninstalling pygments at /databricks/python3/lib/python3.8/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7\n",
       "    Can&#39;t uninstall &#39;Pygments&#39;. No files were found to uninstall.\n",
       "  Attempting uninstall: psutil\n",
       "    Found existing installation: psutil 5.9.8\n",
       "    Not uninstalling psutil at /databricks/python3/lib/python3.8/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7\n",
       "    Can&#39;t uninstall &#39;psutil&#39;. No files were found to uninstall.\n",
       "  Attempting uninstall: Pillow\n",
       "    Found existing installation: Pillow 8.2.0\n",
       "    Not uninstalling pillow at /databricks/python3/lib/python3.8/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7\n",
       "    Can&#39;t uninstall &#39;Pillow&#39;. No files were found to uninstall.\n",
       "  Attempting uninstall: joblib\n",
       "    Found existing installation: joblib 1.0.1\n",
       "    Not uninstalling joblib at /databricks/python3/lib/python3.8/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7\n",
       "    Can&#39;t uninstall &#39;joblib&#39;. No files were found to uninstall.\n",
       "  Attempting uninstall: click\n",
       "    Found existing installation: click 8.1.7\n",
       "    Not uninstalling click at /databricks/python3/lib/python3.8/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7\n",
       "    Can&#39;t uninstall &#39;click&#39;. No files were found to uninstall.\n",
       "  Attempting uninstall: protobuf\n",
       "    Found existing installation: protobuf 3.17.2\n",
       "    Not uninstalling protobuf at /databricks/python3/lib/python3.8/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7\n",
       "    Can&#39;t uninstall &#39;protobuf&#39;. No files were found to uninstall.\n",
       "  Attempting uninstall: autogluon.common\n",
       "    Found existing installation: autogluon.common 0.5.2\n",
       "    Not uninstalling autogluon.common at /databricks/python3/lib/python3.8/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7\n",
       "    Can&#39;t uninstall &#39;autogluon.common&#39;. No files were found to uninstall.\n",
       "  Attempting uninstall: pyarrow\n",
       "    Found existing installation: pyarrow 4.0.0\n",
       "    Not uninstalling pyarrow at /databricks/python3/lib/python3.8/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7\n",
       "    Can&#39;t uninstall &#39;pyarrow&#39;. No files were found to uninstall.\n",
       "  Attempting uninstall: autogluon.features\n",
       "    Found existing installation: autogluon.features 0.5.2\n",
       "    Not uninstalling autogluon.features at /databricks/python3/lib/python3.8/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7\n",
       "    Can&#39;t uninstall &#39;autogluon.features&#39;. No files were found to uninstall.\n",
       "  Attempting uninstall: autogluon.core\n",
       "    Found existing installation: autogluon.core 0.5.2\n",
       "    Not uninstalling autogluon.core at /databricks/python3/lib/python3.8/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7\n",
       "    Can&#39;t uninstall &#39;autogluon.core&#39;. No files were found to uninstall.\n",
       "  Attempting uninstall: xgboost\n",
       "    Found existing installation: xgboost 2.1.1\n",
       "    Not uninstalling xgboost at /databricks/python3/lib/python3.8/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7\n",
       "    Can&#39;t uninstall &#39;xgboost&#39;. No files were found to uninstall.\n",
       "  Attempting uninstall: statsmodels\n",
       "    Found existing installation: statsmodels 0.14.1\n",
       "    Not uninstalling statsmodels at /databricks/python3/lib/python3.8/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7\n",
       "    Can&#39;t uninstall &#39;statsmodels&#39;. No files were found to uninstall.\n",
       "  Attempting uninstall: lightgbm\n",
       "    Found existing installation: lightgbm 4.0.0\n",
       "    Not uninstalling lightgbm at /databricks/python3/lib/python3.8/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7\n",
       "    Can&#39;t uninstall &#39;lightgbm&#39;. No files were found to uninstall.\n",
       "  Attempting uninstall: autogluon.tabular\n",
       "    Found existing installation: autogluon.tabular 0.5.2\n",
       "    Not uninstalling autogluon.tabular at /databricks/python3/lib/python3.8/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7\n",
       "    Can&#39;t uninstall &#39;autogluon.tabular&#39;. No files were found to uninstall.\n",
       "  Attempting uninstall: pmdarima\n",
       "    Found existing installation: pmdarima 2.0.4\n",
       "    Not uninstalling pmdarima at /databricks/python3/lib/python3.8/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7\n",
       "    Can&#39;t uninstall &#39;pmdarima&#39;. No files were found to uninstall.\n",
       "ERROR: pip&#39;s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
       "phgml 1.5.0 requires autogluon-tabular==0.5.2, but you have autogluon-tabular 0.6.0 which is incompatible.\n",
       "phgml 1.5.0 requires lightgbm==4.0.0, but you have lightgbm 3.3.5 which is incompatible.\n",
       "phgml 1.5.0 requires xgboost&lt;3.0.0,&gt;=2.0.3, but you have xgboost 1.7.6 which is incompatible.\n",
       "mlflow 2.2.2 requires pyarrow&lt;12,&gt;=4.0.0, but you have pyarrow 17.0.0 which is incompatible.\n",
       "flask 2.3.3 requires click&gt;=8.1.3, but you have click 8.0.4 which is incompatible.\n",
       "Successfully installed Pillow-9.0.1 PyWavelets-1.4.1 absl-py-2.1.0 accelerate-0.13.2 aiohappyeyeballs-2.4.3 aiohttp-3.10.10 aiosignal-1.3.1 albumentations-1.1.0 antlr4-python3-runtime-4.8 async-timeout-4.0.3 autocfg-0.0.8 autogluon-0.6.0 autogluon.common-0.6.0 autogluon.core-0.6.0 autogluon.features-0.6.0 autogluon.multimodal-0.6.0 autogluon.tabular-0.6.0 autogluon.text-0.6.0 autogluon.timeseries-0.6.0 autogluon.vision-0.6.0 blis-0.7.11 cachetools-5.5.0 catalogue-2.0.10 catboost-1.1.1 click-8.0.4 cloudpathlib-0.19.0 colorama-0.4.6 confection-0.1.5 cymem-2.0.8 datasets-3.0.1 dill-0.3.8 evaluate-0.2.2 fairscale-0.4.6 fastai-2.7.17 fastcore-1.7.13 fastdownload-0.0.7 fastprogress-1.0.3 frozenlist-1.4.1 fsspec-2024.6.1 future-1.0.0 gluoncv-0.10.5.post0 gluonts-0.11.12 google-auth-2.35.0 google-auth-oauthlib-0.4.6 graphviz-0.20.3 grpcio-1.43.0 huggingface-hub-0.25.2 hyperopt-0.2.7 imageio-2.35.1 joblib-1.4.2 langcodes-3.4.1 language-data-1.2.0 lightgbm-3.3.5 marisa-trie-1.2.0 markdown-it-py-3.0.0 mdurl-0.1.2 model-index-0.1.11 multidict-6.1.0 multiprocess-0.70.16 murmurhash-1.0.10 nlpaug-1.1.10 nltk-3.9.1 nptyping-1.4.4 omegaconf-2.1.2 opencv-python-4.10.0.84 opencv-python-headless-4.10.0.84 openmim-0.2.1 ordered-set-4.1.0 pmdarima-1.8.5 portalocker-2.10.1 preshed-3.0.9 propcache-0.2.0 protobuf-3.20.3 psutil-5.8.0 pyDeprecate-0.3.2 pyarrow-17.0.0 pyasn1-0.6.1 pyasn1-modules-0.4.1 pycocotools-2.0.6 pydantic-1.10.18 pygments-2.18.0 pytorch-lightning-1.7.7 pytorch-metric-learning-1.3.2 qudida-0.0.4 ray-2.0.1 regex-2024.9.11 requests-oauthlib-2.0.0 responses-0.18.0 rich-13.9.2 rsa-4.9 scikit-image-0.19.3 sentencepiece-0.1.99 seqeval-1.2.2 shellingham-1.5.4 smart-open-5.2.1 spacy-3.7.5 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 statsmodels-0.13.5 tbats-1.1.3 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorboardX-2.6.2.2 text-unidecode-1.3 thinc-8.2.5 tifffile-2023.7.10 timm-0.6.13 tokenizers-0.13.3 toolz-0.12.1 torch-1.12.1 torchmetrics-0.8.2 torchtext-0.13.1 torchvision-0.13.1 transformers-4.23.1 typer-0.12.5 typish-1.9.3 wasabi-1.1.3 weasel-0.4.1 xgboost-1.7.6 xxhash-3.5.0 yacs-0.1.8 yarl-1.14.0\n",
       "Python interpreter will be restarted.\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Python interpreter will be restarted.\nRequirement already satisfied: mlflow in /databricks/python3/lib/python3.8/site-packages (2.2.2)\nRequirement already satisfied: scipy&lt;2 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.6.2)\nRequirement already satisfied: requests&lt;3,&gt;=2.17.3 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (2.32.3)\nRequirement already satisfied: entrypoints&lt;1 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (0.3)\nRequirement already satisfied: markdown&lt;4,&gt;=3.3 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (3.7)\nRequirement already satisfied: cloudpickle&lt;3 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (2.2.1)\nRequirement already satisfied: scikit-learn&lt;2 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.0.2)\nRequirement already satisfied: shap&lt;1,&gt;=0.40 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (0.44.1)\nRequirement already satisfied: pytz&lt;2023 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (2020.5)\nRequirement already satisfied: pyyaml&lt;7,&gt;=5.1 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (6.0.2)\nRequirement already satisfied: docker&lt;7,&gt;=4.0.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (6.1.3)\nRequirement already satisfied: pandas&lt;3 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.4.4)\nRequirement already satisfied: querystring-parser&lt;2 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.2.4)\nRequirement already satisfied: numpy&lt;2 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.22.4)\nRequirement already satisfied: importlib-metadata!=4.7.0,&lt;7,&gt;=3.7.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (6.11.0)\nRequirement already satisfied: protobuf&lt;5,&gt;=3.12.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (3.17.2)\nRequirement already satisfied: pyarrow&lt;12,&gt;=4.0.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (4.0.0)\nRequirement already satisfied: Flask&lt;3 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (2.3.3)\nRequirement already satisfied: packaging&lt;24 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (23.2)\nRequirement already satisfied: databricks-cli&lt;1,&gt;=0.8.7 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (0.18.0)\nRequirement already satisfied: alembic&lt;2 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.13.3)\nRequirement already satisfied: sqlparse&lt;1,&gt;=0.4.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (0.5.1)\nRequirement already satisfied: matplotlib&lt;4 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (3.4.2)\nRequirement already satisfied: gunicorn&lt;21 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (20.1.0)\nRequirement already satisfied: Jinja2&lt;4,&gt;=2.11 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (3.1.4)\nRequirement already satisfied: gitpython&lt;4,&gt;=2.1.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (3.1.43)\nRequirement already satisfied: click&lt;9,&gt;=7.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (8.1.7)\nRequirement already satisfied: sqlalchemy&lt;3,&gt;=1.4.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (2.0.35)\nRequirement already satisfied: importlib-resources in /databricks/python3/lib/python3.8/site-packages (from alembic&lt;2-&gt;mlflow) (6.4.5)\nRequirement already satisfied: typing-extensions&gt;=4 in /databricks/python3/lib/python3.8/site-packages (from alembic&lt;2-&gt;mlflow) (4.12.2)\nRequirement already satisfied: Mako in /databricks/python3/lib/python3.8/site-packages (from alembic&lt;2-&gt;mlflow) (1.3.5)\nRequirement already satisfied: oauthlib&gt;=3.1.0 in /databricks/python3/lib/python3.8/site-packages (from databricks-cli&lt;1,&gt;=0.8.7-&gt;mlflow) (3.2.2)\nRequirement already satisfied: pyjwt&gt;=1.7.0 in /databricks/python3/lib/python3.8/site-packages (from databricks-cli&lt;1,&gt;=0.8.7-&gt;mlflow) (2.9.0)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.26.7 in /databricks/python3/lib/python3.8/site-packages (from databricks-cli&lt;1,&gt;=0.8.7-&gt;mlflow) (1.26.15)\nRequirement already satisfied: tabulate&gt;=0.7.7 in /databricks/python3/lib/python3.8/site-packages (from databricks-cli&lt;1,&gt;=0.8.7-&gt;mlflow) (0.9.0)\nRequirement already satisfied: six&gt;=1.10.0 in /databricks/python3/lib/python3.8/site-packages (from databricks-cli&lt;1,&gt;=0.8.7-&gt;mlflow) (1.15.0)\nRequirement already satisfied: websocket-client&gt;=0.32.0 in /databricks/python3/lib/python3.8/site-packages (from docker&lt;7,&gt;=4.0.0-&gt;mlflow) (1.8.0)\nRequirement already satisfied: blinker&gt;=1.6.2 in /databricks/python3/lib/python3.8/site-packages (from Flask&lt;3-&gt;mlflow) (1.8.2)\nRequirement already satisfied: Werkzeug&gt;=2.3.7 in /databricks/python3/lib/python3.8/site-packages (from Flask&lt;3-&gt;mlflow) (3.0.4)\nRequirement already satisfied: itsdangerous&gt;=2.1.2 in /databricks/python3/lib/python3.8/site-packages (from Flask&lt;3-&gt;mlflow) (2.2.0)\nRequirement already satisfied: gitdb&lt;5,&gt;=4.0.1 in /databricks/python3/lib/python3.8/site-packages (from gitpython&lt;4,&gt;=2.1.0-&gt;mlflow) (4.0.11)\nRequirement already satisfied: smmap&lt;6,&gt;=3.0.1 in /databricks/python3/lib/python3.8/site-packages (from gitdb&lt;5,&gt;=4.0.1-&gt;gitpython&lt;4,&gt;=2.1.0-&gt;mlflow) (5.0.1)\nRequirement already satisfied: setuptools&gt;=3.0 in /usr/local/lib/python3.8/dist-packages (from gunicorn&lt;21-&gt;mlflow) (52.0.0)\nRequirement already satisfied: zipp&gt;=0.5 in /databricks/python3/lib/python3.8/site-packages (from importlib-metadata!=4.7.0,&lt;7,&gt;=3.7.0-&gt;mlflow) (3.20.2)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /databricks/python3/lib/python3.8/site-packages (from Jinja2&lt;4,&gt;=2.11-&gt;mlflow) (2.1.5)\nRequirement already satisfied: cycler&gt;=0.10 in /databricks/python3/lib/python3.8/site-packages (from matplotlib&lt;4-&gt;mlflow) (0.10.0)\nRequirement already satisfied: pillow&gt;=6.2.0 in /databricks/python3/lib/python3.8/site-packages (from matplotlib&lt;4-&gt;mlflow) (8.2.0)\nRequirement already satisfied: pyparsing&gt;=2.2.1 in /databricks/python3/lib/python3.8/site-packages (from matplotlib&lt;4-&gt;mlflow) (2.4.7)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /databricks/python3/lib/python3.8/site-packages (from matplotlib&lt;4-&gt;mlflow) (1.3.1)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /databricks/python3/lib/python3.8/site-packages (from matplotlib&lt;4-&gt;mlflow) (2.8.1)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.17.3-&gt;mlflow) (2020.12.5)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.17.3-&gt;mlflow) (2.10)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.17.3-&gt;mlflow) (3.4.0)\nRequirement already satisfied: joblib&gt;=0.11 in /databricks/python3/lib/python3.8/site-packages (from scikit-learn&lt;2-&gt;mlflow) (1.0.1)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /databricks/python3/lib/python3.8/site-packages (from scikit-learn&lt;2-&gt;mlflow) (2.1.0)\nRequirement already satisfied: numba in /databricks/python3/lib/python3.8/site-packages (from shap&lt;1,&gt;=0.40-&gt;mlflow) (0.58.1)\nRequirement already satisfied: tqdm&gt;=4.27.0 in /databricks/python3/lib/python3.8/site-packages (from shap&lt;1,&gt;=0.40-&gt;mlflow) (4.66.5)\nRequirement already satisfied: slicer==0.0.7 in /databricks/python3/lib/python3.8/site-packages (from shap&lt;1,&gt;=0.40-&gt;mlflow) (0.0.7)\nRequirement already satisfied: greenlet!=0.4.17 in /databricks/python3/lib/python3.8/site-packages (from sqlalchemy&lt;3,&gt;=1.4.0-&gt;mlflow) (3.1.1)\nRequirement already satisfied: llvmlite&lt;0.42,&gt;=0.41.0dev0 in /databricks/python3/lib/python3.8/site-packages (from numba-&gt;shap&lt;1,&gt;=0.40-&gt;mlflow) (0.41.1)\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nCollecting autogluon==0.6.0\n  Using cached autogluon-0.6.0-py3-none-any.whl (9.8 kB)\nCollecting autogluon.tabular[all]==0.6.0\n  Using cached autogluon.tabular-0.6.0-py3-none-any.whl (285 kB)\nCollecting autogluon.vision==0.6.0\n  Using cached autogluon.vision-0.6.0-py3-none-any.whl (49 kB)\nCollecting autogluon.timeseries[all]==0.6.0\n  Using cached autogluon.timeseries-0.6.0-py3-none-any.whl (101 kB)\nCollecting autogluon.text==0.6.0\n  Using cached autogluon.text-0.6.0-py3-none-any.whl (62 kB)\nCollecting autogluon.multimodal==0.6.0\n  Using cached autogluon.multimodal-0.6.0-py3-none-any.whl (279 kB)\nCollecting autogluon.core[all]==0.6.0\n  Using cached autogluon.core-0.6.0-py3-none-any.whl (224 kB)\nCollecting autogluon.features==0.6.0\n  Using cached autogluon.features-0.6.0-py3-none-any.whl (59 kB)\nRequirement already satisfied: dask&lt;=2021.11.2,&gt;=2021.09.1 in /databricks/python3/lib/python3.8/site-packages (from autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (2021.11.2)\nRequirement already satisfied: scikit-learn&lt;1.2,&gt;=1.0.0 in /databricks/python3/lib/python3.8/site-packages (from autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (1.0.2)\nRequirement already satisfied: distributed&lt;=2021.11.2,&gt;=2021.09.1 in /databricks/python3/lib/python3.8/site-packages (from autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (2021.11.2)\nRequirement already satisfied: pandas!=1.4.0,&lt;1.6,&gt;=1.2.5 in /databricks/python3/lib/python3.8/site-packages (from autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (1.4.4)\nCollecting autogluon.common==0.6.0\n  Using cached autogluon.common-0.6.0-py3-none-any.whl (41 kB)\nRequirement already satisfied: scipy&lt;1.10.0,&gt;=1.5.4 in /databricks/python3/lib/python3.8/site-packages (from autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (1.6.2)\nRequirement already satisfied: tqdm&gt;=4.38.0 in /databricks/python3/lib/python3.8/site-packages (from autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (4.66.5)\nRequirement already satisfied: requests in /databricks/python3/lib/python3.8/site-packages (from autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (2.32.3)\nRequirement already satisfied: numpy&lt;1.24,&gt;=1.21 in /databricks/python3/lib/python3.8/site-packages (from autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (1.22.4)\nRequirement already satisfied: matplotlib in /databricks/python3/lib/python3.8/site-packages (from autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (3.4.2)\nRequirement already satisfied: boto3 in /databricks/python3/lib/python3.8/site-packages (from autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (1.16.7)\nCollecting ray[tune]&lt;2.1,&gt;=2.0\n  Using cached ray-2.0.1-cp38-cp38-manylinux2014_x86_64.whl (60.2 MB)\nCollecting hyperopt&lt;0.2.8,&gt;=0.2.7\n  Using cached hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from autogluon.common==0.6.0-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (52.0.0)\nRequirement already satisfied: psutil&lt;6,&gt;=5.7.3 in /databricks/python3/lib/python3.8/site-packages (from autogluon.features==0.6.0-&gt;autogluon==0.6.0) (5.9.8)\nCollecting torch&lt;1.13,&gt;=1.9\n  Using cached torch-1.12.1-cp38-cp38-manylinux1_x86_64.whl (776.3 MB)\nCollecting sentencepiece&lt;0.2.0,&gt;=0.1.95\n  Using cached sentencepiece-0.1.99-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\nCollecting evaluate&lt;=0.2.2\n  Using cached evaluate-0.2.2-py3-none-any.whl (69 kB)\nCollecting pycocotools&lt;2.0.7,&gt;=2.0.5\n  Using cached pycocotools-2.0.6-cp38-cp38-linux_x86_64.whl\nCollecting timm&lt;0.7.0\n  Using cached timm-0.6.13-py3-none-any.whl (549 kB)\nCollecting pytorch-metric-learning&lt;1.4.0,&gt;=1.3.0\n  Using cached pytorch_metric_learning-1.3.2-py3-none-any.whl (109 kB)\nCollecting text-unidecode&lt;=1.3\n  Using cached text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\nCollecting torchmetrics&lt;0.9.0,&gt;=0.8.0\n  Using cached torchmetrics-0.8.2-py3-none-any.whl (409 kB)\nCollecting torchtext&lt;0.14.0\n  Using cached torchtext-0.13.1-cp38-cp38-manylinux1_x86_64.whl (1.9 MB)\nCollecting nlpaug&lt;=1.1.10,&gt;=1.1.10\n  Using cached nlpaug-1.1.10-py3-none-any.whl (410 kB)\nCollecting Pillow&lt;9.1.0,&gt;=9.0.1\n  Using cached Pillow-9.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\nCollecting nltk&lt;4.0.0,&gt;=3.4.5\n  Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\nCollecting accelerate&lt;0.14,&gt;=0.9\n  Using cached accelerate-0.13.2-py3-none-any.whl (148 kB)\nCollecting nptyping&lt;1.5.0,&gt;=1.4.4\n  Using cached nptyping-1.4.4-py3-none-any.whl (31 kB)\nCollecting fairscale&lt;=0.4.6,&gt;=0.4.5\n  Using cached fairscale-0.4.6-py3-none-any.whl\nCollecting scikit-image&lt;0.20.0,&gt;=0.19.1\n  Using cached scikit_image-0.19.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\nCollecting seqeval&lt;=1.2.2\n  Using cached seqeval-1.2.2-py3-none-any.whl\nCollecting smart-open&lt;5.3.0,&gt;=5.2.1\n  Using cached smart_open-5.2.1-py3-none-any.whl (58 kB)\nCollecting torchvision&lt;0.14.0\n  Using cached torchvision-0.13.1-cp38-cp38-manylinux1_x86_64.whl (19.1 MB)\nCollecting transformers&lt;4.24.0,&gt;=4.23.0\n  Using cached transformers-4.23.1-py3-none-any.whl (5.3 MB)\nRequirement already satisfied: defusedxml&lt;=0.7.1,&gt;=0.7.1 in /databricks/python3/lib/python3.8/site-packages (from autogluon.multimodal==0.6.0-&gt;autogluon==0.6.0) (0.7.1)\nCollecting albumentations&lt;=1.2.0,&gt;=1.1.0\n  Using cached albumentations-1.2.0-py3-none-any.whl (113 kB)\nCollecting omegaconf&lt;2.2.0,&gt;=2.1.1\n  Using cached omegaconf-2.1.2-py3-none-any.whl (74 kB)\nCollecting pytorch-lightning&lt;1.8.0,&gt;=1.7.4\n  Using cached pytorch_lightning-1.7.7-py3-none-any.whl (708 kB)\nCollecting openmim&lt;=0.2.1,&gt;0.1.5\n  Using cached openmim-0.2.1-py2.py3-none-any.whl (49 kB)\nRequirement already satisfied: jsonschema&lt;=4.8.0 in /databricks/python3/lib/python3.8/site-packages (from autogluon.multimodal==0.6.0-&gt;autogluon==0.6.0) (3.2.0)\nRequirement already satisfied: networkx&lt;3.0,&gt;=2.3 in /databricks/python3/lib/python3.8/site-packages (from autogluon.tabular[all]==0.6.0-&gt;autogluon==0.6.0) (2.8.8)\nCollecting lightgbm&lt;3.4,&gt;=3.3\n  Using cached lightgbm-3.3.5-py3-none-manylinux1_x86_64.whl (2.0 MB)\nCollecting fastai&lt;2.8,&gt;=2.3.1\n  Using cached fastai-2.7.17-py3-none-any.whl (234 kB)\nCollecting catboost&lt;1.2,&gt;=1.0\n  Using cached catboost-1.1.1-cp38-none-manylinux1_x86_64.whl (76.6 MB)\nCollecting xgboost&lt;1.8,&gt;=1.6\n  Using cached xgboost-1.7.6-py3-none-manylinux2014_x86_64.whl (200.3 MB)\nCollecting gluonts~=0.11.0\n  Using cached gluonts-0.11.12-py3-none-any.whl (1.0 MB)\nCollecting psutil&lt;6,&gt;=5.7.3\n  Using cached psutil-5.8.0-cp38-cp38-manylinux2010_x86_64.whl (296 kB)\nCollecting joblib~=1.1\n  Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\nCollecting statsmodels~=0.13.0\n  Using cached statsmodels-0.13.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.9 MB)\nCollecting pmdarima~=1.8.2\n  Using cached pmdarima-1.8.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.5 MB)\nRequirement already satisfied: sktime&lt;0.14,&gt;=0.13.1 in /databricks/python3/lib/python3.8/site-packages (from autogluon.timeseries[all]==0.6.0-&gt;autogluon==0.6.0) (0.13.2)\nCollecting tbats~=1.1\n  Using cached tbats-1.1.3-py3-none-any.whl (44 kB)\nCollecting gluoncv&lt;0.10.6,&gt;=0.10.5\n  Using cached gluoncv-0.10.5.post0-py2.py3-none-any.whl (1.3 MB)\nRequirement already satisfied: pyyaml in /databricks/python3/lib/python3.8/site-packages (from accelerate&lt;0.14,&gt;=0.9-&gt;autogluon.multimodal==0.6.0-&gt;autogluon==0.6.0) (6.0.2)\nRequirement already satisfied: packaging&gt;=20.0 in /databricks/python3/lib/python3.8/site-packages (from accelerate&lt;0.14,&gt;=0.9-&gt;autogluon.multimodal==0.6.0-&gt;autogluon==0.6.0) (23.2)\nCollecting qudida&gt;=0.0.4\n  Using cached qudida-0.0.4-py3-none-any.whl (3.5 kB)\nCollecting opencv-python-headless&gt;=4.1.1\n  Using cached opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\nCollecting albumentations&lt;=1.2.0,&gt;=1.1.0\n  Using cached albumentations-1.1.0-py3-none-any.whl (102 kB)\nRequirement already satisfied: plotly in /databricks/python3/lib/python3.8/site-packages (from catboost&lt;1.2,&gt;=1.0-&gt;autogluon.tabular[all]==0.6.0-&gt;autogluon==0.6.0) (5.9.0)\nCollecting graphviz\n  Using cached graphviz-0.20.3-py3-none-any.whl (47 kB)\nRequirement already satisfied: six in /databricks/python3/lib/python3.8/site-packages (from catboost&lt;1.2,&gt;=1.0-&gt;autogluon.tabular[all]==0.6.0-&gt;autogluon==0.6.0) (1.15.0)\nRequirement already satisfied: toolz&gt;=0.8.2 in /databricks/python3/lib/python3.8/site-packages (from dask&lt;=2021.11.2,&gt;=2021.09.1-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (1.0.0)\nRequirement already satisfied: cloudpickle&gt;=1.1.1 in /databricks/python3/lib/python3.8/site-packages (from dask&lt;=2021.11.2,&gt;=2021.09.1-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (2.2.1)\nRequirement already satisfied: fsspec&gt;=0.6.0 in /databricks/python3/lib/python3.8/site-packages (from dask&lt;=2021.11.2,&gt;=2021.09.1-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (2024.9.0)\nRequirement already satisfied: partd&gt;=0.3.10 in /databricks/python3/lib/python3.8/site-packages (from dask&lt;=2021.11.2,&gt;=2021.09.1-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (1.4.1)\nRequirement already satisfied: zict&gt;=0.1.3 in /databricks/python3/lib/python3.8/site-packages (from distributed&lt;=2021.11.2,&gt;=2021.09.1-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (3.0.0)\nRequirement already satisfied: tornado&gt;=6.0.3 in /databricks/python3/lib/python3.8/site-packages (from distributed&lt;=2021.11.2,&gt;=2021.09.1-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (6.1)\nRequirement already satisfied: tblib&gt;=1.6.0 in /databricks/python3/lib/python3.8/site-packages (from distributed&lt;=2021.11.2,&gt;=2021.09.1-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (3.0.0)\nRequirement already satisfied: jinja2 in /databricks/python3/lib/python3.8/site-packages (from distributed&lt;=2021.11.2,&gt;=2021.09.1-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (3.1.4)\nRequirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /databricks/python3/lib/python3.8/site-packages (from distributed&lt;=2021.11.2,&gt;=2021.09.1-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (2.4.0)\nRequirement already satisfied: click&gt;=6.6 in /databricks/python3/lib/python3.8/site-packages (from distributed&lt;=2021.11.2,&gt;=2021.09.1-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (8.1.7)\nRequirement already satisfied: msgpack&gt;=0.6.0 in /databricks/python3/lib/python3.8/site-packages (from distributed&lt;=2021.11.2,&gt;=2021.09.1-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (1.1.0)\nCollecting responses&lt;0.19\n  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\nCollecting dill\n  Using cached dill-0.3.9-py3-none-any.whl (119 kB)\nCollecting datasets&gt;=2.0.0\n  Using cached datasets-3.0.1-py3-none-any.whl (471 kB)\nCollecting multiprocess\n  Using cached multiprocess-0.70.17-py38-none-any.whl (132 kB)\nCollecting huggingface-hub&gt;=0.7.0\n  Using cached huggingface_hub-0.25.2-py3-none-any.whl (436 kB)\nCollecting xxhash\n  Using cached xxhash-3.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\nCollecting pyarrow&gt;=15.0.0\n  Using cached pyarrow-17.0.0-cp38-cp38-manylinux_2_28_x86_64.whl (40.0 MB)\nCollecting aiohttp\n  Using cached aiohttp-3.10.10-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\nCollecting dill\n  Using cached dill-0.3.8-py3-none-any.whl (116 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from datasets&gt;=2.0.0-&gt;evaluate&lt;=0.2.2-&gt;autogluon.multimodal==0.6.0-&gt;autogluon==0.6.0) (3.6.0)\nCollecting fsspec[http]&gt;=2021.05.0\n  Using cached fsspec-2024.6.1-py3-none-any.whl (177 kB)\nCollecting fastcore&lt;1.8,&gt;=1.5.29\n  Using cached fastcore-1.7.13-py3-none-any.whl (80 kB)\nCollecting fastprogress&gt;=0.2.4\n  Using cached fastprogress-1.0.3-py3-none-any.whl (12 kB)\nCollecting fastdownload&lt;2,&gt;=0.0.5\n  Using cached fastdownload-0.0.7-py3-none-any.whl (12 kB)\nRequirement already satisfied: pip in /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/lib/python3.8/site-packages (from fastai&lt;2.8,&gt;=2.3.1-&gt;autogluon.tabular[all]==0.6.0-&gt;autogluon==0.6.0) (21.0.1)\nCollecting spacy&lt;4\n  Using cached spacy-3.8.2.tar.gz (1.3 MB)\n  Installing build dependencies: started\n  Installing build dependencies: finished with status &#39;error&#39;\n  ERROR: Command errored out with exit status 1:\n   command: /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/bin/python /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/lib/python3.8/site-packages/pip install --ignore-installed --no-user --prefix /tmp/pip-build-env-ux529r2t/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools &#39;cython&gt;=0.25,&lt;3.0&#39; &#39;cymem&gt;=2.0.2,&lt;2.1.0&#39; &#39;preshed&gt;=3.0.2,&lt;3.1.0&#39; &#39;murmurhash&gt;=0.28.0,&lt;1.1.0&#39; &#39;thinc&gt;=8.3.0,&lt;8.4.0&#39; &#39;numpy&gt;=2.0.0,&lt;2.1.0; python_version &lt; &#39;&#34;&#39;&#34;&#39;3.9&#39;&#34;&#39;&#34;&#39;&#39; &#39;numpy&gt;=2.0.0,&lt;2.1.0; python_version &gt;= &#39;&#34;&#39;&#34;&#39;3.9&#39;&#34;&#39;&#34;&#39;&#39;\n       cwd: None\n  Complete output (193 lines):\n  Ignoring numpy: markers &#39;python_version &gt;= &#34;3.9&#34;&#39; don&#39;t match your environment\n  Collecting setuptools\n    Using cached setuptools-75.1.0-py3-none-any.whl (1.2 MB)\n  Collecting cython&lt;3.0,&gt;=0.25\n    Using cached Cython-0.29.37-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n  Collecting cymem&lt;2.1.0,&gt;=2.0.2\n    Using cached cymem-2.0.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (46 kB)\n  Collecting preshed&lt;3.1.0,&gt;=3.0.2\n    Using cached preshed-3.0.9-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (154 kB)\n  Collecting murmurhash&lt;1.1.0,&gt;=0.28.0\n    Using cached murmurhash-1.0.10-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n  Collecting thinc&lt;8.4.0,&gt;=8.3.0\n    Using cached thinc-8.3.2.tar.gz (193 kB)\n    Installing build dependencies: started\n    Installing build dependencies: finished with status &#39;error&#39;\n    ERROR: Command errored out with exit status 1:\n     command: /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/bin/python /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/lib/python3.8/site-packages/pip install --ignore-installed --no-user --prefix /tmp/pip-build-env-sufoo46x/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools &#39;cython&gt;=0.25,&lt;3.0&#39; &#39;murmurhash&gt;=1.0.2,&lt;1.1.0&#39; &#39;cymem&gt;=2.0.2,&lt;2.1.0&#39; &#39;preshed&gt;=3.0.2,&lt;3.1.0&#39; &#39;blis&gt;=1.0.0,&lt;1.1.0&#39; &#39;numpy&gt;=2.0.0,&lt;2.1.0; python_version &lt; &#39;&#34;&#39;&#34;&#39;3.9&#39;&#34;&#39;&#34;&#39;&#39; &#39;numpy&gt;=2.0.0,&lt;2.1.0; python_version &gt;= &#39;&#34;&#39;&#34;&#39;3.9&#39;&#34;&#39;&#34;&#39;&#39;\n         cwd: None\n    Complete output (50 lines):\n    Ignoring numpy: markers &#39;python_version &gt;= &#34;3.9&#34;&#39; don&#39;t match your environment\n    Collecting setuptools\n      Using cached setuptools-75.1.0-py3-none-any.whl (1.2 MB)\n    Collecting cython&lt;3.0,&gt;=0.25\n      Using cached Cython-0.29.37-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n    Collecting murmurhash&lt;1.1.0,&gt;=1.0.2\n      Using cached murmurhash-1.0.10-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n    Collecting cymem&lt;2.1.0,&gt;=2.0.2\n      Using cached cymem-2.0.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (46 kB)\n    Collecting preshed&lt;3.1.0,&gt;=3.0.2\n      Using cached preshed-3.0.9-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (154 kB)\n    Collecting blis&lt;1.1.0,&gt;=1.0.0\n      Using cached blis-1.0.1.tar.gz (3.6 MB)\n      Installing build dependencies: started\n      Installing build dependencies: finished with status &#39;error&#39;\n      ERROR: Command errored out with exit status 1:\n       command: /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/bin/python /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/lib/python3.8/site-packages/pip install --ignore-installed --no-user --prefix /tmp/pip-build-env-8y60wzc5/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools &#39;cython&gt;=0.25&#39; &#39;numpy&gt;=2.0.0,&lt;3.0.0&#39;\n           cwd: None\n      Complete output (8 lines):\n      Collecting setuptools\n        Using cached setuptools-75.1.0-py3-none-any.whl (1.2 MB)\n      Collecting cython&gt;=0.25\n        Using cached Cython-3.0.11-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n      ERROR: Could not find a version that satisfies the requirement numpy&lt;3.0.0,&gt;=2.0.0\n      ERROR: No matching distribution found for numpy&lt;3.0.0,&gt;=2.0.0\n      WARNING: You are using pip version 21.0.1; however, version 24.2 is available.\n      You should consider upgrading via the &#39;/local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/bin/python -m pip install --upgrade pip&#39; command.\n      ----------------------------------------\n    WARNING: Discarding https://files.pythonhosted.org/packages/bd/e4/741f20c9b767330e2605d4c71a775303cb6a9c72764b8802232fe6c7afad/blis-1.0.1.tar.gz#sha256=91739cd850ca8100dcddbd8ad66942cab20c9473cdea9a35b165b11d7b8d91e4 (from https://pypi.org/simple/blis/). Command errored out with exit status 1: /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/bin/python /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/lib/python3.8/site-packages/pip install --ignore-installed --no-user --prefix /tmp/pip-build-env-8y60wzc5/overlay --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- setuptools &#39;cython&gt;=0.25&#39; &#39;numpy&gt;=2.0.0,&lt;3.0.0&#39; Check the logs for full command output.\n\n*** WARNING: skipped 19999 bytes of output ***\n\n  Using cached spacy-3.7.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\nCollecting multidict&lt;7.0,&gt;=4.5\n  Using cached multidict-6.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\nCollecting frozenlist&gt;=1.1.1\n  Using cached frozenlist-1.4.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (240 kB)\nRequirement already satisfied: attrs&gt;=17.3.0 in /databricks/python3/lib/python3.8/site-packages (from aiohttp-&gt;datasets&gt;=2.0.0-&gt;evaluate&lt;=0.2.2-&gt;autogluon.multimodal==0.6.0-&gt;autogluon==0.6.0) (20.3.0)\nCollecting aiosignal&gt;=1.1.2\n  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\nCollecting aiohappyeyeballs&gt;=2.3.0\n  Using cached aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\nCollecting async-timeout&lt;5.0,&gt;=4.0\n  Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\nCollecting yarl&lt;2.0,&gt;=1.12.0\n  Using cached yarl-1.14.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\nCollecting opencv-python\n  Using cached opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.5 MB)\nCollecting yacs\n  Using cached yacs-0.1.8-py3-none-any.whl (14 kB)\nCollecting autocfg\n  Using cached autocfg-0.0.8-py3-none-any.whl (13 kB)\nCollecting portalocker\n  Using cached portalocker-2.10.1-py3-none-any.whl (18 kB)\nCollecting pydantic~=1.7\n  Using cached pydantic-1.10.18-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\nCollecting toolz&gt;=0.8.2\n  Using cached toolz-0.12.1-py3-none-any.whl (56 kB)\nRequirement already satisfied: typing-extensions~=4.0 in /databricks/python3/lib/python3.8/site-packages (from gluonts~=0.11.0-&gt;autogluon.timeseries[all]==0.6.0-&gt;autogluon==0.6.0) (4.12.2)\nRequirement already satisfied: py4j in /databricks/python3/lib/python3.8/site-packages (from hyperopt&lt;0.2.8,&gt;=0.2.7-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (0.10.9.7)\nCollecting future\n  Using cached future-1.0.0-py3-none-any.whl (491 kB)\nRequirement already satisfied: pyrsistent&gt;=0.14.0 in /databricks/python3/lib/python3.8/site-packages (from jsonschema&lt;=4.8.0-&gt;autogluon.multimodal==0.6.0-&gt;autogluon==0.6.0) (0.17.3)\nRequirement already satisfied: wheel in /databricks/python3/lib/python3.8/site-packages (from lightgbm&lt;3.4,&gt;=3.3-&gt;autogluon.tabular[all]==0.6.0-&gt;autogluon==0.6.0) (0.36.2)\nCollecting regex&gt;=2021.8.3\n  Using cached regex-2024.9.11-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (785 kB)\nCollecting typish&gt;=1.7.0\n  Using cached typish-1.9.3-py3-none-any.whl (45 kB)\nCollecting antlr4-python3-runtime==4.8\n  Using cached antlr4_python3_runtime-4.8-py3-none-any.whl\nCollecting model-index\n  Using cached model_index-0.1.11-py3-none-any.whl (34 kB)\nCollecting colorama\n  Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\nCollecting rich\n  Using cached rich-13.9.2-py3-none-any.whl (242 kB)\nRequirement already satisfied: tabulate in /databricks/python3/lib/python3.8/site-packages (from openmim&lt;=0.2.1,&gt;0.1.5-&gt;autogluon.multimodal==0.6.0-&gt;autogluon==0.6.0) (0.9.0)\nRequirement already satisfied: pytz&gt;=2020.1 in /databricks/python3/lib/python3.8/site-packages (from pandas!=1.4.0,&lt;1.6,&gt;=1.2.5-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (2020.5)\nRequirement already satisfied: python-dateutil&gt;=2.8.1 in /databricks/python3/lib/python3.8/site-packages (from pandas!=1.4.0,&lt;1.6,&gt;=1.2.5-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (2.8.1)\nRequirement already satisfied: locket in /databricks/python3/lib/python3.8/site-packages (from partd&gt;=0.3.10-&gt;dask&lt;=2021.11.2,&gt;=2021.09.1-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (1.0.0)\nRequirement already satisfied: urllib3 in /databricks/python3/lib/python3.8/site-packages (from pmdarima~=1.8.2-&gt;autogluon.timeseries[all]==0.6.0-&gt;autogluon==0.6.0) (1.26.15)\nRequirement already satisfied: Cython!=0.29.18,&gt;=0.29 in /databricks/python3/lib/python3.8/site-packages (from pmdarima~=1.8.2-&gt;autogluon.timeseries[all]==0.6.0-&gt;autogluon==0.6.0) (0.29.23)\nRequirement already satisfied: cycler&gt;=0.10 in /databricks/python3/lib/python3.8/site-packages (from matplotlib-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (0.10.0)\nRequirement already satisfied: pyparsing&gt;=2.2.1 in /databricks/python3/lib/python3.8/site-packages (from matplotlib-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (2.4.7)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /databricks/python3/lib/python3.8/site-packages (from matplotlib-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (1.3.1)\nCollecting tensorboard&gt;=2.9.1\n  Using cached tensorboard-2.14.0-py3-none-any.whl (5.5 MB)\nCollecting pyDeprecate&gt;=0.3.1\n  Using cached pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\nRequirement already satisfied: virtualenv in /usr/local/lib/python3.8/dist-packages (from ray[tune]&lt;2.1,&gt;=2.0-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (20.4.1)\nRequirement already satisfied: protobuf&lt;4.0.0,&gt;=3.15.3 in /databricks/python3/lib/python3.8/site-packages (from ray[tune]&lt;2.1,&gt;=2.0-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (3.17.2)\nCollecting grpcio&lt;=1.43.0,&gt;=1.32.0\n  Using cached grpcio-1.43.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\nCollecting click&gt;=6.6\n  Using cached click-8.0.4-py3-none-any.whl (97 kB)\nCollecting tensorboardX&gt;=1.9\n  Using cached tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /databricks/python3/lib/python3.8/site-packages (from requests-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (2020.12.5)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /databricks/python3/lib/python3.8/site-packages (from requests-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (2.10)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /databricks/python3/lib/python3.8/site-packages (from requests-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (3.4.0)\nCollecting PyWavelets&gt;=1.1.1\n  Using cached PyWavelets-1.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.9 MB)\nCollecting imageio&gt;=2.4.1\n  Using cached imageio-2.35.1-py3-none-any.whl (315 kB)\nCollecting tifffile&gt;=2019.7.26\n  Using cached tifffile-2023.7.10-py3-none-any.whl (220 kB)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /databricks/python3/lib/python3.8/site-packages (from scikit-learn&lt;1.2,&gt;=1.0.0-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (2.1.0)\nRequirement already satisfied: numba&gt;=0.53 in /databricks/python3/lib/python3.8/site-packages (from sktime&lt;0.14,&gt;=0.13.1-&gt;autogluon.timeseries[all]==0.6.0-&gt;autogluon==0.6.0) (0.58.1)\nRequirement already satisfied: deprecated&gt;=1.2.13 in /databricks/python3/lib/python3.8/site-packages (from sktime&lt;0.14,&gt;=0.13.1-&gt;autogluon.timeseries[all]==0.6.0-&gt;autogluon==0.6.0) (1.2.14)\nRequirement already satisfied: wrapt&lt;2,&gt;=1.10 in /databricks/python3/lib/python3.8/site-packages (from deprecated&gt;=1.2.13-&gt;sktime&lt;0.14,&gt;=0.13.1-&gt;autogluon.timeseries[all]==0.6.0-&gt;autogluon==0.6.0) (1.16.0)\nRequirement already satisfied: llvmlite&lt;0.42,&gt;=0.41.0dev0 in /databricks/python3/lib/python3.8/site-packages (from numba&gt;=0.53-&gt;sktime&lt;0.14,&gt;=0.13.1-&gt;autogluon.timeseries[all]==0.6.0-&gt;autogluon==0.6.0) (0.41.1)\nRequirement already satisfied: importlib-metadata in /databricks/python3/lib/python3.8/site-packages (from numba&gt;=0.53-&gt;sktime&lt;0.14,&gt;=0.13.1-&gt;autogluon.timeseries[all]==0.6.0-&gt;autogluon==0.6.0) (6.11.0)\nCollecting spacy-loggers&lt;2.0.0,&gt;=1.0.0\n  Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\nCollecting preshed&lt;3.1.0,&gt;=3.0.2\n  Using cached preshed-3.0.9-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (154 kB)\nCollecting weasel&lt;0.5.0,&gt;=0.1.0\n  Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\nCollecting spacy-legacy&lt;3.1.0,&gt;=3.0.11\n  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\nCollecting catalogue&lt;2.1.0,&gt;=2.0.6\n  Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\nCollecting typer&lt;1.0.0,&gt;=0.3.0\n  Using cached typer-0.12.5-py3-none-any.whl (47 kB)\nCollecting srsly&lt;3.0.0,&gt;=2.4.3\n  Using cached srsly-2.4.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (494 kB)\nCollecting thinc&lt;8.3.0,&gt;=8.2.2\n  Using cached thinc-8.2.5-cp38-cp38-linux_x86_64.whl\nCollecting murmurhash&lt;1.1.0,&gt;=0.28.0\n  Using cached murmurhash-1.0.10-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\nCollecting wasabi&lt;1.2.0,&gt;=0.9.1\n  Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\nCollecting cymem&lt;2.1.0,&gt;=2.0.2\n  Using cached cymem-2.0.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (46 kB)\nCollecting langcodes&lt;4.0.0,&gt;=3.2.0\n  Using cached langcodes-3.4.1-py3-none-any.whl (182 kB)\nCollecting language-data&gt;=1.2\n  Using cached language_data-1.2.0-py3-none-any.whl (5.4 MB)\nCollecting marisa-trie&gt;=0.7.7\n  Using cached marisa_trie-1.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\nRequirement already satisfied: patsy&gt;=0.5.2 in /databricks/python3/lib/python3.8/site-packages (from statsmodels~=0.13.0-&gt;autogluon.timeseries[all]==0.6.0-&gt;autogluon==0.6.0) (0.5.6)\nCollecting absl-py&gt;=0.4\n  Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\nCollecting google-auth-oauthlib&lt;1.1,&gt;=0.5\n  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\nCollecting tensorboard-data-server&lt;0.8.0,&gt;=0.7.0\n  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\nCollecting google-auth&lt;3,&gt;=1.6.3\n  Using cached google_auth-2.35.0-py2.py3-none-any.whl (208 kB)\nRequirement already satisfied: werkzeug&gt;=1.0.1 in /databricks/python3/lib/python3.8/site-packages (from tensorboard&gt;=2.9.1-&gt;pytorch-lightning&lt;1.8.0,&gt;=1.7.4-&gt;autogluon.multimodal==0.6.0-&gt;autogluon==0.6.0) (3.0.4)\nCollecting protobuf&lt;4.0.0,&gt;=3.15.3\n  Using cached protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\nCollecting tensorboard&gt;=2.9.1\n  Using cached tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n  Using cached tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n  Using cached tensorboard-2.12.2-py3-none-any.whl (5.6 MB)\nCollecting tensorboard-plugin-wit&gt;=1.6.0\n  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\nCollecting tensorboard&gt;=2.9.1\n  Using cached tensorboard-2.12.1-py3-none-any.whl (5.6 MB)\n  Using cached tensorboard-2.12.0-py3-none-any.whl (5.6 MB)\nCollecting google-auth-oauthlib&lt;0.5,&gt;=0.4.1\n  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\nCollecting tensorboard&gt;=2.9.1\n  Using cached tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\nCollecting tensorboard-data-server&lt;0.7.0,&gt;=0.6.0\n  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\nRequirement already satisfied: markdown&gt;=2.6.8 in /databricks/python3/lib/python3.8/site-packages (from tensorboard&gt;=2.9.1-&gt;pytorch-lightning&lt;1.8.0,&gt;=1.7.4-&gt;autogluon.multimodal==0.6.0-&gt;autogluon==0.6.0) (3.7)\nCollecting pyasn1-modules&gt;=0.2.1\n  Using cached pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\nCollecting cachetools&lt;6.0,&gt;=2.0.0\n  Using cached cachetools-5.5.0-py3-none-any.whl (9.5 kB)\nCollecting rsa&lt;5,&gt;=3.1.4\n  Using cached rsa-4.9-py3-none-any.whl (34 kB)\nCollecting requests-oauthlib&gt;=0.7.0\n  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\nRequirement already satisfied: zipp&gt;=0.5 in /databricks/python3/lib/python3.8/site-packages (from importlib-metadata-&gt;numba&gt;=0.53-&gt;sktime&lt;0.14,&gt;=0.13.1-&gt;autogluon.timeseries[all]==0.6.0-&gt;autogluon==0.6.0) (3.20.2)\nCollecting pyasn1&lt;0.7.0,&gt;=0.4.6\n  Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\nRequirement already satisfied: oauthlib&gt;=3.0.0 in /databricks/python3/lib/python3.8/site-packages (from requests-oauthlib&gt;=0.7.0-&gt;google-auth-oauthlib&lt;0.5,&gt;=0.4.1-&gt;tensorboard&gt;=2.9.1-&gt;pytorch-lightning&lt;1.8.0,&gt;=1.7.4-&gt;autogluon.multimodal==0.6.0-&gt;autogluon==0.6.0) (3.2.2)\nCollecting confection&lt;1.0.0,&gt;=0.0.1\n  Using cached confection-0.1.5-py3-none-any.whl (35 kB)\nCollecting blis&lt;0.8.0,&gt;=0.7.8\n  Using cached blis-0.7.11-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\nCollecting tokenizers!=0.11.3,&lt;0.14,&gt;=0.11.1\n  Using cached tokenizers-0.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\nCollecting shellingham&gt;=1.3.0\n  Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\nCollecting pygments&lt;3.0.0,&gt;=2.13.0\n  Using cached pygments-2.18.0-py3-none-any.whl (1.2 MB)\nCollecting markdown-it-py&gt;=2.2.0\n  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\nCollecting mdurl~=0.1\n  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\nCollecting cloudpathlib&lt;1.0.0,&gt;=0.7.0\n  Using cached cloudpathlib-0.19.0-py3-none-any.whl (49 kB)\nRequirement already satisfied: MarkupSafe&gt;=2.1.1 in /databricks/python3/lib/python3.8/site-packages (from werkzeug&gt;=1.0.1-&gt;tensorboard&gt;=2.9.1-&gt;pytorch-lightning&lt;1.8.0,&gt;=1.7.4-&gt;autogluon.multimodal==0.6.0-&gt;autogluon==0.6.0) (2.1.5)\nCollecting propcache&gt;=0.2.0\n  Using cached propcache-0.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\nRequirement already satisfied: jmespath&lt;1.0.0,&gt;=0.7.1 in /databricks/python3/lib/python3.8/site-packages (from boto3-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (0.10.0)\nRequirement already satisfied: s3transfer&lt;0.4.0,&gt;=0.3.0 in /databricks/python3/lib/python3.8/site-packages (from boto3-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (0.3.7)\nRequirement already satisfied: botocore&lt;1.20.0,&gt;=1.19.7 in /databricks/python3/lib/python3.8/site-packages (from boto3-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (1.19.63)\nCollecting ordered-set\n  Using cached ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\nCollecting multiprocess\n  Using cached multiprocess-0.70.16-py38-none-any.whl (132 kB)\nRequirement already satisfied: tenacity&gt;=6.2.0 in /databricks/python3/lib/python3.8/site-packages (from plotly-&gt;catboost&lt;1.2,&gt;=1.0-&gt;autogluon.tabular[all]==0.6.0-&gt;autogluon==0.6.0) (8.0.1)\nRequirement already satisfied: appdirs&lt;2,&gt;=1.4.3 in /usr/local/lib/python3.8/dist-packages (from virtualenv-&gt;ray[tune]&lt;2.1,&gt;=2.0-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (1.4.4)\nRequirement already satisfied: distlib&lt;1,&gt;=0.3.1 in /usr/local/lib/python3.8/dist-packages (from virtualenv-&gt;ray[tune]&lt;2.1,&gt;=2.0-&gt;autogluon.core[all]==0.6.0-&gt;autogluon==0.6.0) (0.3.4)\nInstalling collected packages: toolz, pyasn1, propcache, multidict, mdurl, fsspec, frozenlist, yarl, rsa, pygments, pyasn1-modules, psutil, Pillow, markdown-it-py, joblib, click, catalogue, cachetools, async-timeout, aiosignal, aiohappyeyeballs, srsly, shellingham, rich, requests-oauthlib, pydantic, protobuf, murmurhash, marisa-trie, grpcio, google-auth, dill, cymem, autogluon.common, aiohttp, xxhash, wasabi, typer, torch, tifffile, tensorboardX, tensorboard-plugin-wit, tensorboard-data-server, smart-open, ray, PyWavelets, pyDeprecate, pyarrow, preshed, ordered-set, opencv-python-headless, multiprocess, language-data, imageio, huggingface-hub, graphviz, google-auth-oauthlib, future, confection, cloudpathlib, blis, autogluon.features, autogluon.core, absl-py, xgboost, weasel, typish, torchvision, torchmetrics, tokenizers, thinc, tensorboard, statsmodels, spacy-loggers, spacy-legacy, scikit-image, responses, regex, qudida, model-index, lightgbm, langcodes, hyperopt, fastprogress, fastcore, datasets, colorama, catboost, autogluon.tabular, antlr4-python3-runtime, yacs, transformers, torchtext, timm, text-unidecode, spacy, seqeval, sentencepiece, pytorch-metric-learning, pytorch-lightning, pycocotools, portalocker, pmdarima, openmim, opencv-python, omegaconf, nptyping, nltk, nlpaug, gluonts, fastdownload, fairscale, evaluate, autocfg, albumentations, accelerate, tbats, gluoncv, fastai, autogluon.timeseries, autogluon.multimodal, autogluon.vision, autogluon.text, autogluon\n  Attempting uninstall: toolz\n    Found existing installation: toolz 1.0.0\n    Not uninstalling toolz at /databricks/python3/lib/python3.8/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7\n    Can&#39;t uninstall &#39;toolz&#39;. No files were found to uninstall.\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2024.9.0\n    Not uninstalling fsspec at /databricks/python3/lib/python3.8/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7\n    Can&#39;t uninstall &#39;fsspec&#39;. No files were found to uninstall.\n  Attempting uninstall: pygments\n    Found existing installation: Pygments 2.8.1\n    Not uninstalling pygments at /databricks/python3/lib/python3.8/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7\n    Can&#39;t uninstall &#39;Pygments&#39;. No files were found to uninstall.\n  Attempting uninstall: psutil\n    Found existing installation: psutil 5.9.8\n    Not uninstalling psutil at /databricks/python3/lib/python3.8/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7\n    Can&#39;t uninstall &#39;psutil&#39;. No files were found to uninstall.\n  Attempting uninstall: Pillow\n    Found existing installation: Pillow 8.2.0\n    Not uninstalling pillow at /databricks/python3/lib/python3.8/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7\n    Can&#39;t uninstall &#39;Pillow&#39;. No files were found to uninstall.\n  Attempting uninstall: joblib\n    Found existing installation: joblib 1.0.1\n    Not uninstalling joblib at /databricks/python3/lib/python3.8/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7\n    Can&#39;t uninstall &#39;joblib&#39;. No files were found to uninstall.\n  Attempting uninstall: click\n    Found existing installation: click 8.1.7\n    Not uninstalling click at /databricks/python3/lib/python3.8/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7\n    Can&#39;t uninstall &#39;click&#39;. No files were found to uninstall.\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 3.17.2\n    Not uninstalling protobuf at /databricks/python3/lib/python3.8/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7\n    Can&#39;t uninstall &#39;protobuf&#39;. No files were found to uninstall.\n  Attempting uninstall: autogluon.common\n    Found existing installation: autogluon.common 0.5.2\n    Not uninstalling autogluon.common at /databricks/python3/lib/python3.8/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7\n    Can&#39;t uninstall &#39;autogluon.common&#39;. No files were found to uninstall.\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 4.0.0\n    Not uninstalling pyarrow at /databricks/python3/lib/python3.8/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7\n    Can&#39;t uninstall &#39;pyarrow&#39;. No files were found to uninstall.\n  Attempting uninstall: autogluon.features\n    Found existing installation: autogluon.features 0.5.2\n    Not uninstalling autogluon.features at /databricks/python3/lib/python3.8/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7\n    Can&#39;t uninstall &#39;autogluon.features&#39;. No files were found to uninstall.\n  Attempting uninstall: autogluon.core\n    Found existing installation: autogluon.core 0.5.2\n    Not uninstalling autogluon.core at /databricks/python3/lib/python3.8/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7\n    Can&#39;t uninstall &#39;autogluon.core&#39;. No files were found to uninstall.\n  Attempting uninstall: xgboost\n    Found existing installation: xgboost 2.1.1\n    Not uninstalling xgboost at /databricks/python3/lib/python3.8/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7\n    Can&#39;t uninstall &#39;xgboost&#39;. No files were found to uninstall.\n  Attempting uninstall: statsmodels\n    Found existing installation: statsmodels 0.14.1\n    Not uninstalling statsmodels at /databricks/python3/lib/python3.8/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7\n    Can&#39;t uninstall &#39;statsmodels&#39;. No files were found to uninstall.\n  Attempting uninstall: lightgbm\n    Found existing installation: lightgbm 4.0.0\n    Not uninstalling lightgbm at /databricks/python3/lib/python3.8/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7\n    Can&#39;t uninstall &#39;lightgbm&#39;. No files were found to uninstall.\n  Attempting uninstall: autogluon.tabular\n    Found existing installation: autogluon.tabular 0.5.2\n    Not uninstalling autogluon.tabular at /databricks/python3/lib/python3.8/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7\n    Can&#39;t uninstall &#39;autogluon.tabular&#39;. No files were found to uninstall.\n  Attempting uninstall: pmdarima\n    Found existing installation: pmdarima 2.0.4\n    Not uninstalling pmdarima at /databricks/python3/lib/python3.8/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7\n    Can&#39;t uninstall &#39;pmdarima&#39;. No files were found to uninstall.\nERROR: pip&#39;s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nphgml 1.5.0 requires autogluon-tabular==0.5.2, but you have autogluon-tabular 0.6.0 which is incompatible.\nphgml 1.5.0 requires lightgbm==4.0.0, but you have lightgbm 3.3.5 which is incompatible.\nphgml 1.5.0 requires xgboost&lt;3.0.0,&gt;=2.0.3, but you have xgboost 1.7.6 which is incompatible.\nmlflow 2.2.2 requires pyarrow&lt;12,&gt;=4.0.0, but you have pyarrow 17.0.0 which is incompatible.\nflask 2.3.3 requires click&gt;=8.1.3, but you have click 8.0.4 which is incompatible.\nSuccessfully installed Pillow-9.0.1 PyWavelets-1.4.1 absl-py-2.1.0 accelerate-0.13.2 aiohappyeyeballs-2.4.3 aiohttp-3.10.10 aiosignal-1.3.1 albumentations-1.1.0 antlr4-python3-runtime-4.8 async-timeout-4.0.3 autocfg-0.0.8 autogluon-0.6.0 autogluon.common-0.6.0 autogluon.core-0.6.0 autogluon.features-0.6.0 autogluon.multimodal-0.6.0 autogluon.tabular-0.6.0 autogluon.text-0.6.0 autogluon.timeseries-0.6.0 autogluon.vision-0.6.0 blis-0.7.11 cachetools-5.5.0 catalogue-2.0.10 catboost-1.1.1 click-8.0.4 cloudpathlib-0.19.0 colorama-0.4.6 confection-0.1.5 cymem-2.0.8 datasets-3.0.1 dill-0.3.8 evaluate-0.2.2 fairscale-0.4.6 fastai-2.7.17 fastcore-1.7.13 fastdownload-0.0.7 fastprogress-1.0.3 frozenlist-1.4.1 fsspec-2024.6.1 future-1.0.0 gluoncv-0.10.5.post0 gluonts-0.11.12 google-auth-2.35.0 google-auth-oauthlib-0.4.6 graphviz-0.20.3 grpcio-1.43.0 huggingface-hub-0.25.2 hyperopt-0.2.7 imageio-2.35.1 joblib-1.4.2 langcodes-3.4.1 language-data-1.2.0 lightgbm-3.3.5 marisa-trie-1.2.0 markdown-it-py-3.0.0 mdurl-0.1.2 model-index-0.1.11 multidict-6.1.0 multiprocess-0.70.16 murmurhash-1.0.10 nlpaug-1.1.10 nltk-3.9.1 nptyping-1.4.4 omegaconf-2.1.2 opencv-python-4.10.0.84 opencv-python-headless-4.10.0.84 openmim-0.2.1 ordered-set-4.1.0 pmdarima-1.8.5 portalocker-2.10.1 preshed-3.0.9 propcache-0.2.0 protobuf-3.20.3 psutil-5.8.0 pyDeprecate-0.3.2 pyarrow-17.0.0 pyasn1-0.6.1 pyasn1-modules-0.4.1 pycocotools-2.0.6 pydantic-1.10.18 pygments-2.18.0 pytorch-lightning-1.7.7 pytorch-metric-learning-1.3.2 qudida-0.0.4 ray-2.0.1 regex-2024.9.11 requests-oauthlib-2.0.0 responses-0.18.0 rich-13.9.2 rsa-4.9 scikit-image-0.19.3 sentencepiece-0.1.99 seqeval-1.2.2 shellingham-1.5.4 smart-open-5.2.1 spacy-3.7.5 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 statsmodels-0.13.5 tbats-1.1.3 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorboardX-2.6.2.2 text-unidecode-1.3 thinc-8.2.5 tifffile-2023.7.10 timm-0.6.13 tokenizers-0.13.3 toolz-0.12.1 torch-1.12.1 torchmetrics-0.8.2 torchtext-0.13.1 torchvision-0.13.1 transformers-4.23.1 typer-0.12.5 typish-1.9.3 wasabi-1.1.3 weasel-0.4.1 xgboost-1.7.6 xxhash-3.5.0 yacs-0.1.8 yarl-1.14.0\nPython interpreter will be restarted.\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pip install mlflow\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcc10ce7-548b-41b4-a4fe-598457be0e8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33daa7f6-0f16-4f7a-8233-ad68bf89d3bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">sys.version_info(major=3, minor=8, micro=10, releaselevel=&#39;final&#39;, serial=0)\n",
       "Python version: 3.8.10\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">sys.version_info(major=3, minor=8, micro=10, releaselevel=&#39;final&#39;, serial=0)\nPython version: 3.8.10\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version_info)\n",
    "print(f\"Python version: {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06292424-691c-4a24-8667-b72c83930280",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install autogluon==0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afe309c6-c388-45f9-8cca-722637c2b6dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.widgets.dropdown(\"env_stage\", \"dev\", [\"dev\", \"prod\"], \"Pipeline stage\")\n",
    "dbutils.widgets.dropdown(\"exclude_pms\", \"False\", [\"True\", \"False\"], \"Exclude PMS\")\n",
    "dbutils.widgets.dropdown(\"target_type\", \"REVENUE\", [\"REVENUE\", \"ROOMS\"], \"Target Type\")\n",
    "dbutils.widgets.dropdown(\"is_usd_currency\", \"True\", [\"True\", \"False\"], \"Use USD currency\")\n",
    "dbutils.widgets.text(\"selected_hotels\", \"\", \"Hotels\")\n",
    "dbutils.widgets.text(\"lag_numbers\",\"1,7,14,28\", \"Lag Numbers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64c58489-de6f-47a8-8327-e2c1a65f8ce0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import os\n",
    "# from autogluon.core.utils.loaders import load_pkl\n",
    "import logging\n",
    "import shutil\n",
    "import mlflow\n",
    "from mlflow import MlflowException\n",
    "import mlflow.pyfunc\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41f60c52-c50f-4a8c-b3b5-de70b17adce0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from autogluon.core.utils.loaders import load_pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56fa25c5-8fcd-42dc-a144-881cf2aa7b49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ENV = getArgument(\"env_stage\")\n",
    "\n",
    "REPOPATH = \"/Workspace/Repos/manik@surge.global/phg-data-mlsys/src\"\n",
    "cluster_name = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterName\") \n",
    "\n",
    "if (ENV == \"dev\") and (\"dev\" in cluster_name):\n",
    "    print(f\"Loading phgml package from repo {REPOPATH}\")\n",
    "    sys.path.append(os.path.abspath(REPOPATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26b143d3-b856-45cc-8760-0601ff041b72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# dbutils.fs.cp(\"dbfs:/FileStore/ml/base_model.py\", \"file:/tmp/base_model.py\")    \n",
    "# dbutils.fs.cp(\"dbfs:/FileStore/ml/model_strategy.py\", \"file:/tmp/model_strategy.py\") \n",
    "# dbutils.fs.cp(\"dbfs:/FileStore/ml/model_wrapper.py\", \"file:/tmp/model_wrapper.py\") \n",
    "# sys.path.append('/tmp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4610a17f-5969-463c-8f6e-b3e5ded21018",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from base_model import \n",
    "# from model_strategy import \n",
    "# from model_wrapper import ModelWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "598dc74f-5002-4d1a-9f62-b68a20e94a65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from phgml.models.model_wrapper import ModelWrapper\n",
    "# from phgml.models.model_strategy import StrategyLGBM, StrategyAG\n",
    "\n",
    "from phgml.data.processing_distr_ca import remove_padded_cols\n",
    "from phgml.reporting.output_metrics import *\n",
    "from phgml.reporting.report_results import get_output_df, interpolated_fill # , correct_prediction_list\n",
    "from phgml.data.data_types import inference_output_schema\n",
    "from phgml.reporting.logging import get_dbx_logger\n",
    "from phgml.data.config import ForecastingHotelConfigProvider,EnvironmentConfig\n",
    "from phgml.utilities.task_utilities import str_to_bool, str_to_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ef61a27-1859-43c5-a350-b5eb10b409e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import shutil\n",
    "import mlflow\n",
    "from mlflow.entities.model_registry.model_version import ModelVersion\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import datetime\n",
    "import cloudpickle\n",
    "from sys import version_info\n",
    "from mlflow import MlflowClient\n",
    "import glob\n",
    "from typing import Optional, Tuple, Union, List, Dict, Any\n",
    "\n",
    "\n",
    "__all__ = [\"BaseModel\"]\n",
    "\n",
    "\n",
    "class BaseModel(ABC):\n",
    "    \"\"\"Base model class to be used as a blueprint for other model classes which has common attributes and methods.\n",
    "    The methods can be overriden as neccessary based on the requirement.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_type: str,\n",
    "        prediction_horizon: int = 14,\n",
    "        lag_numbers: List[int] = [1, 7, 14, 28],\n",
    "        quantiles: List[float] = [0.1, 0.5, 0.9],\n",
    "        mlflow_run_id: Optional[str] = None,\n",
    "        hotel_id: Optional[str] = None,\n",
    "        version: Optional[Union[str, int]] = None,\n",
    "        stage: Optional[str] = None,\n",
    "        target_type: str = \"REVENUE\",\n",
    "        exclude_pms: bool = False,\n",
    "        save_models: bool = True,\n",
    "        local_root_dir: Optional[str] = None,\n",
    "        model_name_prefix: Optional[str] = None,\n",
    "        meta_data: Dict[str, Any] = {},\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if target_type not in [\"REVENUE\", \"ROOMS\", \"ADR\"]:\n",
    "            raise ValueError(\"target_type should be either REVENUE, ROOMS or ADR\")\n",
    "\n",
    "        if hotel_id is not None:\n",
    "            self.hotel_id = hotel_id\n",
    "        else:\n",
    "            self.hotel_id = \"000\"\n",
    "\n",
    "        if mlflow_run_id is not None:\n",
    "            self.run_id = mlflow_run_id\n",
    "        else:\n",
    "            self.run_id = datetime.datetime.now().strftime(\"%Y-%m-%d:%H-%M-%S\")\n",
    "\n",
    "        self.prediction_horizon = prediction_horizon\n",
    "        self.models: Dict[Union[str, int], Any] = {}\n",
    "        self.lag_numbers = lag_numbers\n",
    "        self.quantile_levels = quantiles\n",
    "        self.target_type = target_type\n",
    "        self.model_type = model_type\n",
    "        self.model_name_prefix = model_name_prefix\n",
    "        self.meta_data = meta_data\n",
    "\n",
    "        pms = \"PMS\"\n",
    "        if exclude_pms:\n",
    "            pms = \"NOPMS\"\n",
    "\n",
    "        self.local_root = (\n",
    "            f\"{self.model_type.lower()}_{self.hotel_id}_{self.target_type}_{pms}/\"\n",
    "        )\n",
    "\n",
    "        if local_root_dir is not None:\n",
    "            self.local_root = local_root_dir\n",
    "\n",
    "        self.local_dir = f\"{self.local_root}artifacts/\"\n",
    "        self.local_path = self.local_dir + \"model.pkl\"\n",
    "        self.artifacts = {}\n",
    "        self.artifacts[\"model_dir\"] = self.local_dir\n",
    "        self.version = version\n",
    "        self.stage = stage\n",
    "        self.target_prefix = \"RV\"  # booking axis column prefixes\n",
    "        if self.target_type == \"ROOMS\":\n",
    "            self.target_prefix = \"RM\"\n",
    "        elif self.target_type == \"ADR\":\n",
    "            self.target_prefix = \"ADR\"\n",
    "\n",
    "        self.exclude_pms = exclude_pms\n",
    "        self.do_save_models = save_models\n",
    "\n",
    "    def add_model_metadata(self, meta_key: str, meta_value: Any):\n",
    "        \"\"\"Adds metadata to the model during training. The added metadata will be saved as tags in the model version page in MLFlow model registry.\n",
    "\n",
    "        Args:\n",
    "            meta_key (str): key for the metadata\n",
    "            meta_value (str): Metadata value\n",
    "        \"\"\"\n",
    "        if meta_key in self.meta_data.keys():\n",
    "            raise ValueError(\n",
    "                \"The provided key {meta_value} already exists use a different key\"\n",
    "            )\n",
    "\n",
    "        self.meta_data[meta_key] = meta_value\n",
    "\n",
    "    def get_model_name(self) -> str:\n",
    "        \"\"\"Returns the model name used in creating the directory under mlflow experiment artifacts\"\"\"\n",
    "        target_type = self.target_type\n",
    "        if not self.exclude_pms:\n",
    "            target_type = target_type + \"_PMS\"\n",
    "\n",
    "        name = f\"{self.hotel_id}_{target_type}_{self.model_type}_model\"\n",
    "\n",
    "        if self.model_name_prefix is not None:\n",
    "            return f\"{self.model_name_prefix}_{name}\"\n",
    "\n",
    "        return name\n",
    "\n",
    "    def set_latest_model_version(self, model_stage: Optional[str] = None):\n",
    "        \"\"\"Sets the model version meta data and mlflow model version\n",
    "\n",
    "        Args:\n",
    "             model_stage (str): specify whether \"dev\", \"qa\" or \"prod\"\n",
    "\n",
    "         if model_stage specified, then will set the version meta and version of the\n",
    "         model with the specified model_stage, otherwise it will consider the latest version\n",
    "         of the model registered in mlflow.\n",
    "        \"\"\"\n",
    "        client = MlflowClient()\n",
    "        all_registered_models_info = client.search_model_versions(\n",
    "            f\"name ='{self.get_model_name()}'\"\n",
    "        )\n",
    "\n",
    "        # sorting the model meta data list by version number of the considered model name in descending order\n",
    "        sorted_model_versions = sorted(\n",
    "            all_registered_models_info, key=lambda x: int(x.version), reverse=True\n",
    "        )\n",
    "        if len(sorted_model_versions) == 0:\n",
    "            raise ValueError(\n",
    "                f\"there are no model versions for the model name: {self.get_model_name()}\"\n",
    "            )\n",
    "\n",
    "        if model_stage is None:\n",
    "\n",
    "            # 0 index will have the latest version\n",
    "            self.version_meta = sorted_model_versions[0]\n",
    "            self.version = self.version_meta.version\n",
    "            print(f\"Setting the latest version {self.version}\")\n",
    "        else:\n",
    "            version_meta_temp = None\n",
    "            for version_meta in sorted_model_versions:\n",
    "                if version_meta.tags.get(f\"model_stage_{model_stage}\") == \"yes\":\n",
    "                    version_meta_temp = version_meta\n",
    "            if version_meta_temp is None:\n",
    "                raise ValueError(\n",
    "                    f\"{self.get_model_name()} : Requested model_stage_{model_stage} = 'yes' is not among the registered {len(all_registered_models_info)} models\"\n",
    "                )\n",
    "            else:\n",
    "                self.stage = model_stage\n",
    "                self.version_meta = version_meta_temp\n",
    "                self.version = version_meta_temp.version\n",
    "                print(\n",
    "                    f\"Model Retrived: version: {self.version} current_stage: {self.stage}\"\n",
    "                )\n",
    "\n",
    "    def get_remote_model_metadata(self) -> Dict[str, Any]:\n",
    "        \"\"\"Returns model meta data\"\"\"\n",
    "        # if self.version_meta is None:\n",
    "        #     self.set_latest_model_version()\n",
    "\n",
    "        self.meta_data = self.version_meta.tags\n",
    "\n",
    "        return self.meta_data\n",
    "\n",
    "    def get_model_log_path(self) -> str:\n",
    "        \"\"\"Returns the directory path for mlflow artifacts\"\"\"\n",
    "        return f\"forecasting/{self.hotel_id}/models/{self.get_model_name()}\"\n",
    "\n",
    "    def get_model_register_path(self) -> str:\n",
    "        \"\"\"Returns the mlflow model registry path as per the mlflow experiment run id\"\"\"\n",
    "        return f\"runs:/{self.run_id}/{self.get_model_log_path()}\"\n",
    "\n",
    "    def get_model_uri(self, tag: Optional[Union[str, int]] = None) -> str:\n",
    "        \"\"\"Returns models uri\"\"\"\n",
    "        if tag is None:\n",
    "            tag = self.version\n",
    "\n",
    "        return f\"models:/{self.get_model_name()}/{tag}\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def save_model(self) -> None:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def log_models(self) -> None:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_pyfunc_model(\n",
    "        self, dst_path: Optional[str] = None, tag: Optional[Union[str, int]] = None\n",
    "    ) -> mlflow.pyfunc.PyFuncModel:\n",
    "        pass\n",
    "\n",
    "    # n_jobs parameter added for NearField training\n",
    "    # @abstractmethod\n",
    "    # def train(self, train_data: pd.DataFrame) -> None:\n",
    "    #     pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63864884-62bb-4556-8b4c-d040df55c87e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from typing import Optional, Tuple, Union, List, Dict, Any, Callable\n",
    "from lightgbm import LGBMRegressor\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "__all__ = [\"BaseStrategy\", \"StrategyLGBM\", \"StrategyAG\", \"StrategyLGBMFarField\"]\n",
    "\n",
    "\n",
    "class BaseStrategy(ABC):\n",
    "    model_type = \"base_strategy\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        day_ahead: int,\n",
    "        quantile_levels: list,\n",
    "        cd_axis_targets: list,\n",
    "        path: str,\n",
    "        is_auto_reg: bool,\n",
    "        # verbose: int,\n",
    "    ):\n",
    "        self.quantile_levels = quantile_levels\n",
    "        self.day_ahead = day_ahead\n",
    "        self.cd_axis_targets = cd_axis_targets\n",
    "        self.target_prefix = self.cd_axis_targets[0][:2]\n",
    "        self.path = path\n",
    "        self.autoregressive_predictions = is_auto_reg\n",
    "        # self.verbose = verbose\n",
    "        self.objective = \"quantile\"\n",
    "        self.sub_predictors: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "    @abstractmethod\n",
    "    def _fit(self, train_data: pd.DataFrame) -> None:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _predict(self, test_data: pd.Series) -> Dict[float, List[npt.NDArray]]:\n",
    "        pass\n",
    "\n",
    "\n",
    "class StrategyLGBM(BaseStrategy):\n",
    "    model_type = \"LGBM\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        day_ahead: int,\n",
    "        quantile_levels: list,\n",
    "        cd_axis_targets: list,\n",
    "        path: str,\n",
    "        is_auto_reg: bool,\n",
    "        n_jobs: int = 1,\n",
    "        # verbose: int = -1,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            day_ahead=day_ahead,\n",
    "            quantile_levels=quantile_levels,\n",
    "            cd_axis_targets=cd_axis_targets,\n",
    "            path=path,\n",
    "            is_auto_reg=is_auto_reg,\n",
    "            # verbose=verbose,\n",
    "        )\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def _fit(self, train_data: pd.DataFrame) -> None:\n",
    "        self.target_feature_dtypes = dict(train_data.dtypes)\n",
    "\n",
    "        # using lambda function to identify dropping cols, rather than using a if condition inside the for loop\n",
    "        drop_cols_func = lambda target_index: self.cd_axis_targets\n",
    "        if self.autoregressive_predictions:\n",
    "            drop_cols_func = lambda target_index: self.cd_axis_targets[target_index:]\n",
    "\n",
    "        for index, target_ in enumerate(self.cd_axis_targets):\n",
    "            x_data = train_data.drop(drop_cols_func(index), axis=1)\n",
    "            y_data = train_data[[target_]]\n",
    "\n",
    "            print(\n",
    "                \"\\t\\ttarget: \",\n",
    "                target_,\n",
    "                \" x_data_cols:\",\n",
    "                [col for col in x_data.columns if self.target_prefix in col][:6],\n",
    "                \" y_data_cols:\",\n",
    "                list(y_data.columns),\n",
    "            )\n",
    "\n",
    "            reg_objs = {}\n",
    "            for qtile in self.quantile_levels:\n",
    "                sub_predictor = LGBMRegressor(\n",
    "                    objective=self.objective,\n",
    "                    alpha=qtile,\n",
    "                    verbose=-1,\n",
    "                    n_jobs=self.n_jobs,\n",
    "                )\n",
    "                sub_predictor.fit(x_data, y_data)\n",
    "                reg_objs[qtile] = sub_predictor\n",
    "            self.sub_predictors[target_] = {\n",
    "                \"predictors\": reg_objs,\n",
    "                \"targets\": list(y_data.columns),\n",
    "                \"features\": list(x_data.columns),\n",
    "            }\n",
    "\n",
    "    def _predict(self, test_data: pd.Series) -> Dict[float, List[npt.NDArray]]:\n",
    "\n",
    "        # making sure dtypes are the same as in training, and filtering out the target columns from the dtypes dict since in test data its not there.\n",
    "        needed_dtypes = {\n",
    "            col: col_dtype\n",
    "            for col, col_dtype in self.target_feature_dtypes.items()\n",
    "            if \"_tgt\" not in col\n",
    "        }\n",
    "        test_data_cpy = (\n",
    "            test_data.to_frame().T.reset_index(drop=True).copy().astype(needed_dtypes)\n",
    "        )\n",
    "\n",
    "        data = {qtile: test_data_cpy.copy() for qtile in self.quantile_levels}\n",
    "        for index, target_ in enumerate(self.cd_axis_targets):\n",
    "            feature_cols = self.sub_predictors[target_][\"features\"]\n",
    "            other_cols = [col for col in feature_cols if self.target_prefix not in col]\n",
    "\n",
    "            # if self.verbose != -1:\n",
    "            print(\n",
    "                \"\\t\\ttarget: \",\n",
    "                target_,\n",
    "                \" x_data_cols:\",\n",
    "                feature_cols[:6],\n",
    "                \" other_cols :\",\n",
    "                other_cols[:6],\n",
    "            )\n",
    "            for qtile in self.quantile_levels:\n",
    "                pred = self.sub_predictors[target_][\"predictors\"][qtile].predict(\n",
    "                    data[0.5][feature_cols]\n",
    "                )\n",
    "                data[qtile][target_] = pred\n",
    "                data[qtile] = data[qtile].sort_index(axis=1)\n",
    "\n",
    "        data = {\n",
    "            qtile: data[qtile][self.cd_axis_targets].to_numpy() for qtile in data.keys()\n",
    "        }\n",
    "        return data\n",
    "\n",
    "\n",
    "class StrategyAG(BaseStrategy):\n",
    "    model_type = \"AG\"\n",
    "    excluded_models = [\"NN_TORCH\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        day_ahead: int,\n",
    "        quantile_levels: list,\n",
    "        cd_axis_targets: list,\n",
    "        path: str,\n",
    "        is_auto_reg: bool,\n",
    "        # verbose: int = -1,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            day_ahead=day_ahead,\n",
    "            quantile_levels=quantile_levels,\n",
    "            cd_axis_targets=cd_axis_targets,\n",
    "            path=path,\n",
    "            is_auto_reg=is_auto_reg,\n",
    "            # verbose=verbose,\n",
    "        )\n",
    "        self.included_model_types = [\"GBM\"]\n",
    "\n",
    "    def _fit(self, train_data: pd.DataFrame, **kwargs) -> None:\n",
    "        self.target_feature_dtypes = dict(train_data.dtypes)\n",
    "\n",
    "        # using lambda function to identify dropping cols, rather than using a if condition inside the for loop\n",
    "        drop_cols_func = lambda target_index: self.cd_axis_targets\n",
    "        if self.autoregressive_predictions:\n",
    "            drop_cols_func = lambda target_index: self.cd_axis_targets[target_index:]\n",
    "\n",
    "        for index, target_ in enumerate(self.cd_axis_targets):\n",
    "            x_data = train_data.drop(drop_cols_func(index), axis=1)\n",
    "            y_data = train_data[[target_]]\n",
    "\n",
    "            # if self.verbose != -1:\n",
    "            print(\n",
    "                \"\\t\\ttarget: \",\n",
    "                target_,\n",
    "                \" x_data_cols:\",\n",
    "                [col for col in x_data.columns if self.target_prefix in col][:6],\n",
    "                \" y_data_cols:\",\n",
    "                list(y_data.columns),\n",
    "            )\n",
    "\n",
    "            path_i = self.path + f\"day{self.day_ahead}_{target_}\"\n",
    "\n",
    "            sub_predictor = TabularPredictor(\n",
    "                label=target_,\n",
    "                problem_type=self.objective,\n",
    "                path=path_i,\n",
    "                quantile_levels=self.quantile_levels,\n",
    "                verbosity=1,\n",
    "            )\n",
    "\n",
    "            cols_to_consider = [target_] + list(x_data.columns)\n",
    "\n",
    "            sub_predictor.fit(\n",
    "                train_data=train_data[cols_to_consider],\n",
    "                tuning_data=None,\n",
    "                excluded_model_types=self.excluded_models,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "            self.sub_predictors[target_] = {\n",
    "                \"predictors\": sub_predictor,\n",
    "                \"targets\": list(y_data.columns),\n",
    "                \"features\": list(x_data.columns),\n",
    "            }\n",
    "\n",
    "    def _predict(self, test_data: pd.Series) -> Dict[float, List[npt.NDArray]]:\n",
    "\n",
    "        # making sure dtypes are the same as in training, and filtering out the target columns from the dtypes dict since in test data its not there.\n",
    "        needed_dtypes = {\n",
    "            col: col_dtype\n",
    "            for col, col_dtype in self.target_feature_dtypes.items()\n",
    "            if \"_tgt\" not in col\n",
    "        }\n",
    "        test_data_cpy = (\n",
    "            test_data.to_frame().T.reset_index(drop=True).copy().astype(needed_dtypes)\n",
    "        )\n",
    "\n",
    "        data = {qtile: test_data_cpy.copy() for qtile in self.quantile_levels}\n",
    "        for index, target_ in enumerate(self.cd_axis_targets):\n",
    "            feature_cols = self.sub_predictors[target_][\"features\"]\n",
    "            other_cols = [col for col in feature_cols if self.target_prefix not in col]\n",
    "\n",
    "            # if self.verbose != -1:\n",
    "            print(\n",
    "                \"\\t\\ttarget: \",\n",
    "                target_,\n",
    "                \" x_data_cols:\",\n",
    "                feature_cols[:6],\n",
    "                \" other_cols :\",\n",
    "                other_cols[:6],\n",
    "            )\n",
    "            pred = self.sub_predictors[target_][\"predictors\"].predict(\n",
    "                data[0.5][feature_cols]\n",
    "            )\n",
    "            for index, qtile in enumerate(self.quantile_levels):\n",
    "\n",
    "                data[qtile][target_] = pred.iloc[:, index]\n",
    "                data[qtile] = data[qtile].sort_index(axis=1)\n",
    "\n",
    "        data = {\n",
    "            qtile: data[qtile][self.cd_axis_targets].to_numpy() for qtile in data.keys()\n",
    "        }\n",
    "        return data\n",
    "\n",
    "\n",
    "class StrategyLGBMFarField(BaseStrategy):\n",
    "    model_type = \"LGBM_FARFIELD\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        day_ahead: int,\n",
    "        quantile_levels: list,\n",
    "        cd_axis_targets: list,\n",
    "        path: str,\n",
    "        is_auto_reg: bool,\n",
    "        # verbose: int = -1,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            day_ahead=day_ahead,\n",
    "            quantile_levels=quantile_levels,\n",
    "            cd_axis_targets=cd_axis_targets,\n",
    "            path=path,\n",
    "            is_auto_reg=is_auto_reg,\n",
    "            # verbose=verbose,\n",
    "        )\n",
    "\n",
    "    def _fit(self, train_data: pd.DataFrame) -> None:\n",
    "\n",
    "        x_data = train_data.drop([self.cd_axis_targets], axis=1)\n",
    "        self.target_feature_dtypes = dict(x_data.dtypes)\n",
    "        y_data = train_data[self.cd_axis_targets]\n",
    "        print(x_data)\n",
    "\n",
    "        # if self.verbose != -1:\n",
    "        print(\n",
    "            \"\\t\\ttarget: \",\n",
    "            self.cd_axis_targets,\n",
    "            \" x_data_cols:\",\n",
    "            x_data.columns.tolist(),\n",
    "            \" y_data_cols:\",\n",
    "            self.cd_axis_targets,\n",
    "        )\n",
    "\n",
    "        reg_objs = {}\n",
    "        for qtile in self.quantile_levels:\n",
    "            sub_predictor = LGBMRegressor(\n",
    "                objective=self.objective,\n",
    "                alpha=qtile,\n",
    "                verbose=-1,\n",
    "            )\n",
    "            sub_predictor.fit(x_data, y_data)\n",
    "            reg_objs[qtile] = sub_predictor\n",
    "\n",
    "        self.sub_predictor = {\n",
    "            \"predictors\": reg_objs,\n",
    "            \"targets\": self.cd_axis_targets,\n",
    "            \"features\": x_data.columns.tolist(),\n",
    "        }\n",
    "\n",
    "    def _predict(self, test_data: pd.Series) -> Dict[float, List[npt.NDArray]]:\n",
    "\n",
    "        # making sure dtypes are the same as in training, and filtering out the target columns from the dtypes dict since in test data its not there.\n",
    "        needed_dtypes = {\n",
    "            col: col_dtype\n",
    "            for col, col_dtype in self.target_feature_dtypes.items()\n",
    "            if \"_tgt\" not in col\n",
    "        }\n",
    "\n",
    "        test_data_cpy = (\n",
    "            test_data.to_frame().T.reset_index(drop=True).copy().astype(needed_dtypes)\n",
    "        )\n",
    "\n",
    "        data = {}\n",
    "\n",
    "        for qtile in self.quantile_levels:\n",
    "            pred = self.sub_predictor[\"predictors\"][qtile].predict(test_data_cpy)\n",
    "            data[qtile] = pred\n",
    "\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f861730f-8976-4404-9b78-d5c307b1365d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mlflow import MlflowClient\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cloudpickle\n",
    "from sys import version_info\n",
    "import pickle\n",
    "import os\n",
    "import shutil\n",
    "from typing import Optional, Tuple, Union, List, Dict, Any, Callable\n",
    "import numpy.typing as npt\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n",
    "\n",
    "__all__ = [\"ModelWrapper\", \"ModelWrapperMlflowModel\", \"ModelWrapperFarField\"]\n",
    "\n",
    "PYTHON_VERSION = \"{major}.{minor}.{micro}\".format(\n",
    "    major=version_info.major, minor=version_info.minor, micro=version_info.micro\n",
    ")\n",
    "\n",
    "conda_env = {\n",
    "    \"channels\": [\"defaults\"],\n",
    "    \"dependencies\": [\n",
    "        \"python={}\".format(PYTHON_VERSION),\n",
    "        \"pip\",\n",
    "        {\n",
    "            \"pip\": [\n",
    "                \"mlflow\",\n",
    "                \"lightgbm\",\n",
    "                \"cloudpickle=={}\".format(cloudpickle.__version__),\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "    \"name\": \"model_wrapper_env\",\n",
    "}\n",
    "\n",
    "\n",
    "class ModelWrapper(BaseModel):\n",
    "    \"\"\"Custom class which wraps a model type to generate\n",
    "    predictions in a timeseries format.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cd_axis_max_lags: int,\n",
    "        static_cols: List[str],\n",
    "        model_strategy: BaseStrategy,\n",
    "        is_auto_reg: bool = False,\n",
    "        is_ca3_training: bool = True,\n",
    "        prediction_horizon: int = 28,\n",
    "        lag_numbers: List[int] = [1, 7, 14, 28],\n",
    "        quantiles: List[float] = [0.5],\n",
    "        mlflow_run_id: Optional[str] = None,\n",
    "        hotel_id: Optional[str] = None,\n",
    "        version: Optional[Union[str, int]] = None,\n",
    "        stage: Optional[str] = None,\n",
    "        target_type: str = \"REVENUE\",\n",
    "        exclude_pms: bool = False,\n",
    "        save_models: bool = True,\n",
    "        local_root_dir: Optional[str] = None,\n",
    "        model_type: str = \"MODELWRAPPER\",\n",
    "        model_name_prefix: Optional[str] = None,\n",
    "        meta_data: Dict[str, Any] = {},\n",
    "        n_cd_lags: Optional[int] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            model_type=model_strategy.model_type,\n",
    "            prediction_horizon=prediction_horizon,\n",
    "            lag_numbers=lag_numbers,\n",
    "            quantiles=quantiles,\n",
    "            mlflow_run_id=mlflow_run_id,\n",
    "            hotel_id=hotel_id,\n",
    "            version=version,\n",
    "            stage=stage,\n",
    "            target_type=target_type,\n",
    "            exclude_pms=exclude_pms,\n",
    "            save_models=save_models,\n",
    "            local_root_dir=local_root_dir,\n",
    "            model_name_prefix=model_name_prefix,\n",
    "            meta_data=meta_data,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.quantile_levels.sort()\n",
    "        self.cd_axis_max_lags = cd_axis_max_lags\n",
    "        self.sd_axis_lag_prefix = \"lag\"\n",
    "        self.static_cols = static_cols\n",
    "        self.n_cd_lags = n_cd_lags\n",
    "        self.target_suffix = \"_tgt\"\n",
    "        self.is_auto_reg = is_auto_reg\n",
    "\n",
    "        self.model_strategy = model_strategy\n",
    "        self.model_type = self.model_strategy.model_type\n",
    "        self.all_cd_cols = [\n",
    "            f\"{self.target_prefix}{i}\" for i in range(self.cd_axis_max_lags + 1)\n",
    "        ]\n",
    "        self.is_ca3_training = is_ca3_training\n",
    "\n",
    "        # initializing targets variables\n",
    "        self.target_cols: Dict[int, List[str]] = {}\n",
    "\n",
    "        # initializing feature variables\n",
    "        self.feature_cols: Dict[int, List[str]] = {}\n",
    "\n",
    "        self.envs = [\"dev\", \"qa\", \"prod\"]\n",
    "\n",
    "        if 0.5 not in self.quantile_levels:\n",
    "            raise ValueError(\n",
    "                \"median quantile (0.5) is not included in the quantile_levels. please ensure that its included\"\n",
    "            )\n",
    "\n",
    "    def save_model(self) -> None:\n",
    "        \"\"\"Saves the models in the local directory, which will then be logged as artifacts in MLflow\"\"\"\n",
    "        if os.path.exists(self.local_root):\n",
    "            self.clean()\n",
    "\n",
    "        os.makedirs(self.local_dir)\n",
    "        with open(self.local_path, \"wb\") as pkl_file:\n",
    "            pickle.dump(obj=self, file=pkl_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def change_current_env_tags(self, incoming_tags: Dict[str, str]):\n",
    "\n",
    "        env_model_tag_keys = set([f\"model_stage_{env}\" for env in self.envs])\n",
    "        incoming_tags_keys = set(incoming_tags.keys())\n",
    "\n",
    "        tags_detected = env_model_tag_keys.intersection(incoming_tags_keys)\n",
    "\n",
    "        if len(tags_detected) > 0:\n",
    "            client = MlflowClient()\n",
    "            all_registered_models_info = client.search_model_versions(\n",
    "                f\"name ='{self.get_model_name()}'\"\n",
    "            )\n",
    "            # sorting the model meta data list by version number of the considered model name in descending order\n",
    "            sorted_model_versions = sorted(\n",
    "                all_registered_models_info, key=lambda x: int(x.version), reverse=True\n",
    "            )\n",
    "\n",
    "            for version_meta in sorted_model_versions:\n",
    "                for env_tag in tags_detected:\n",
    "\n",
    "                    if (incoming_tags[env_tag] == \"yes\") and (\n",
    "                        version_meta.tags.get(env_tag) == \"yes\"\n",
    "                    ):\n",
    "                        client.set_model_version_tag(\n",
    "                            name=self.get_model_name(),\n",
    "                            version=str(version_meta.version),\n",
    "                            key=env_tag,\n",
    "                            value=\"no\",\n",
    "                        )\n",
    "\n",
    "    def log_models(self) -> None:\n",
    "        \"\"\"Carries out the mlflow model registry procedures\"\"\"\n",
    "        print(\"Starting model logging\")\n",
    "        self.save_model()\n",
    "\n",
    "        modelpath = self.get_model_log_path()\n",
    "        print(\"Logging model\")\n",
    "        mlflow.pyfunc.log_model(\n",
    "            artifact_path=self.get_model_log_path(),\n",
    "            python_model=ModelWrapperMlflowModel(),\n",
    "            artifacts=self.artifacts,\n",
    "            conda_env=conda_env,\n",
    "        )\n",
    "\n",
    "        # enforcing lower case for env based string keys and values\n",
    "        decap_meta_data = {}\n",
    "        for key, value in self.meta_data.items():\n",
    "            env_str_match = re.findall(pattern=f\"({'|'.join(self.envs)})\", string=key)\n",
    "            if len(env_str_match) > 0:\n",
    "                decap_meta_data[key.lower()] = (\n",
    "                    value.lower() if isinstance(value, str) else value\n",
    "                )\n",
    "            else:\n",
    "                decap_meta_data[key] = value\n",
    "\n",
    "        self.meta_data = decap_meta_data\n",
    "\n",
    "        self.change_current_env_tags(self.meta_data)\n",
    "\n",
    "        print(\"Registering model\")\n",
    "        result = mlflow.register_model(\n",
    "            self.get_model_register_path(),\n",
    "            self.get_model_name(),\n",
    "            tags=self.meta_data,\n",
    "        )\n",
    "\n",
    "    def clean(self) -> None:\n",
    "        if os.path.exists(self.local_root):\n",
    "            shutil.rmtree(self.local_root)\n",
    "\n",
    "    def load_pyfunc_model(\n",
    "        self, dst_path: Optional[str] = None, tag: Optional[Union[str, int]] = None\n",
    "    ) -> mlflow.pyfunc.PyFuncModel:\n",
    "        \"\"\"Load and return the pyfunc model from the MLFlow model repository\n",
    "\n",
    "        Args:\n",
    "            dst_path (str, optional): Destination path to save the loaded model.\n",
    "                                      If not provided the files will be saved in the local_root path.\n",
    "                                      Defaults to None.\n",
    "            tag (str, optional): Tag to specify the version or model stage to be loaded.\n",
    "                                 If not provided the latest model version will be loaded.\n",
    "                                Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            mlflow.pyfunc.model : pyfunc model\n",
    "        \"\"\"\n",
    "        # self.local_dir = dst_path\n",
    "        print(f\"Loading model {self.get_model_uri()}\")\n",
    "\n",
    "        if dst_path is not None:\n",
    "            self.local_root = dst_path\n",
    "\n",
    "        if os.path.exists(self.local_root):\n",
    "            self.clean()\n",
    "\n",
    "        os.mkdir(self.local_root)\n",
    "\n",
    "        model = mlflow.pyfunc.load_model(\n",
    "            self.get_model_uri(tag=tag), dst_path=self.local_root\n",
    "        )\n",
    "\n",
    "        self.run_id = model._model_meta.run_id\n",
    "\n",
    "        # following is a bit of a round about way to set local_dir\n",
    "        # having the run id in the directory name is a bit troublesome as the run id is not available to us when we create the autogluon object\n",
    "        # TODO make sure to remove the run id from the local_dir and include either or both task_type/exclude_pms\n",
    "        # TODO make sure to set the local_dir consistently for both training and inference tasks\n",
    "        # self.local_dir = \"/ag_models/\"\n",
    "\n",
    "        # if self.exclude_pms:\n",
    "        #     self.local_dir = f\"ag_models_{self.hotel_id}_{self.run_id}/\"\n",
    "\n",
    "        # os.rename(\"artifacts\",self.local_dir)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def get_filtered_data(\n",
    "        self, data: pd.DataFrame, day_ahead: int\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "        target_columns = self.all_cd_cols[:day_ahead]\n",
    "        target_columns = list(\n",
    "            map(lambda target: target + self.target_suffix, target_columns)\n",
    "        )\n",
    "\n",
    "        # original target order\n",
    "        target_columns_orig = target_columns.copy()\n",
    "        target_columns.reverse()\n",
    "\n",
    "        cd_axis_lag_columns = self.all_cd_cols[day_ahead:]\n",
    "        if self.n_cd_lags != None:\n",
    "            cd_axis_lag_columns = cd_axis_lag_columns[: self.n_cd_lags]\n",
    "\n",
    "        sd_axis_lag_columns = [\n",
    "            f\"{self.sd_axis_lag_prefix}{SD_lag}\"\n",
    "            for SD_lag in self.lag_numbers\n",
    "            if SD_lag > day_ahead\n",
    "        ]\n",
    "\n",
    "        # assigning target and feature variables corresponding to the particular day ahead. This will be retrieved through the attributes in the inference phase.\n",
    "        self.target_cols[day_ahead] = target_columns\n",
    "        self.feature_cols[day_ahead] = (\n",
    "            cd_axis_lag_columns + sd_axis_lag_columns + self.static_cols\n",
    "        )\n",
    "\n",
    "        if self.is_ca3_training:\n",
    "            # Condition helps us get the specific entry for the cancellation day index\n",
    "            condition = data[\"forecast_index\"] == (day_ahead - 1)\n",
    "            filt_data = data[condition].copy()\n",
    "        else:\n",
    "            filt_data = data.copy()\n",
    "\n",
    "        x_data = filt_data[self.feature_cols[day_ahead]]\n",
    "        y_data = filt_data[self.target_cols[day_ahead]]\n",
    "\n",
    "        return (\n",
    "            x_data,\n",
    "            y_data,\n",
    "            filt_data[target_columns_orig + self.feature_cols[day_ahead]],\n",
    "        )\n",
    "\n",
    "    def train_inner(self, train_data: pd.DataFrame, day_ahead: int):\n",
    "        x_train, y_train, xy_train = self.get_filtered_data(\n",
    "            data=train_data, day_ahead=day_ahead\n",
    "        )\n",
    "\n",
    "        reg_obj = self.model_strategy(\n",
    "            quantile_levels=self.quantile_levels,\n",
    "            day_ahead=day_ahead,\n",
    "            cd_axis_targets=self.target_cols[day_ahead],\n",
    "            path=self.local_dir,\n",
    "            is_auto_reg=self.is_auto_reg,\n",
    "        )  # type: ignore\n",
    "        reg_obj._fit(xy_train)\n",
    "\n",
    "        return day_ahead, reg_obj\n",
    "\n",
    "    def train(self, train_data: pd.DataFrame, n_threads: int) -> None:\n",
    "        \"\"\"\n",
    "        trains models for each day ahead quantile predictions and  relevant\n",
    "        to the specified prediction_horizon value and the specific quantile\n",
    "        levels.\n",
    "\n",
    "        parameters:\n",
    "            train_data = training data with the booking pace lags, stay date lags or\n",
    "                    other features such as date features.\n",
    "\n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        with ThreadPoolExecutor(max_workers=n_threads) as executor:\n",
    "            future_to_target = {\n",
    "                executor.submit(self.train_inner, train_data, day_ahead): day_ahead\n",
    "                for day_ahead in range(1, self.prediction_horizon + 1)\n",
    "            }\n",
    "\n",
    "            for future in as_completed(future_to_target):\n",
    "                try:\n",
    "                    day_ahead, reg_obj = future.result()\n",
    "                except Exception as exc:\n",
    "                    print(exc)\n",
    "                else:\n",
    "                    self.models[day_ahead] = reg_obj\n",
    "\n",
    "        if self.do_save_models:\n",
    "            self.log_models()\n",
    "\n",
    "    def predict(\n",
    "        self, test_data: pd.DataFrame\n",
    "    ) -> Dict[Union[str, float], Union[List[npt.NDArray], List[pd.Series]]]:\n",
    "        \"\"\"generating quantile predictions for the test data provided. test_data\n",
    "        should be provided which aligns with the prediction_horizon. If\n",
    "        test_data has less rows than the prediction_horizon, then the length\n",
    "        of the test_data will be considered as the prediction horizon.\n",
    "\n",
    "        eg: if prediction_horizon= 28, ideally test_data should have 28 rows\n",
    "            which are relevant for 28 stay dates.\n",
    "\n",
    "        parameters:\n",
    "            test_data = test data which aligns with the prediction horizon.\n",
    "                        rows of test_data <= prediction_horizon.\n",
    "\n",
    "        Returns: Lists with actual values and corresponding predicted values along the booking axis leading upto the\n",
    "          relevant stay date ahead\n",
    "        \"\"\"\n",
    "        output_pred: Dict[\n",
    "            Union[str, float], Union[List[npt.NDArray], List[pd.Series]]\n",
    "        ] = {}\n",
    "\n",
    "        for day_ahead in range(1, self.prediction_horizon + 1):\n",
    "            test_idx = day_ahead - 1\n",
    "\n",
    "            try:\n",
    "                needed_test_data = test_data[test_data.day_ahead == day_ahead].iloc[0]\n",
    "            except IndexError as e:\n",
    "                days_str = \"days\" if day_ahead > 1 else \"day\"\n",
    "\n",
    "                print(f\"Error when predicting {day_ahead} {days_str} ahead\")\n",
    "                print(f\"Encountered error {e}\")\n",
    "                print(\"Skipping this row\")\n",
    "                continue\n",
    "\n",
    "            x_test = needed_test_data[self.feature_cols[day_ahead]]\n",
    "            y_test = needed_test_data[self.target_cols[day_ahead]]\n",
    "\n",
    "            predictor = self.models[day_ahead]\n",
    "            y_pred_dct = predictor._predict(x_test)\n",
    "\n",
    "            if output_pred.get(\"y_test\") == None:\n",
    "                output_pred[\"y_test\"] = [y_test]\n",
    "            else:\n",
    "                output_pred[\"y_test\"] += [y_test]\n",
    "\n",
    "            for qtile in self.quantile_levels:\n",
    "                if output_pred.get(qtile) == None:\n",
    "                    output_pred[qtile] = [y_pred_dct[qtile][0]]\n",
    "                else:\n",
    "                    output_pred[qtile] += [y_pred_dct[qtile][0]]\n",
    "\n",
    "        return output_pred\n",
    "\n",
    "\n",
    "class ModelWrapperMlflowModel(mlflow.pyfunc.PythonModel):\n",
    "    \"\"\"Custom Pyfunc model since the main model is not of native format and hence,\n",
    "    doesn't belong to predefined MLflow flavors\n",
    "    \"\"\"\n",
    "\n",
    "    def load_context(self, context):\n",
    "        with open(f\"{context.artifacts['model_dir']}/model.pkl\", \"rb\") as pkl_file:\n",
    "            self.model_wrapper_model = pickle.load(pkl_file)\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        return self.model_wrapper_model.predict(model_input)\n",
    "\n",
    "\n",
    "class ModelWrapperFarField(BaseModel):\n",
    "    \"\"\"Custom class which wraps a model type to generate\n",
    "    predictions in a timeseries format.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cd_axis_max_lags: int,\n",
    "        static_cols: List[str],\n",
    "        model_strategy: BaseStrategy,\n",
    "        is_auto_reg: bool = False,\n",
    "        is_ca3_training: bool = True,\n",
    "        prediction_horizon: int = 7,\n",
    "        lag_numbers: List[int] = [],\n",
    "        quantiles: List[float] = [0.5],\n",
    "        mlflow_run_id: Optional[str] = None,\n",
    "        hotel_id: Optional[str] = None,\n",
    "        version: Optional[Union[str, int]] = None,\n",
    "        stage: Optional[str] = None,\n",
    "        target_type: str = \"REVENUE\",\n",
    "        exclude_pms: bool = False,\n",
    "        save_models: bool = True,\n",
    "        local_root_dir: Optional[str] = None,\n",
    "        model_type: str = \"MODELWRAPPER_FARFIELD\",\n",
    "        model_name_prefix: Optional[str] = None,\n",
    "        meta_data: Dict[str, Any] = {},\n",
    "        n_cd_lags: Optional[int] = None,\n",
    "        forecast_points=[91, 84, 77, 70, 63, 56, 49, 42, 35],\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            model_type=model_strategy.model_type,\n",
    "            prediction_horizon=prediction_horizon,\n",
    "            lag_numbers=lag_numbers,\n",
    "            quantiles=quantiles,\n",
    "            mlflow_run_id=mlflow_run_id,\n",
    "            hotel_id=hotel_id,\n",
    "            version=version,\n",
    "            stage=stage,\n",
    "            target_type=target_type,\n",
    "            exclude_pms=exclude_pms,\n",
    "            save_models=save_models,\n",
    "            local_root_dir=local_root_dir,\n",
    "            model_name_prefix=model_name_prefix,\n",
    "            meta_data=meta_data,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.quantile_levels.sort()\n",
    "        self.cd_axis_max_lags = cd_axis_max_lags\n",
    "        self.sd_axis_lag_prefix = \"lag\"\n",
    "        self.static_cols = static_cols\n",
    "        self.n_cd_lags = n_cd_lags\n",
    "        self.target_suffix = \"_tgt\"\n",
    "        self.is_auto_reg = is_auto_reg\n",
    "        self.forecast_points = forecast_points\n",
    "        self.model_strategy = model_strategy\n",
    "        self.model_type = self.model_strategy.model_type\n",
    "        self.all_cd_cols = [\n",
    "            f\"{self.target_prefix}{i}\" for i in range(self.cd_axis_max_lags + 1)\n",
    "        ]\n",
    "        self.lag_numbers = [\n",
    "            x for x in range(0, self.cd_axis_max_lags + 1, self.prediction_horizon)\n",
    "        ]\n",
    "        self.is_ca3_training = is_ca3_training\n",
    "\n",
    "        # initializing targets variables\n",
    "        self.target_cols: Dict[int, str] = {}\n",
    "\n",
    "        # initializing feature variables\n",
    "        self.feature_cols: Dict[int, List[str]] = {}\n",
    "\n",
    "        if 0.5 not in self.quantile_levels:\n",
    "            raise ValueError(\n",
    "                \"median quantile (0.5) is not included in the quantile_levels. please ensure that its included\"\n",
    "            )\n",
    "\n",
    "    def save_model(self) -> None:\n",
    "        \"\"\"Saves the models in the local directory, which will then be logged as artifacts in MLflow\"\"\"\n",
    "        os.makedirs(self.local_dir)\n",
    "        with open(self.local_path, \"wb\") as pkl_file:\n",
    "            pickle.dump(obj=self, file=pkl_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def log_models(self) -> None:\n",
    "        \"\"\"Carries out the mlflow model registry procedures\"\"\"\n",
    "        print(\"Starting model logging\")\n",
    "        self.save_model()\n",
    "\n",
    "        modelpath = self.get_model_log_path()\n",
    "        print(\"Logging model\")\n",
    "        mlflow.pyfunc.log_model(\n",
    "            artifact_path=self.get_model_log_path(),\n",
    "            python_model=ModelWrapperMlflowModel(),\n",
    "            artifacts=self.artifacts,\n",
    "            conda_env=conda_env,\n",
    "        )\n",
    "\n",
    "        print(\"Registering model\")\n",
    "        result = mlflow.register_model(\n",
    "            self.get_model_register_path(),\n",
    "            self.get_model_name(),\n",
    "            tags=self.meta_data,\n",
    "        )\n",
    "\n",
    "    def clean(self) -> None:\n",
    "        if os.path.exists(self.local_root):\n",
    "            shutil.rmtree(self.local_root)\n",
    "\n",
    "    def load_pyfunc_model(\n",
    "        self, dst_path: Optional[str] = None, tag: Optional[Union[str, int]] = None\n",
    "    ) -> mlflow.pyfunc.PyFuncModel:\n",
    "        \"\"\"Load and return the pyfunc model from the MLFlow model repository\n",
    "\n",
    "        Args:\n",
    "            dst_path (str, optional): Destination path to save the loaded model.\n",
    "                                      If not provided the files will be saved in the local_root path.\n",
    "                                      Defaults to None.\n",
    "            tag (str, optional): Tag to specify the version or model stage to be loaded.\n",
    "                                 If not provided the latest model version will be loaded.\n",
    "                                Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            mlflow.pyfunc.model : pyfunc model\n",
    "        \"\"\"\n",
    "        # self.local_dir = dst_path\n",
    "        print(f\"Loading model {self.get_model_uri()}\")\n",
    "\n",
    "        if dst_path is not None:\n",
    "            self.local_root = dst_path\n",
    "\n",
    "        if os.path.exists(self.local_root):\n",
    "            self.clean()\n",
    "\n",
    "        os.mkdir(self.local_root)\n",
    "\n",
    "        model = mlflow.pyfunc.load_model(\n",
    "            self.get_model_uri(tag=tag), dst_path=self.local_root\n",
    "        )\n",
    "\n",
    "        self.run_id = model._model_meta.run_id\n",
    "\n",
    "        # following is a bit of a round about way to set local_dir\n",
    "        # having the run id in the directory name is a bit troublesome as the run id is not available to us when we create the autogluon object\n",
    "        # TODO make sure to remove the run id from the local_dir and include either or both task_type/exclude_pms\n",
    "        # TODO make sure to set the local_dir consistently for both training and inference tasks\n",
    "        # self.local_dir = \"/ag_models/\"\n",
    "\n",
    "        # if self.exclude_pms:\n",
    "        #     self.local_dir = f\"ag_models_{self.hotel_id}_{self.run_id}/\"\n",
    "\n",
    "        # os.rename(\"artifacts\",self.local_dir)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def get_filtered_data(\n",
    "        self, data: pd.DataFrame, forecast_point: int\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "\n",
    "        if self.is_ca3_training:\n",
    "            # Condition helps us get the specific entry for the cancellation day index\n",
    "            target_column = (\n",
    "                f\"{self.target_prefix}{forecast_point-self.prediction_horizon}_tgt\"\n",
    "            )\n",
    "            condition = data[\"forecast_index\"] == forecast_point\n",
    "            filt_data = data[condition].copy()\n",
    "        else:\n",
    "            target_column = (\n",
    "                f\"{self.target_prefix}{forecast_point-self.prediction_horizon}\"\n",
    "            )\n",
    "            filt_data = data.copy()\n",
    "\n",
    "        cd_axis_lag_columns = [\n",
    "            f\"{self.target_prefix}{x}\"\n",
    "            for x in range(forecast_point, self.cd_axis_max_lags + 1)\n",
    "        ]\n",
    "\n",
    "        sd_axis_lag_columns = [\n",
    "            f\"{self.sd_axis_lag_prefix}{SD_lag}\"\n",
    "            for SD_lag in self.lag_numbers\n",
    "            if SD_lag > forecast_point\n",
    "        ]\n",
    "\n",
    "        # assigning target and feature variables corresponding to the particular day ahead. This will be retrieved through the attributes in the inference phase.\n",
    "        self.target_cols[forecast_point] = target_column\n",
    "        self.feature_cols[forecast_point] = (\n",
    "            cd_axis_lag_columns + sd_axis_lag_columns + self.static_cols\n",
    "        )\n",
    "\n",
    "        x_data = filt_data[self.feature_cols[forecast_point]]\n",
    "        y_data = filt_data[self.target_cols[forecast_point]]\n",
    "\n",
    "        return (\n",
    "            x_data,\n",
    "            y_data,\n",
    "            filt_data[[target_column] + self.feature_cols[forecast_point]],\n",
    "        )\n",
    "\n",
    "    def train(self, train_data: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        trains models for each day ahead quantile predictions and  relevant\n",
    "        to the specified prediction_horizon value and the specific quantile\n",
    "        levels.\n",
    "\n",
    "        parameters:\n",
    "            train_data = training data with the booking pace lags, stay date lags or\n",
    "                    other features such as date features.\n",
    "\n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        for forecast_point in self.forecast_points:\n",
    "            print(\"\\tForecast point: \", forecast_point)\n",
    "            x_train, y_train, xy_train = self.get_filtered_data(\n",
    "                data=train_data, forecast_point=forecast_point\n",
    "            )\n",
    "            reg_obj = self.model_strategy(\n",
    "                quantile_levels=self.quantile_levels,\n",
    "                day_ahead=self.prediction_horizon,\n",
    "                cd_axis_targets=self.target_cols[forecast_point],\n",
    "                path=self.local_dir,\n",
    "                is_auto_reg=self.is_auto_reg,\n",
    "            )  # type: ignore\n",
    "            reg_obj._fit(xy_train)\n",
    "\n",
    "            self.models[forecast_point] = reg_obj\n",
    "\n",
    "        if self.do_save_models:\n",
    "            self.log_models()\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        test_data: pd.DataFrame,\n",
    "    ) -> Tuple[\n",
    "        List[npt.NDArray], List[pd.Series], List[npt.NDArray], List[npt.NDArray]\n",
    "    ]:\n",
    "        \"\"\"generating quantile predictions for the test data provided. test_data\n",
    "        should be provided which aligns with the prediction_horizon. If\n",
    "        test_data has less rows than the prediction_horizon, then the length\n",
    "        of the test_data will be considered as the prediction horizon.\n",
    "\n",
    "        eg: if prediction_horizon= 28, ideally test_data should have 28 rows\n",
    "            which are relevant for 28 stay dates.\n",
    "\n",
    "        parameters:\n",
    "            test_data = test data which aligns with the prediction horizon.\n",
    "                        rows of test_data <= prediction_horizon.\n",
    "\n",
    "        Returns: Lists with actual values and corresponding predicted values along the booking axis leading upto the\n",
    "          relevant stay date ahead\n",
    "        \"\"\"\n",
    "        y_test_lst = []\n",
    "        y_pred_lst = []\n",
    "        y_upper_lst = []\n",
    "        y_lower_lst = []\n",
    "        forecast_point = test_data[\"forecast_point\"].iloc[0]\n",
    "\n",
    "        # for forecast_point in self.forecast_points:\n",
    "        target_column = f\"{self.target_prefix}{forecast_point-self.prediction_horizon}\"\n",
    "        predictor = self.models[forecast_point]\n",
    "\n",
    "        x_test = test_data[predictor.sub_predictor[\"features\"]]\n",
    "        y_test_lst.append(test_data[target_column].iloc[0])\n",
    "\n",
    "        y_pred_dct = predictor._predict(x_test.squeeze())\n",
    "\n",
    "        if 0.1 in self.quantile_levels:\n",
    "            y_lower_lst.append(y_pred_dct[0.1][0])\n",
    "        else:\n",
    "            y_lower_lst.append(y_pred_dct[0.5][0])\n",
    "\n",
    "        if 0.5 in self.quantile_levels:\n",
    "            y_pred_lst.append(y_pred_dct[0.5][0])\n",
    "\n",
    "        if 0.9 in self.quantile_levels:\n",
    "            y_upper_lst.append(y_pred_dct[0.9][0])\n",
    "        else:\n",
    "            y_upper_lst.append(y_pred_dct[0.5][0])\n",
    "\n",
    "        return y_pred_lst, y_test_lst, y_upper_lst, y_lower_lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07acac0a-4000-4232-8185-65294d99d27c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Disable adaptive query optimization\n",
    "# Adaptive query optimization groups together smaller tasks into a larger tasks.\n",
    "# This may result in limited parallelism if the parallel inference tasks are deemed to be too small by the query optimizer\n",
    "# We are diableing AQE here to circumevent this limitation on parallelism\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5be9323b-708d-47c7-9a79-0a625f6af830",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "REVENUE_COL = \"_reservationRevenuePerRoomUSD\"\n",
    "ROOMS_COL = \"_rooms\"\n",
    "PIPELINE = \"INFERENCE\"\n",
    "\n",
    "WITHOUT_PMS = str_to_bool(getArgument(\"exclude_pms\"))\n",
    "IS_USD_CURRENCY = str_to_bool(getArgument(\"is_usd_currency\"))\n",
    "TARGET_TYPE = getArgument(\"target_type\")\n",
    "selected_hotels = str_to_lst(getArgument(\"selected_hotels\"))\n",
    "LAG_NUMBERS = list(map(int,str_to_lst(getArgument('lag_numbers'))))\n",
    "\n",
    "### The start of the model data\n",
    "MODEL_START_DATE = pd.to_datetime(\"2018-10-01\")\n",
    "COVID_START_DATE = pd.to_datetime(\"2020-03-01\")\n",
    "COVID_END_DATE = pd.to_datetime(\"2021-08-01\")\n",
    "\n",
    "CALC_UNCERTAINTY = False\n",
    "# MODEL_TYPE = \"XGB\"  # Use \"AG\" to try out the auto gloun approach\n",
    "MODEL_TYPE = \"AG\"\n",
    "\n",
    "LEAD_WINDOW = 60\n",
    "\n",
    "ML_EXPERIMENT_ID = 1079527465953184\n",
    "\n",
    "if MODEL_TYPE == \"XGB\":\n",
    "    RUN_ID = \"92907cac187f4c8cadb63ff60a05d72e\"  # XGB Run\n",
    "elif CALC_UNCERTAINTY and (MODEL_TYPE == \"AG\"):\n",
    "    RUN_ID = \"9549361574484dc58fcf1b7d130541a0\"\n",
    "else:\n",
    "    RUN_ID = \"19dee6420aed45f29e956016c5ea6e8a\"\n",
    "\n",
    "\n",
    "lead_window_start_days = 14\n",
    "lead_window_end_days = 60\n",
    "prediction_horizon = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d707ac6-578b-4078-be83-7f0ee85fc8fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env_config = EnvironmentConfig(env=ENV, target=TARGET_TYPE, spark=spark, is_usd_currency=IS_USD_CURRENCY)\n",
    "forecasting_config_provider = ForecastingHotelConfigProvider(spark=spark,env=ENV)\n",
    "target_column = env_config.target_column\n",
    "schema = inference_output_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ee033c4-dca5-4987-845a-44648f1852f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">2024-10-10\n",
       "2024-11-07\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">2024-10-10\n2024-11-07\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# As a workaround for the bug PHG-2157\n",
    "PARTITION_DATE = spark.sql(\n",
    "    f\"select max(confirmationDate) from {env_config.source_data_table}\"\n",
    ").collect()[0][0]\n",
    "print(PARTITION_DATE)\n",
    "\n",
    "max_inference_length = spark.sql(f'select max(inference_prediction_length) from {forecasting_config_provider.config_table_name}').collect()[0][0]\n",
    "TEST_PARTIITON_END = PARTITION_DATE + pd.Timedelta(max_inference_length, \"D\")\n",
    "print(TEST_PARTIITON_END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "393437d7-ed06-41ba-bc65-d37283901994",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PARTITION_DATE = pd.to_datetime(\"2024-10-07\")\n",
    "# TEST_PARTIITON_END = pd.to_datetime(\"2024-11-04\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6746c673-c1d9-4d51-b5fd-0c6cc7c2d479",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger = get_dbx_logger(pipeline=PIPELINE,\n",
    "                        task_type=TARGET_TYPE,\n",
    "                        exclude_pms=WITHOUT_PMS)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26b666df-4ba3-41fd-b2ae-b1f11e4f7913",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def pyfunc_load_model_retry(model_uri, max_tries):\n",
    "    '''Retry mechanism for loading models from mlflow model registry to \n",
    "    handle the model loading error\n",
    "    '''\n",
    "    loop_len = max_tries+1\n",
    "    for i in range(loop_len):\n",
    "            try:\n",
    "                return mlflow.pyfunc.load_model(model_uri)\n",
    "            except Exception as e:\n",
    "                if i+1==loop_len:\n",
    "                    raise e\n",
    "                else:\n",
    "                    print(e)\n",
    "                    print(f'Retrying: attempt {i+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a93c7e83-8fe7-4388-9d76-613d901d3c8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def correct_prediction_list(y_med, y_test, y_upper, y_lower, target, available_rooms):\n",
    "    y_med_lst = []\n",
    "    y_upper_lst = []\n",
    "    y_lower_lst = []\n",
    "\n",
    "    for i, vals in enumerate(zip(y_med, y_test, y_upper, y_lower)):\n",
    "        (\n",
    "            y_med_corrected,\n",
    "            y_test_corrected,\n",
    "            y_upper_corrected,\n",
    "            y_lower_corrected,\n",
    "        ) = post_process_prediction(\n",
    "            y_med=vals[0],\n",
    "            y_test=vals[1],\n",
    "            y_upper=vals[2],\n",
    "            y_lower=vals[3],\n",
    "            target=target,\n",
    "            available_rooms=available_rooms,\n",
    "        )\n",
    "\n",
    "        y_med_lst.append(y_med_corrected)\n",
    "        y_upper_lst.append(y_upper_corrected)\n",
    "        y_lower_lst.append(y_lower_corrected)\n",
    "\n",
    "    return y_med_lst, y_upper_lst, y_lower_lst\n",
    "\n",
    "\n",
    "def post_process_prediction(y_med, y_test, y_upper, y_lower, target, available_rooms):\n",
    "    # This step corrects the dipping issue\n",
    "    (\n",
    "        y_med_corrected,\n",
    "        y_test_corrected,\n",
    "        y_upper_corrected,\n",
    "        y_lower_corrected,\n",
    "    ) = correct_dipping(y_med=y_med, y_test=y_test, y_upper=y_upper, y_lower=y_lower)\n",
    "\n",
    "    if target == \"ROOMS\":\n",
    "        # If the target type is ROOMS we can try to correct the max rooms capping issue\n",
    "        if available_rooms is None:\n",
    "            raise ValueError(\"The argument available_rooms must be provided\")\n",
    "\n",
    "        (\n",
    "            y_med_corrected,\n",
    "            y_test_corrected,\n",
    "            y_upper_corrected,\n",
    "            y_lower_corrected,\n",
    "        ) = correct_capping(\n",
    "            y_med=y_med_corrected,\n",
    "            y_test=y_test_corrected,\n",
    "            y_upper=y_upper_corrected,\n",
    "            y_lower=y_lower_corrected,\n",
    "            available_rooms=available_rooms,\n",
    "        )\n",
    "\n",
    "    return y_med_corrected, y_test, y_upper_corrected, y_lower_corrected\n",
    "\n",
    "\n",
    "def correct_capping(y_med, y_test, y_upper, y_lower, available_rooms):\n",
    "    \"\"\"\n",
    "    Transforms forecast predictions to be below the available_rooms.\n",
    "\n",
    "    Args:\n",
    "        y_med : raw mean/median predictions\n",
    "        y_test: actual values\n",
    "        y_upper : raw upper quantile predictions\n",
    "        y_lower : raw lower quantile predictions\n",
    "        available_rooms: available number of rooms for a the specific hotel\n",
    "\n",
    "    Returns corrected predicted values.\n",
    "    \"\"\"\n",
    "\n",
    "    y_med_corrected = np.where(y_med > available_rooms, available_rooms, y_med)\n",
    "    y_upper_corrected = np.where(y_upper > available_rooms, available_rooms, y_upper)\n",
    "    y_lower_corrected = np.where(y_lower > available_rooms, available_rooms, y_lower)\n",
    "\n",
    "    return y_med_corrected, y_test, y_upper_corrected, y_lower_corrected\n",
    "\n",
    "\n",
    "def correct_dipping(y_med, y_test, y_upper, y_lower):\n",
    "    \"\"\"This returns the adjusted predicted values such that,\n",
    "    1) median/mean predictions have a strict cumulative nature\n",
    "    2) upper and lower quantile predictions are restricted to be on either\n",
    "    side of the median/mean predictions.\n",
    "\n",
    "    Args:\n",
    "        y_med : raw mean/median predictions\n",
    "        y_test: actual values\n",
    "        y_upper : raw upper quantile predictions\n",
    "        y_lower : raw lower quantile predictions\n",
    "\n",
    "    Returns corrected predicted values.\n",
    "    \"\"\"\n",
    "    last_val = y_med[0]\n",
    "\n",
    "    # correcting predictions if they are lower than the last known actual value\n",
    "    y_med_corrected = np.where(y_med < last_val, last_val, y_med)\n",
    "    y_lower_corrected = y_lower #np.where(y_lower < last_val, last_val, y_lower)\n",
    "\n",
    "    delta = y_med_corrected - np.abs(y_med)\n",
    "\n",
    "    y_upper_corrected = y_upper + delta\n",
    "\n",
    "    # further correction of predictions along the booking axis to have the cumulative nature\n",
    "    for index in range(len(y_med)):\n",
    "        # adjusting median/mean predictions\n",
    "        if (y_med_corrected[index] < y_med_corrected[index - 1]) and (index > 0):\n",
    "            y_med_corrected[index] = y_med_corrected[index - 1]\n",
    "\n",
    "        # adjusting upper quantile predictions\n",
    "        if y_med_corrected[index] > y_upper_corrected[index]:\n",
    "            y_upper_corrected[index] = y_med_corrected[index]\n",
    "\n",
    "        # adjusting lower quantile predictions\n",
    "        if y_med_corrected[index] < y_lower_corrected[index]:\n",
    "            y_lower_corrected[index] = y_med_corrected[index]\n",
    "\n",
    "    return y_med_corrected, y_test, y_upper_corrected, y_lower_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "392ac16e-e8cc-4f19-ad30-0d53c37c6a07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prediction_wrapper(\n",
    "    target_type, run_id, exclude_pms,hotel_config_provider,model_cache_dir,environment\n",
    "):\n",
    "    def predict_distributed(data):\n",
    "        static_cols_ = ['year', 'quarter_of_year', 'month_of_year', 'week_of_year',\n",
    "                         'day_of_year', 'month_of_quarter', 'week_of_quarter', 'day_of_quarter',\n",
    "                           'week_of_month', 'day_of_month', 'holiday',\n",
    "                             'day_of_week_0', 'day_of_week_1', 'day_of_week_2', \n",
    "                             'day_of_week_3', 'day_of_week_4', 'day_of_week_5', 'day_of_week_6']\n",
    "\n",
    "        logger = get_dbx_logger(\"PHGML\")\n",
    "        \n",
    "        max_lead_window = 100\n",
    "        \n",
    "        hotel_id = data[\"HotelID\"].iloc[0]\n",
    "        hotel_config = hotel_config_provider.get_config(hotel_id)\n",
    "        model_type = hotel_config.inference_model_name\n",
    "\n",
    "        print(f\"Processing Hotel {hotel_id}\")\n",
    "        \n",
    "        if target_type == \"REVENUE\":\n",
    "            col_prefix = \"RV\"\n",
    "\n",
    "            if hotel_config.forecast_currency is None:\n",
    "                # If the target type is REVENUE, we should have a defined forecast_currency\n",
    "                raise ValueError(f\"Forecast currency cannot be None for target_type {target_type}\")\n",
    "            \n",
    "        elif target_type == \"ROOMS\":\n",
    "            col_prefix = \"RM\"\n",
    "        \n",
    "        data = remove_padded_cols(data,hotel_config.lead_window,max_lead_window,col_prefix)\n",
    "        \n",
    "        model_version = 1\n",
    "        model_stage = \"Staging\"\n",
    "        model_name = None\n",
    "\n",
    "        try:\n",
    "\n",
    "            if model_type == \"LIGHTGBM\":\n",
    "\n",
    "                model_obj = ModelWrapper(\n",
    "                                model_strategy=StrategyLGBM,\n",
    "                                prediction_horizon=hotel_config.inference_length,\n",
    "                                hotel_id=hotel_id,\n",
    "                                target_type=target_type,\n",
    "                                exclude_pms=exclude_pms,\n",
    "                                cd_axis_max_lags=99, \n",
    "                                static_cols =static_cols_,)\n",
    "                \n",
    "                model_obj.set_latest_model_version(model_stage = environment)\n",
    "\n",
    "                loaded_model = pyfunc_load_model_retry(model_obj.get_model_uri(), 6)\n",
    "                \n",
    "                loaded_model.unwrap_python_model().model_wrapper_model.prediction_horizon = hotel_config.inference_length\n",
    "                #during training time, the target variables are suffixed as '_tgt' to differentiate between target booking pace values and feature booking pace values. but while doing daily inferences,\n",
    "                # that distinction doesnt matter since we dont have the true values anyway, hence overriding the the target columns as below to avoid columns being not detected.\n",
    "                loaded_model.unwrap_python_model().model_wrapper_model.target_cols = {day_ahead:[ f\"{col_prefix}{j}\" for j in range(day_ahead)] for day_ahead in range(1,hotel_config.inference_length+1)}\n",
    "                    \n",
    "            elif model_type == \"AUTOGLUON\":\n",
    "\n",
    "                model_obj = ModelWrapper(\n",
    "                                model_strategy=StrategyAG,\n",
    "                                is_auto_reg=True,\n",
    "                                prediction_horizon=hotel_config.inference_length,\n",
    "                                hotel_id=hotel_id,\n",
    "                                target_type=target_type,\n",
    "                                exclude_pms=exclude_pms,\n",
    "                                cd_axis_max_lags=99, \n",
    "                                static_cols =static_cols_,)\n",
    "\n",
    "\n",
    "                model_obj.set_latest_model_version()\n",
    "            \n",
    "                pms = \"PMS\"\n",
    "                if exclude_pms:\n",
    "                    pms = \"NOPMS\"\n",
    "\n",
    "                #dbfs_dir = f\"/dbfs/mnt/models/forecasting/individual_hotels/{hotel_id}_{target_type}_{pms}/\"\n",
    "                dbfs_dir = f\"{model_cache_dir}{hotel_id}_{target_type}_{pms}\" \n",
    "                #f\"/dbfs/mnt/models/forecasting/dev_individual_hotels/{hotel_id}_{target_type}_{pms}/\"\n",
    "                local_dir = model_obj.local_root\n",
    "\n",
    "                if os.path.exists(local_dir):\n",
    "                    shutil.rmtree(local_dir)\n",
    "\n",
    "                # Copy cached model from blob storage to local dir\n",
    "                \n",
    "                shutil.copytree(dbfs_dir, local_dir)\n",
    "\n",
    "                # load model\n",
    "                loaded_model = load_pkl.load(path=model_obj.local_path)\n",
    "                loaded_model.prediction_horizon = model_obj.prediction_horizon\n",
    "\n",
    "            model_version = int(model_obj.version)\n",
    "            model_name = [\n",
    "                model_obj.get_model_name()\n",
    "                for step in range(1, hotel_config.inference_length + 1)\n",
    "            ]\n",
    "            model_metadata = model_obj.get_remote_model_metadata()\n",
    "            logger.info(\"Using model version {model_version}\")\n",
    "\n",
    "            logger.info(f\"Inference length of model: {model_metadata.get('inference_length','NOT_FOUND')}\")\n",
    "            logger.info(f\"Last trained date: {model_metadata.get('last_trained_date','NOT_FOUND')}\")           \n",
    "\n",
    "            output_dct = loaded_model.predict(data)\n",
    "            y_pred_raw, y_test, y_upper_raw, y_lower_raw = output_dct[0.5], output_dct['y_test'], output_dct[0.9], output_dct[0.1]\n",
    "\n",
    "            y_pred_interpolated = [interpolated_fill(day_ahead_array) for day_ahead_array in y_pred_raw]\n",
    "            \n",
    "            y_pred, y_upper, y_lower = correct_prediction_list(\n",
    "                y_pred_interpolated, y_test, y_upper_raw, y_lower_raw,target_type,available_rooms = hotel_config.available_rooms\n",
    "            )\n",
    "\n",
    "            data[\"status\"] = \"complete\"\n",
    "            data[\"message\"] = f\"Successfully processed {hotel_id}\"\n",
    "\n",
    "            output_df = get_output_df(\n",
    "                y_pred=y_pred,\n",
    "                y_true=y_test,\n",
    "                run_id=run_id,\n",
    "                hotel_id=hotel_id,\n",
    "                data=data.sort_values('day_ahead'),\n",
    "                model_name=model_name,\n",
    "                model_version=model_version,\n",
    "                pms_sync_off=exclude_pms,\n",
    "                forecast_currency=hotel_config.forecast_currency,\n",
    "                prediction_horizon=hotel_config.inference_length,\n",
    "                y_upper=y_upper,\n",
    "                y_lower=y_lower,\n",
    "                y_med_raw=y_pred_raw,\n",
    "                y_upper_raw=y_upper_raw,\n",
    "                y_lower_raw=y_lower_raw,\n",
    "            )\n",
    "\n",
    "            output_df[\"status\"] = \"complete\"\n",
    "            output_df[\"message\"] = f\"Successfully processed {hotel_id}\"\n",
    "\n",
    "        except MlflowException as e:\n",
    "            if \"RESOURCE_DOES_NOT_EXIST\" in e.message:\n",
    "                print(\n",
    "                        f\"Model {model_obj.get_model_name()} was not  found in the model registry. Skipping this model...\"\n",
    "                    )\n",
    "            else:\n",
    "                print(\"An MLFlowException occured\")\n",
    "                print(e)\n",
    "\n",
    "            empty = pd.DataFrame(\n",
    "                {\n",
    "                    \"HotelID\": [hotel_id],\n",
    "                    \"run_id\": [run_id],\n",
    "                    \"stay_date\": [pd.Timestamp(\"1900-01-01\")],\n",
    "                    \"booking_date\": [pd.Timestamp(\"1900-01-01\")],\n",
    "                    \"model_version\": [0],\n",
    "                    \"timestamp\": [pd.Timestamp(\"1900-01-01\")],\n",
    "                    \"pms_sync_off\": [exclude_pms],\n",
    "                    \"forecast_currency\":[hotel_config.forecast_currency],\n",
    "                    \"day_index\": [0],\n",
    "                    \"y_med\": [0],\n",
    "                    \"model_name\": [\"\"],\n",
    "                    \"y_upper\": [0],\n",
    "                    \"y_lower\": [0],\n",
    "                    \"y_med_raw\": [0],\n",
    "                    \"y_upper_raw\": [0],\n",
    "                    \"y_lower_raw\": [0],\n",
    "                    \"status\": \"incomplete\",\n",
    "                    \"message\": e.message,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            return empty\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Hotel {hotel_id} encountered an error \")\n",
    "            raise e\n",
    "        finally:\n",
    "            if model_type == \"AUTOGLUON\":\n",
    "                model_obj.clean()\n",
    "\n",
    "        return output_df\n",
    "\n",
    "    return predict_distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd4e379e-9deb-44f9-aba6-2828ed3837ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env_config.preprocess_intermediate_table = \"test_preprocess_intermediate_table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7f11cb8-635f-42c2-beb0-cfa98e216874",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">24/10/11/ 10:01:12 UTC:PHGML-INFERENCE-REVENUE-PMS-INFO-Read preprocessing data\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">24/10/11/ 10:01:12 UTC:PHGML-INFERENCE-REVENUE-PMS-INFO-Read preprocessing data\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger.info(\"Read preprocessing data\")\n",
    "df = spark.sql(\n",
    "    f\"select * from {env_config.preprocess_intermediate_table}\"\n",
    ").withColumn(\"status\", lit(\"incomplete\"))\n",
    "\n",
    "# df = df.filter(df.HotelID=='63662')\n",
    "df = df.withColumn(\"_StayDates\", to_timestamp(\"_StayDates\", \"yyyy-MM-dd\")).orderBy([\"HotelID\", \"_StayDates\"])\n",
    "\n",
    "df = df.withColumn('partition_date', lit(str(PARTITION_DATE)))\n",
    "df = df.withColumn(\"day_ahead\", datediff(col(\"_StayDates\"), to_timestamp('partition_date', \"yyyy-MM-dd\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e6bc518-b638-4e62-b4de-241a909b1864",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DEBUG\n",
    "# output = debug_prediction(df,MODEL_TYPE, TARGET_TYPE, ML_EXPERIMENT_ID, RUN_ID, WITHOUT_PMS, CALC_UNCERTAINTY,forecasting_config_provider,model_cache_dir=env_config.model_cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5aade37-19d4-43c1-8053-d7e120c7bd65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">24/10/11/ 10:01:12 UTC:PHGML-INFERENCE-REVENUE-PMS-INFO-Starting parallell processing\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">24/10/11/ 10:01:12 UTC:PHGML-INFERENCE-REVENUE-PMS-INFO-Starting parallell processing\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Group the data by hotel id and execute the inferences in parallel\n",
    "logger.info(\"Starting parallell processing\")\n",
    "output_df = df.groupby(\"HotelID\").applyInPandas(\n",
    "    prediction_wrapper(\n",
    "        target_type=TARGET_TYPE, \n",
    "        run_id=RUN_ID, \n",
    "        exclude_pms=WITHOUT_PMS, \n",
    "        hotel_config_provider=forecasting_config_provider,\n",
    "        model_cache_dir=env_config.model_cache_dir,\n",
    "        environment=ENV\n",
    "    ),\n",
    "    schema,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed7d276f-7476-4ec5-b1b1-407f4f1ade8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# logger.info(\"Drop intermediate results table if it exists\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {env_config.inference_intermediate_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "104f1911-603b-45d3-95ca-785a0fa04e87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 64.0 failed 4 times, most recent failure: Lost task 0.3 in stage 64.0 (TID 1554) (10.139.64.18 executor 1): org.apache.spark.api.python.PythonException: 'AttributeError: 'Booster' object has no attribute 'handle'', from <command-537005303498582>, line 177. Full traceback below:\n",
       "Traceback (most recent call last):\n",
       "  File \"<command-537005303498582>\", line 177, in predict_distributed\n",
       "  File \"<command-537005303498582>\", line 177, in predict_distributed\n",
       "  File \"/databricks/python/lib/python3.8/site-packages/mlflow/pyfunc/__init__.py\", line 413, in predict\n",
       "    return self._predict_fn(data)\n",
       "  File \"/databricks/python/lib/python3.8/site-packages/mlflow/pyfunc/model.py\", line 305, in predict\n",
       "    return self.python_model.predict(self.context, model_input)\n",
       "  File \"/databricks/python/lib/python3.8/site-packages/phgml/models/model_wrapper.py\", line 380, in predict\n",
       "    return self.model_wrapper_model.predict(model_input)\n",
       "  File \"/databricks/python/lib/python3.8/site-packages/phgml/models/model_wrapper.py\", line 354, in predict\n",
       "    y_pred_dct = predictor._predict(x_test)\n",
       "  File \"/databricks/python/lib/python3.8/site-packages/phgml/models/model_strategy.py\", line 127, in _predict\n",
       "    pred = self.sub_predictors[target_][\"predictors\"][qtile].predict(\n",
       "  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/lib/python3.8/site-packages/lightgbm/sklearn.py\", line 803, in predict\n",
       "    return self._Booster.predict(X, raw_score=raw_score, start_iteration=start_iteration, num_iteration=num_iteration,\n",
       "  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/lib/python3.8/site-packages/lightgbm/basic.py\", line 3532, in predict\n",
       "    predictor = self._to_predictor(deepcopy(kwargs))\n",
       "  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/lib/python3.8/site-packages/lightgbm/basic.py\", line 3622, in _to_predictor\n",
       "    predictor = _InnerPredictor(booster_handle=self.handle, pred_parameter=pred_parameter)\n",
       "AttributeError: 'Booster' object has no attribute 'handle'\n",
       "\n",
       "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n",
       "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
       "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
       "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n",
       "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:80)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:155)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:161)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "\n",
       "Driver stacktrace:\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3185)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3120)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3107)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
       "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3107)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1448)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1448)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1448)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3393)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3334)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3322)\n",
       "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1214)\n",
       "\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2665)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:266)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:276)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:81)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:87)\n",
       "\tat org.apache.spark.sql.execution.collect.InternalRowFormat$.collect(cachedSparkResults.scala:75)\n",
       "\tat org.apache.spark.sql.execution.collect.InternalRowFormat$.collect(cachedSparkResults.scala:62)\n",
       "\tat org.apache.spark.sql.execution.ResultCacheManager.collectResult$1(ResultCacheManager.scala:573)\n",
       "\tat org.apache.spark.sql.execution.ResultCacheManager.computeResult(ResultCacheManager.scala:582)\n",
       "\tat org.apache.spark.sql.execution.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:528)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:527)\n",
       "\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:424)\n",
       "\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:403)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:424)\n",
       "\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3153)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$collectResult$1(Dataset.scala:3144)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3951)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:245)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:393)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:192)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:979)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:147)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:343)\n",
       "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3949)\n",
       "\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3143)\n",
       "\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation0(OutputAggregator.scala:266)\n",
       "\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation(OutputAggregator.scala:100)\n",
       "\tat com.databricks.backend.daemon.driver.PythonDriverLocalBase.generateTableResult(PythonDriverLocalBase.scala:729)\n",
       "\tat com.databricks.backend.daemon.driver.PythonDriverLocal.computeListResultsItem(PythonDriverLocal.scala:627)\n",
       "\tat com.databricks.backend.daemon.driver.PythonDriverLocalBase.genListResults(PythonDriverLocalBase.scala:636)\n",
       "\tat com.databricks.backend.daemon.driver.PythonDriverLocal.$anonfun$getResultBufferInternal$1(PythonDriverLocal.scala:682)\n",
       "\tat com.databricks.backend.daemon.driver.PythonDriverLocal.withInterpLock(PythonDriverLocal.scala:563)\n",
       "\tat com.databricks.backend.daemon.driver.PythonDriverLocal.getResultBufferInternal(PythonDriverLocal.scala:642)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.getResultBuffer(DriverLocal.scala:759)\n",
       "\tat com.databricks.backend.daemon.driver.PythonDriverLocal.outputSuccess(PythonDriverLocal.scala:605)\n",
       "\tat com.databricks.backend.daemon.driver.PythonDriverLocal.$anonfun$repl$6(PythonDriverLocal.scala:223)\n",
       "\tat com.databricks.backend.daemon.driver.PythonDriverLocal.withInterpLock(PythonDriverLocal.scala:563)\n",
       "\tat com.databricks.backend.daemon.driver.PythonDriverLocal.repl(PythonDriverLocal.scala:210)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$13(DriverLocal.scala:645)\n",
       "\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n",
       "\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:60)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:60)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:622)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:634)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:626)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$3(DriverWrapper.scala:665)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:425)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:445)\n",
       "\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n",
       "\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:56)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionTags(DriverWrapper.scala:56)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:420)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:339)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.recordOperationWithResultTags(DriverWrapper.scala:56)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:665)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:543)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:578)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$runInnerLoop$1(DriverWrapper.scala:446)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n",
       "\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:56)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:446)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:383)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:234)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: org.apache.spark.api.python.PythonException: 'AttributeError: 'Booster' object has no attribute 'handle'', from <command-537005303498582>, line 177. Full traceback below:\n",
       "Traceback (most recent call last):\n",
       "  File \"<command-537005303498582>\", line 177, in predict_distributed\n",
       "  File \"<command-537005303498582>\", line 177, in predict_distributed\n",
       "  File \"/databricks/python/lib/python3.8/site-packages/mlflow/pyfunc/__init__.py\", line 413, in predict\n",
       "    return self._predict_fn(data)\n",
       "  File \"/databricks/python/lib/python3.8/site-packages/mlflow/pyfunc/model.py\", line 305, in predict\n",
       "    return self.python_model.predict(self.context, model_input)\n",
       "  File \"/databricks/python/lib/python3.8/site-packages/phgml/models/model_wrapper.py\", line 380, in predict\n",
       "    return self.model_wrapper_model.predict(model_input)\n",
       "  File \"/databricks/python/lib/python3.8/site-packages/phgml/models/model_wrapper.py\", line 354, in predict\n",
       "    y_pred_dct = predictor._predict(x_test)\n",
       "  File \"/databricks/python/lib/python3.8/site-packages/phgml/models/model_strategy.py\", line 127, in _predict\n",
       "    pred = self.sub_predictors[target_][\"predictors\"][qtile].predict(\n",
       "  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/lib/python3.8/site-packages/lightgbm/sklearn.py\", line 803, in predict\n",
       "    return self._Booster.predict(X, raw_score=raw_score, start_iteration=start_iteration, num_iteration=num_iteration,\n",
       "  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/lib/python3.8/site-packages/lightgbm/basic.py\", line 3532, in predict\n",
       "    predictor = self._to_predictor(deepcopy(kwargs))\n",
       "  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/lib/python3.8/site-packages/lightgbm/basic.py\", line 3622, in _to_predictor\n",
       "    predictor = _InnerPredictor(booster_handle=self.handle, pred_parameter=pred_parameter)\n",
       "AttributeError: 'Booster' object has no attribute 'handle'\n",
       "\n",
       "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n",
       "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
       "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
       "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n",
       "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:80)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:155)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:161)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\t... 1 more"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 64.0 failed 4 times, most recent failure: Lost task 0.3 in stage 64.0 (TID 1554) (10.139.64.18 executor 1): org.apache.spark.api.python.PythonException: 'AttributeError: 'Booster' object has no attribute 'handle'', from <command-537005303498582>, line 177. Full traceback below:\nTraceback (most recent call last):\n  File \"<command-537005303498582>\", line 177, in predict_distributed\n  File \"<command-537005303498582>\", line 177, in predict_distributed\n  File \"/databricks/python/lib/python3.8/site-packages/mlflow/pyfunc/__init__.py\", line 413, in predict\n    return self._predict_fn(data)\n  File \"/databricks/python/lib/python3.8/site-packages/mlflow/pyfunc/model.py\", line 305, in predict\n    return self.python_model.predict(self.context, model_input)\n  File \"/databricks/python/lib/python3.8/site-packages/phgml/models/model_wrapper.py\", line 380, in predict\n    return self.model_wrapper_model.predict(model_input)\n  File \"/databricks/python/lib/python3.8/site-packages/phgml/models/model_wrapper.py\", line 354, in predict\n    y_pred_dct = predictor._predict(x_test)\n  File \"/databricks/python/lib/python3.8/site-packages/phgml/models/model_strategy.py\", line 127, in _predict\n    pred = self.sub_predictors[target_][\"predictors\"][qtile].predict(\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/lib/python3.8/site-packages/lightgbm/sklearn.py\", line 803, in predict\n    return self._Booster.predict(X, raw_score=raw_score, start_iteration=start_iteration, num_iteration=num_iteration,\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/lib/python3.8/site-packages/lightgbm/basic.py\", line 3532, in predict\n    predictor = self._to_predictor(deepcopy(kwargs))\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/lib/python3.8/site-packages/lightgbm/basic.py\", line 3622, in _to_predictor\n    predictor = _InnerPredictor(booster_handle=self.handle, pred_parameter=pred_parameter)\nAttributeError: 'Booster' object has no attribute 'handle'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:155)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:161)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3185)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3120)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3107)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3107)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1448)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1448)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1448)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3393)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3334)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3322)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1214)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2665)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:266)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:276)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:81)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:87)\n\tat org.apache.spark.sql.execution.collect.InternalRowFormat$.collect(cachedSparkResults.scala:75)\n\tat org.apache.spark.sql.execution.collect.InternalRowFormat$.collect(cachedSparkResults.scala:62)\n\tat org.apache.spark.sql.execution.ResultCacheManager.collectResult$1(ResultCacheManager.scala:573)\n\tat org.apache.spark.sql.execution.ResultCacheManager.computeResult(ResultCacheManager.scala:582)\n\tat org.apache.spark.sql.execution.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:528)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:527)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:424)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:403)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:424)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3153)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectResult$1(Dataset.scala:3144)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3951)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:245)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:393)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:192)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:979)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:147)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:343)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3949)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3143)\n\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation0(OutputAggregator.scala:266)\n\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation(OutputAggregator.scala:100)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocalBase.generateTableResult(PythonDriverLocalBase.scala:729)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal.computeListResultsItem(PythonDriverLocal.scala:627)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocalBase.genListResults(PythonDriverLocalBase.scala:636)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal.$anonfun$getResultBufferInternal$1(PythonDriverLocal.scala:682)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal.withInterpLock(PythonDriverLocal.scala:563)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal.getResultBufferInternal(PythonDriverLocal.scala:642)\n\tat com.databricks.backend.daemon.driver.DriverLocal.getResultBuffer(DriverLocal.scala:759)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal.outputSuccess(PythonDriverLocal.scala:605)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal.$anonfun$repl$6(PythonDriverLocal.scala:223)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal.withInterpLock(PythonDriverLocal.scala:563)\n\tat com.databricks.backend.daemon.driver.PythonDriverLocal.repl(PythonDriverLocal.scala:210)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$13(DriverLocal.scala:645)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:60)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:60)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:622)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:634)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:626)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$3(DriverWrapper.scala:665)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:425)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:445)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:56)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionTags(DriverWrapper.scala:56)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:420)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:339)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.recordOperationWithResultTags(DriverWrapper.scala:56)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:665)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:543)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:578)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$runInnerLoop$1(DriverWrapper.scala:446)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.withAttributionContext(DriverWrapper.scala:56)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:446)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:383)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:234)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.api.python.PythonException: 'AttributeError: 'Booster' object has no attribute 'handle'', from <command-537005303498582>, line 177. Full traceback below:\nTraceback (most recent call last):\n  File \"<command-537005303498582>\", line 177, in predict_distributed\n  File \"<command-537005303498582>\", line 177, in predict_distributed\n  File \"/databricks/python/lib/python3.8/site-packages/mlflow/pyfunc/__init__.py\", line 413, in predict\n    return self._predict_fn(data)\n  File \"/databricks/python/lib/python3.8/site-packages/mlflow/pyfunc/model.py\", line 305, in predict\n    return self.python_model.predict(self.context, model_input)\n  File \"/databricks/python/lib/python3.8/site-packages/phgml/models/model_wrapper.py\", line 380, in predict\n    return self.model_wrapper_model.predict(model_input)\n  File \"/databricks/python/lib/python3.8/site-packages/phgml/models/model_wrapper.py\", line 354, in predict\n    y_pred_dct = predictor._predict(x_test)\n  File \"/databricks/python/lib/python3.8/site-packages/phgml/models/model_strategy.py\", line 127, in _predict\n    pred = self.sub_predictors[target_][\"predictors\"][qtile].predict(\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/lib/python3.8/site-packages/lightgbm/sklearn.py\", line 803, in predict\n    return self._Booster.predict(X, raw_score=raw_score, start_iteration=start_iteration, num_iteration=num_iteration,\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/lib/python3.8/site-packages/lightgbm/basic.py\", line 3532, in predict\n    predictor = self._to_predictor(deepcopy(kwargs))\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/lib/python3.8/site-packages/lightgbm/basic.py\", line 3622, in _to_predictor\n    predictor = _InnerPredictor(booster_handle=self.handle, pred_parameter=pred_parameter)\nAttributeError: 'Booster' object has no attribute 'handle'\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:155)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:161)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
       "errorSummary": "PythonException: 'AttributeError: 'Booster' object has no attribute 'handle'', from <command-537005303498582>, line 177. Full traceback below:",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e8c4bcd-915f-4807-97ee-3819a5450ab1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env_config.inference_intermediate_table = \"test_inference_intermediate_table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80eb5498-147f-41da-b043-d94178e59cf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">24/10/11/ 10:02:20 UTC:PHGML-INFERENCE-REVENUE-PMS-INFO-Writing inference results to temporary table test_inference_intermediate_table\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">24/10/11/ 10:02:20 UTC:PHGML-INFERENCE-REVENUE-PMS-INFO-Writing inference results to temporary table test_inference_intermediate_table\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n",
       "<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n",
       "<span class=\"ansi-green-fg\">&lt;command-537005303498587&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> start_time_temp <span class=\"ansi-blue-fg\">=</span> time<span class=\"ansi-blue-fg\">.</span>perf_counter<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> (\n",
       "<span class=\"ansi-green-fg\">----&gt; 6</span><span class=\"ansi-red-fg\">     </span>output_df<span class=\"ansi-blue-fg\">.</span>write<span class=\"ansi-blue-fg\">.</span>mode<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;overwrite&#34;</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      7</span>     <span class=\"ansi-blue-fg\">.</span>option<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;overwriteSchema&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;true&#34;</span><span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">      8</span>     <span class=\"ansi-blue-fg\">.</span>saveAsTable<span class=\"ansi-blue-fg\">(</span>env_config<span class=\"ansi-blue-fg\">.</span>inference_intermediate_table<span class=\"ansi-blue-fg\">)</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">saveAsTable</span><span class=\"ansi-blue-fg\">(self, name, format, mode, partitionBy, **options)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    804</span>         <span class=\"ansi-green-fg\">if</span> format <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">not</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    805</span>             self<span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span>format<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 806</span><span class=\"ansi-red-fg\">         </span>self<span class=\"ansi-blue-fg\">.</span>_jwrite<span class=\"ansi-blue-fg\">.</span>saveAsTable<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    807</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    808</span>     def json(self, path, mode=None, compression=None, dateFormat=None, timestampFormat=None,\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n",
       "</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    115</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    116</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 117</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    118</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    119</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n",
       "\n",
       "<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n",
       "<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n",
       "</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n",
       "<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n",
       "\n",
       "<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o3404.saveAsTable.\n",
       ": org.apache.spark.SparkException: Job aborted.\n",
       "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:606)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:360)\n",
       "\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFiles$8(TransactionalWriteEdge.scala:434)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:245)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:393)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:192)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:979)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:147)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:343)\n",
       "\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFiles$1(TransactionalWriteEdge.scala:369)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:171)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:158)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.withOperationTypeTag(OptimisticTransaction.scala:105)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperation$6(DeltaLogging.scala:136)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:186)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:184)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordFrameProfile(OptimisticTransaction.scala:105)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperation$5(DeltaLogging.scala:135)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:330)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:425)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:445)\n",
       "\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n",
       "\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:24)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:24)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:420)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:339)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:24)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:330)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:302)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:24)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:61)\n",
       "\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:143)\n",
       "\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:73)\n",
       "\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:60)\n",
       "\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:102)\n",
       "\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:431)\n",
       "\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:410)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordOperation(OptimisticTransaction.scala:105)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:134)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:119)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordDeltaOperation(OptimisticTransaction.scala:105)\n",
       "\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$recordWriteFilesOperation$1(TransactionalWriteEdge.scala:211)\n",
       "\tat com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:1585)\n",
       "\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.recordWriteFilesOperation(TransactionalWriteEdge.scala:210)\n",
       "\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles(TransactionalWriteEdge.scala:232)\n",
       "\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles$(TransactionalWriteEdge.scala:226)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:105)\n",
       "\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles(TransactionalWriteEdge.scala:492)\n",
       "\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles$(TransactionalWriteEdge.scala:483)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:105)\n",
       "\tat com.databricks.sql.transaction.tahoe.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:218)\n",
       "\tat com.databricks.sql.transaction.tahoe.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:215)\n",
       "\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:105)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.write(WriteIntoDelta.scala:332)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$2(CreateDeltaTableCommand.scala:144)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:171)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:158)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:55)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperation$6(DeltaLogging.scala:136)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:186)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:184)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:55)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperation$5(DeltaLogging.scala:135)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:330)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:425)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:445)\n",
       "\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n",
       "\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:24)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\n",
       "\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:24)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:420)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:339)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:24)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:330)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:302)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:24)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:61)\n",
       "\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:143)\n",
       "\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:73)\n",
       "\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:60)\n",
       "\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:102)\n",
       "\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:431)\n",
       "\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:410)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:55)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:134)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:119)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:55)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:119)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:216)\n",
       "\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:607)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:504)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1715)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable(WriteToDataSourceV2Exec.scala:485)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable$(WriteToDataSourceV2Exec.scala:480)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:203)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:237)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
       "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:174)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:245)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:393)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:192)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:979)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:147)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:343)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:174)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:170)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:590)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:168)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:590)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:566)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:170)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:170)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:155)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:146)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:200)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:959)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:721)\n",
       "\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:652)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:295)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 71.0 failed 4 times, most recent failure: Lost task 5.3 in stage 71.0 (TID 1794) (10.139.64.18 executor 1): org.apache.spark.api.python.PythonException: &#39;AttributeError: &#39;Booster&#39; object has no attribute &#39;handle&#39;&#39;, from &lt;command-537005303498582&gt;, line 177. Full traceback below:\n",
       "Traceback (most recent call last):\n",
       "  File &#34;&lt;command-537005303498582&gt;&#34;, line 177, in predict_distributed\n",
       "  File &#34;&lt;command-537005303498582&gt;&#34;, line 177, in predict_distributed\n",
       "  File &#34;/databricks/python/lib/python3.8/site-packages/mlflow/pyfunc/__init__.py&#34;, line 413, in predict\n",
       "    return self._predict_fn(data)\n",
       "  File &#34;/databricks/python/lib/python3.8/site-packages/mlflow/pyfunc/model.py&#34;, line 305, in predict\n",
       "    return self.python_model.predict(self.context, model_input)\n",
       "  File &#34;/databricks/python/lib/python3.8/site-packages/phgml/models/model_wrapper.py&#34;, line 380, in predict\n",
       "    return self.model_wrapper_model.predict(model_input)\n",
       "  File &#34;/databricks/python/lib/python3.8/site-packages/phgml/models/model_wrapper.py&#34;, line 354, in predict\n",
       "    y_pred_dct = predictor._predict(x_test)\n",
       "  File &#34;/databricks/python/lib/python3.8/site-packages/phgml/models/model_strategy.py&#34;, line 127, in _predict\n",
       "    pred = self.sub_predictors[target_][&#34;predictors&#34;][qtile].predict(\n",
       "  File &#34;/local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/lib/python3.8/site-packages/lightgbm/sklearn.py&#34;, line 803, in predict\n",
       "    return self._Booster.predict(X, raw_score=raw_score, start_iteration=start_iteration, num_iteration=num_iteration,\n",
       "  File &#34;/local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/lib/python3.8/site-packages/lightgbm/basic.py&#34;, line 3532, in predict\n",
       "    predictor = self._to_predictor(deepcopy(kwargs))\n",
       "  File &#34;/local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/lib/python3.8/site-packages/lightgbm/basic.py&#34;, line 3622, in _to_predictor\n",
       "    predictor = _InnerPredictor(booster_handle=self.handle, pred_parameter=pred_parameter)\n",
       "AttributeError: &#39;Booster&#39; object has no attribute &#39;handle&#39;\n",
       "\n",
       "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n",
       "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
       "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
       "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n",
       "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:409)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$20(FileFormatWriter.scala:336)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:161)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "\n",
       "Driver stacktrace:\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3185)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3120)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3107)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
       "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
       "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3107)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1448)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1448)\n",
       "\tat scala.Option.foreach(Option.scala:407)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1448)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3393)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3334)\n",
       "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3322)\n",
       "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1214)\n",
       "\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2665)\n",
       "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2648)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:325)\n",
       "\t... 151 more\n",
       "Caused by: org.apache.spark.api.python.PythonException: &#39;AttributeError: &#39;Booster&#39; object has no attribute &#39;handle&#39;&#39;, from &lt;command-537005303498582&gt;, line 177. Full traceback below:\n",
       "Traceback (most recent call last):\n",
       "  File &#34;&lt;command-537005303498582&gt;&#34;, line 177, in predict_distributed\n",
       "  File &#34;&lt;command-537005303498582&gt;&#34;, line 177, in predict_distributed\n",
       "  File &#34;/databricks/python/lib/python3.8/site-packages/mlflow/pyfunc/__init__.py&#34;, line 413, in predict\n",
       "    return self._predict_fn(data)\n",
       "  File &#34;/databricks/python/lib/python3.8/site-packages/mlflow/pyfunc/model.py&#34;, line 305, in predict\n",
       "    return self.python_model.predict(self.context, model_input)\n",
       "  File &#34;/databricks/python/lib/python3.8/site-packages/phgml/models/model_wrapper.py&#34;, line 380, in predict\n",
       "    return self.model_wrapper_model.predict(model_input)\n",
       "  File &#34;/databricks/python/lib/python3.8/site-packages/phgml/models/model_wrapper.py&#34;, line 354, in predict\n",
       "    y_pred_dct = predictor._predict(x_test)\n",
       "  File &#34;/databricks/python/lib/python3.8/site-packages/phgml/models/model_strategy.py&#34;, line 127, in _predict\n",
       "    pred = self.sub_predictors[target_][&#34;predictors&#34;][qtile].predict(\n",
       "  File &#34;/local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/lib/python3.8/site-packages/lightgbm/sklearn.py&#34;, line 803, in predict\n",
       "    return self._Booster.predict(X, raw_score=raw_score, start_iteration=start_iteration, num_iteration=num_iteration,\n",
       "  File &#34;/local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/lib/python3.8/site-packages/lightgbm/basic.py&#34;, line 3532, in predict\n",
       "    predictor = self._to_predictor(deepcopy(kwargs))\n",
       "  File &#34;/local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/lib/python3.8/site-packages/lightgbm/basic.py&#34;, line 3622, in _to_predictor\n",
       "    predictor = _InnerPredictor(booster_handle=self.handle, pred_parameter=pred_parameter)\n",
       "AttributeError: &#39;Booster&#39; object has no attribute &#39;handle&#39;\n",
       "\n",
       "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n",
       "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n",
       "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n",
       "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n",
       "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
       "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
       "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:409)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$20(FileFormatWriter.scala:336)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n",
       "\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:161)\n",
       "\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n",
       "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n",
       "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\t... 1 more\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-537005303498587&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      4</span> start_time_temp <span class=\"ansi-blue-fg\">=</span> time<span class=\"ansi-blue-fg\">.</span>perf_counter<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> (\n<span class=\"ansi-green-fg\">----&gt; 6</span><span class=\"ansi-red-fg\">     </span>output_df<span class=\"ansi-blue-fg\">.</span>write<span class=\"ansi-blue-fg\">.</span>mode<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;overwrite&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      7</span>     <span class=\"ansi-blue-fg\">.</span>option<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;overwriteSchema&#34;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#34;true&#34;</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      8</span>     <span class=\"ansi-blue-fg\">.</span>saveAsTable<span class=\"ansi-blue-fg\">(</span>env_config<span class=\"ansi-blue-fg\">.</span>inference_intermediate_table<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/readwriter.py</span> in <span class=\"ansi-cyan-fg\">saveAsTable</span><span class=\"ansi-blue-fg\">(self, name, format, mode, partitionBy, **options)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    804</span>         <span class=\"ansi-green-fg\">if</span> format <span class=\"ansi-green-fg\">is</span> <span class=\"ansi-green-fg\">not</span> <span class=\"ansi-green-fg\">None</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    805</span>             self<span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span>format<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">--&gt; 806</span><span class=\"ansi-red-fg\">         </span>self<span class=\"ansi-blue-fg\">.</span>_jwrite<span class=\"ansi-blue-fg\">.</span>saveAsTable<span class=\"ansi-blue-fg\">(</span>name<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    807</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    808</span>     def json(self, path, mode=None, compression=None, dateFormat=None, timestampFormat=None,\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1302</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-fg\">-&gt; 1304</span><span class=\"ansi-red-fg\">         return_value = get_return_value(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1305</span>             answer, self.gateway_client, self.target_id, self.name)\n<span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    115</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    116</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 117</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    118</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    119</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    324</span>             value <span class=\"ansi-blue-fg\">=</span> OUTPUT_CONVERTER<span class=\"ansi-blue-fg\">[</span>type<span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">(</span>answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">2</span><span class=\"ansi-blue-fg\">:</span><span class=\"ansi-blue-fg\">]</span><span class=\"ansi-blue-fg\">,</span> gateway_client<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    325</span>             <span class=\"ansi-green-fg\">if</span> answer<span class=\"ansi-blue-fg\">[</span><span class=\"ansi-cyan-fg\">1</span><span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-blue-fg\">==</span> REFERENCE_TYPE<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 326</span><span class=\"ansi-red-fg\">                 raise Py4JJavaError(\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    328</span>                     format(target_id, &#34;.&#34;, name), value)\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o3404.saveAsTable.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:606)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:360)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFiles$8(TransactionalWriteEdge.scala:434)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:245)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:393)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:192)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:979)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:147)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:343)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$writeFiles$1(TransactionalWriteEdge.scala:369)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:171)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:158)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.withOperationTypeTag(OptimisticTransaction.scala:105)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperation$6(DeltaLogging.scala:136)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:186)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:184)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordFrameProfile(OptimisticTransaction.scala:105)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperation$5(DeltaLogging.scala:135)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:330)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:425)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:445)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:24)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:24)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:420)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:339)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:24)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:330)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:302)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:24)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:61)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:143)\n\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:73)\n\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:60)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:102)\n\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:431)\n\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:410)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordOperation(OptimisticTransaction.scala:105)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:134)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:119)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.recordDeltaOperation(OptimisticTransaction.scala:105)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.$anonfun$recordWriteFilesOperation$1(TransactionalWriteEdge.scala:211)\n\tat com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:1585)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.recordWriteFilesOperation(TransactionalWriteEdge.scala:210)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles(TransactionalWriteEdge.scala:232)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles$(TransactionalWriteEdge.scala:226)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:105)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles(TransactionalWriteEdge.scala:492)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWriteEdge.writeFiles$(TransactionalWriteEdge.scala:483)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:105)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:218)\n\tat com.databricks.sql.transaction.tahoe.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:215)\n\tat com.databricks.sql.transaction.tahoe.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:105)\n\tat com.databricks.sql.transaction.tahoe.commands.WriteIntoDelta.write(WriteIntoDelta.scala:332)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.$anonfun$run$2(CreateDeltaTableCommand.scala:144)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:171)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:158)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.withOperationTypeTag(CreateDeltaTableCommand.scala:55)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperation$6(DeltaLogging.scala:136)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:186)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:184)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordFrameProfile(CreateDeltaTableCommand.scala:55)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperation$5(DeltaLogging.scala:135)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:330)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:425)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:445)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.$anonfun$withAttributionContext$1(Log4jUsageLoggingShim.scala:33)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:94)\n\tat com.databricks.logging.Log4jUsageLoggingShim$.withAttributionContext(Log4jUsageLoggingShim.scala:31)\n\tat com.databricks.logging.UsageLogging.withAttributionContext(UsageLogging.scala:205)\n\tat com.databricks.logging.UsageLogging.withAttributionContext$(UsageLogging.scala:204)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:24)\n\tat com.databricks.logging.UsageLogging.withAttributionTags(UsageLogging.scala:240)\n\tat com.databricks.logging.UsageLogging.withAttributionTags$(UsageLogging.scala:225)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:24)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:420)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:339)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:24)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:330)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:302)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:24)\n\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:61)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:143)\n\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:73)\n\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:60)\n\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:102)\n\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:431)\n\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:410)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordOperation(CreateDeltaTableCommand.scala:55)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:134)\n\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:119)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.recordDeltaOperation(CreateDeltaTableCommand.scala:55)\n\tat com.databricks.sql.transaction.tahoe.commands.CreateDeltaTableCommand.run(CreateDeltaTableCommand.scala:119)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog.com$databricks$sql$transaction$tahoe$catalog$DeltaCatalog$$createDeltaTable(DeltaCatalog.scala:216)\n\tat com.databricks.sql.transaction.tahoe.catalog.DeltaCatalog$StagedDeltaTableV2.commitStagedChanges(DeltaCatalog.scala:607)\n\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:504)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1715)\n\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable(WriteToDataSourceV2Exec.scala:485)\n\tat org.apache.spark.sql.execution.datasources.v2.TableWriteExecHelper.writeToTable$(WriteToDataSourceV2Exec.scala:480)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:203)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:237)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:174)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$8(SQLExecution.scala:245)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:393)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:192)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:979)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:147)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:343)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:174)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:170)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:590)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:168)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:590)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:566)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:170)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:324)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:170)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:155)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:146)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:200)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:959)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:721)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:652)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 71.0 failed 4 times, most recent failure: Lost task 5.3 in stage 71.0 (TID 1794) (10.139.64.18 executor 1): org.apache.spark.api.python.PythonException: &#39;AttributeError: &#39;Booster&#39; object has no attribute &#39;handle&#39;&#39;, from &lt;command-537005303498582&gt;, line 177. Full traceback below:\nTraceback (most recent call last):\n  File &#34;&lt;command-537005303498582&gt;&#34;, line 177, in predict_distributed\n  File &#34;&lt;command-537005303498582&gt;&#34;, line 177, in predict_distributed\n  File &#34;/databricks/python/lib/python3.8/site-packages/mlflow/pyfunc/__init__.py&#34;, line 413, in predict\n    return self._predict_fn(data)\n  File &#34;/databricks/python/lib/python3.8/site-packages/mlflow/pyfunc/model.py&#34;, line 305, in predict\n    return self.python_model.predict(self.context, model_input)\n  File &#34;/databricks/python/lib/python3.8/site-packages/phgml/models/model_wrapper.py&#34;, line 380, in predict\n    return self.model_wrapper_model.predict(model_input)\n  File &#34;/databricks/python/lib/python3.8/site-packages/phgml/models/model_wrapper.py&#34;, line 354, in predict\n    y_pred_dct = predictor._predict(x_test)\n  File &#34;/databricks/python/lib/python3.8/site-packages/phgml/models/model_strategy.py&#34;, line 127, in _predict\n    pred = self.sub_predictors[target_][&#34;predictors&#34;][qtile].predict(\n  File &#34;/local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/lib/python3.8/site-packages/lightgbm/sklearn.py&#34;, line 803, in predict\n    return self._Booster.predict(X, raw_score=raw_score, start_iteration=start_iteration, num_iteration=num_iteration,\n  File &#34;/local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/lib/python3.8/site-packages/lightgbm/basic.py&#34;, line 3532, in predict\n    predictor = self._to_predictor(deepcopy(kwargs))\n  File &#34;/local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/lib/python3.8/site-packages/lightgbm/basic.py&#34;, line 3622, in _to_predictor\n    predictor = _InnerPredictor(booster_handle=self.handle, pred_parameter=pred_parameter)\nAttributeError: &#39;Booster&#39; object has no attribute &#39;handle&#39;\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:409)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$20(FileFormatWriter.scala:336)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:161)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3185)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3120)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3107)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3107)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1448)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1448)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1448)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3393)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3334)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3322)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1214)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2665)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2648)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:325)\n\t... 151 more\nCaused by: org.apache.spark.api.python.PythonException: &#39;AttributeError: &#39;Booster&#39; object has no attribute &#39;handle&#39;&#39;, from &lt;command-537005303498582&gt;, line 177. Full traceback below:\nTraceback (most recent call last):\n  File &#34;&lt;command-537005303498582&gt;&#34;, line 177, in predict_distributed\n  File &#34;&lt;command-537005303498582&gt;&#34;, line 177, in predict_distributed\n  File &#34;/databricks/python/lib/python3.8/site-packages/mlflow/pyfunc/__init__.py&#34;, line 413, in predict\n    return self._predict_fn(data)\n  File &#34;/databricks/python/lib/python3.8/site-packages/mlflow/pyfunc/model.py&#34;, line 305, in predict\n    return self.python_model.predict(self.context, model_input)\n  File &#34;/databricks/python/lib/python3.8/site-packages/phgml/models/model_wrapper.py&#34;, line 380, in predict\n    return self.model_wrapper_model.predict(model_input)\n  File &#34;/databricks/python/lib/python3.8/site-packages/phgml/models/model_wrapper.py&#34;, line 354, in predict\n    y_pred_dct = predictor._predict(x_test)\n  File &#34;/databricks/python/lib/python3.8/site-packages/phgml/models/model_strategy.py&#34;, line 127, in _predict\n    pred = self.sub_predictors[target_][&#34;predictors&#34;][qtile].predict(\n  File &#34;/local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/lib/python3.8/site-packages/lightgbm/sklearn.py&#34;, line 803, in predict\n    return self._Booster.predict(X, raw_score=raw_score, start_iteration=start_iteration, num_iteration=num_iteration,\n  File &#34;/local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/lib/python3.8/site-packages/lightgbm/basic.py&#34;, line 3532, in predict\n    predictor = self._to_predictor(deepcopy(kwargs))\n  File &#34;/local_disk0/.ephemeral_nfs/envs/pythonEnv-313cd6c0-bb9d-456f-a8a0-cffe0045e9a7/lib/python3.8/site-packages/lightgbm/basic.py&#34;, line 3622, in _to_predictor\n    predictor = _InnerPredictor(booster_handle=self.handle, pred_parameter=pred_parameter)\nAttributeError: &#39;Booster&#39; object has no attribute &#39;handle&#39;\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:694)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:50)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:647)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:409)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$20(FileFormatWriter.scala:336)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:161)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:832)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1681)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:835)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:690)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n</div>",
       "errorSummary": "org.apache.spark.SparkException: Job aborted.",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger.info(\n",
    "    f\"Writing inference results to temporary table {env_config.inference_intermediate_table}\"\n",
    ")\n",
    "start_time_temp = time.perf_counter()\n",
    "(\n",
    "    output_df.write.mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(env_config.inference_intermediate_table)\n",
    ")\n",
    "elapsed_time_temp = time.perf_counter() - start_time_temp\n",
    "logger.info(f\"Time elapsed {elapsed_time_temp}\")\n",
    "logger.info(f\"Time elapsed in minutes {elapsed_time_temp/60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e778322-6569-414e-a48f-379500537368",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">24/10/11/ 10:03:35 UTC:PHGML-INFERENCE-REVENUE-PMS-INFO-43 out of 43 hotels processed succussfully\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">24/10/11/ 10:03:35 UTC:PHGML-INFERENCE-REVENUE-PMS-INFO-43 out of 43 hotels processed succussfully\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "meta_columns = [\"HotelID\", \"run_id\", \"timestamp\", \"pms_sync_off\", \"status\", \"message\"]\n",
    "results_table = spark.sql(f\"select * from {env_config.inference_intermediate_table}\")\n",
    "output_meta = results_table.select(meta_columns).toPandas()\n",
    "\n",
    "num_completed = output_meta[output_meta[\"status\"] == \"complete\"][\"HotelID\"].nunique()\n",
    "total = output_meta[\"HotelID\"].nunique()\n",
    "logger.info(f\"{num_completed} out of {total} hotels processed succussfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7165f8c3-e2b7-4fce-b65a-5000188237b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "incomplete = output_meta[~(output_meta[\"status\"] == \"complete\")]\n",
    "\n",
    "for row in incomplete.itertuples():\n",
    "    logger.error(\n",
    "        f\"Error encountered when processing hotel {row.HotelID}: {row.message}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f690b047-e988-4d79-85c0-5a31da964165",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_df = results_table.filter(results_table.status == \"complete\").drop(\n",
    "    \"status\", \"message\"\n",
    ")\n",
    "\n",
    "#Drop forecast currency if TARGET_TYPE is ROOMS\n",
    "if TARGET_TYPE == \"ROOMS\":\n",
    "    output_df = output_df.drop(\"forecast_currency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "429bc7a9-70ca-4e16-9b5d-0629f6e2480d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env_config.inference_output_table = \"test_inference_output_table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ce7a835-9951-4468-bba7-b51fd51024bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">24/10/11/ 10:03:36 UTC:PHGML-INFERENCE-REVENUE-PMS-INFO-Writing completed results to table\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">24/10/11/ 10:03:36 UTC:PHGML-INFERENCE-REVENUE-PMS-INFO-Writing completed results to table\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger.info(\"Writing completed results to table\")\n",
    "file_format = \"delta\"\n",
    "\n",
    "(\n",
    "    output_df.write.format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .partitionBy(\"HotelID\")\n",
    "    # .option(\"path\", env_config.inference_output_table_blob)\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(env_config.inference_output_table)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85c6364a-a576-4b10-8316-0cc42361c14e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">24/10/11/ 10:03:48 UTC:PHGML-INFERENCE-REVENUE-PMS-INFO-Time elapsed 1229.457101305\n",
       "24/10/11/ 10:03:48 UTC:PHGML-INFERENCE-REVENUE-PMS-INFO-Time elapsed in minutes 20.490951688416665\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">24/10/11/ 10:03:48 UTC:PHGML-INFERENCE-REVENUE-PMS-INFO-Time elapsed 1229.457101305\n24/10/11/ 10:03:48 UTC:PHGML-INFERENCE-REVENUE-PMS-INFO-Time elapsed in minutes 20.490951688416665\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "elapsed_time = time.perf_counter() - start_time\n",
    "logger.info(f\"Time elapsed {elapsed_time}\")\n",
    "logger.info(f\"Time elapsed in minutes {elapsed_time/60}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "mod_forecast_inference_distributed",
   "widgets": {
    "env_stage": {
     "currentValue": "dev",
     "nuid": "52458213-982d-4c83-9d32-c0fbccca2763",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dev",
      "label": "Pipeline stage",
      "name": "env_stage",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "dev",
        "prod"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "dev",
      "label": "Pipeline stage",
      "name": "env_stage",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "dev",
        "prod"
       ]
      }
     }
    },
    "exclude_pms": {
     "currentValue": "False",
     "nuid": "f9f908da-bea1-4591-a4a0-e6e50dde3cb6",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "False",
      "label": "Exclude PMS",
      "name": "exclude_pms",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "True",
        "False"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "False",
      "label": "Exclude PMS",
      "name": "exclude_pms",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "True",
        "False"
       ]
      }
     }
    },
    "is_usd_currency": {
     "currentValue": "True",
     "nuid": "7d5dc722-0b9e-4093-8205-7d3b592cb9d1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "True",
      "label": "Use USD currency",
      "name": "is_usd_currency",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "True",
        "False"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "True",
      "label": "Use USD currency",
      "name": "is_usd_currency",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "True",
        "False"
       ]
      }
     }
    },
    "lag_numbers": {
     "currentValue": "1,7,14,28",
     "nuid": "4ddfc541-da5d-4742-8d21-54aee71a8c80",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "1,7,14,28",
      "label": "Lag Numbers",
      "name": "lag_numbers",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "1,7,14,28",
      "label": "Lag Numbers",
      "name": "lag_numbers",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "selected_hotels": {
     "currentValue": "",
     "nuid": "8e69e311-6913-404f-9f85-8ebf9742afbd",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Hotels",
      "name": "selected_hotels",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Hotels",
      "name": "selected_hotels",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "target_type": {
     "currentValue": "REVENUE",
     "nuid": "96a2866e-a29a-4d94-9653-3322e4430b05",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "REVENUE",
      "label": "Target Type",
      "name": "target_type",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "REVENUE",
        "ROOMS"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "REVENUE",
      "label": "Target Type",
      "name": "target_type",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "REVENUE",
        "ROOMS"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}