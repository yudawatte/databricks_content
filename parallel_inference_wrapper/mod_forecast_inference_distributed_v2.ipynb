{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d0cfc3e-dfae-47ed-bb91-70c147eec77e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Python interpreter will be restarted.\n",
       "Requirement already satisfied: mlflow in /databricks/python3/lib/python3.8/site-packages (2.2.2)\n",
       "Requirement already satisfied: scipy&lt;2 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.6.2)\n",
       "Requirement already satisfied: requests&lt;3,&gt;=2.17.3 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (2.32.3)\n",
       "Requirement already satisfied: entrypoints&lt;1 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (0.3)\n",
       "Requirement already satisfied: markdown&lt;4,&gt;=3.3 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (3.7)\n",
       "Requirement already satisfied: cloudpickle&lt;3 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (2.2.1)\n",
       "Requirement already satisfied: scikit-learn&lt;2 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.0.2)\n",
       "Requirement already satisfied: shap&lt;1,&gt;=0.40 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (0.44.1)\n",
       "Requirement already satisfied: pytz&lt;2023 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (2020.5)\n",
       "Requirement already satisfied: pyyaml&lt;7,&gt;=5.1 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (6.0.2)\n",
       "Requirement already satisfied: docker&lt;7,&gt;=4.0.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (6.1.3)\n",
       "Requirement already satisfied: pandas&lt;3 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.4.4)\n",
       "Requirement already satisfied: querystring-parser&lt;2 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.2.4)\n",
       "Requirement already satisfied: numpy&lt;2 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.22.4)\n",
       "Requirement already satisfied: importlib-metadata!=4.7.0,&lt;7,&gt;=3.7.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (6.11.0)\n",
       "Requirement already satisfied: protobuf&lt;5,&gt;=3.12.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (3.17.2)\n",
       "Requirement already satisfied: pyarrow&lt;12,&gt;=4.0.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (4.0.0)\n",
       "Requirement already satisfied: Flask&lt;3 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (2.3.3)\n",
       "Requirement already satisfied: packaging&lt;24 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (23.2)\n",
       "Requirement already satisfied: databricks-cli&lt;1,&gt;=0.8.7 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (0.18.0)\n",
       "Requirement already satisfied: alembic&lt;2 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.13.3)\n",
       "Requirement already satisfied: sqlparse&lt;1,&gt;=0.4.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (0.5.1)\n",
       "Requirement already satisfied: matplotlib&lt;4 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (3.4.2)\n",
       "Requirement already satisfied: gunicorn&lt;21 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (20.1.0)\n",
       "Requirement already satisfied: Jinja2&lt;4,&gt;=2.11 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (3.1.4)\n",
       "Requirement already satisfied: gitpython&lt;4,&gt;=2.1.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (3.1.43)\n",
       "Requirement already satisfied: click&lt;9,&gt;=7.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (8.1.7)\n",
       "Requirement already satisfied: sqlalchemy&lt;3,&gt;=1.4.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (2.0.35)\n",
       "Requirement already satisfied: importlib-resources in /databricks/python3/lib/python3.8/site-packages (from alembic&lt;2-&gt;mlflow) (6.4.5)\n",
       "Requirement already satisfied: typing-extensions&gt;=4 in /databricks/python3/lib/python3.8/site-packages (from alembic&lt;2-&gt;mlflow) (4.12.2)\n",
       "Requirement already satisfied: Mako in /databricks/python3/lib/python3.8/site-packages (from alembic&lt;2-&gt;mlflow) (1.3.5)\n",
       "Requirement already satisfied: oauthlib&gt;=3.1.0 in /databricks/python3/lib/python3.8/site-packages (from databricks-cli&lt;1,&gt;=0.8.7-&gt;mlflow) (3.2.2)\n",
       "Requirement already satisfied: pyjwt&gt;=1.7.0 in /databricks/python3/lib/python3.8/site-packages (from databricks-cli&lt;1,&gt;=0.8.7-&gt;mlflow) (2.9.0)\n",
       "Requirement already satisfied: urllib3&lt;3,&gt;=1.26.7 in /databricks/python3/lib/python3.8/site-packages (from databricks-cli&lt;1,&gt;=0.8.7-&gt;mlflow) (1.26.15)\n",
       "Requirement already satisfied: tabulate&gt;=0.7.7 in /databricks/python3/lib/python3.8/site-packages (from databricks-cli&lt;1,&gt;=0.8.7-&gt;mlflow) (0.9.0)\n",
       "Requirement already satisfied: six&gt;=1.10.0 in /databricks/python3/lib/python3.8/site-packages (from databricks-cli&lt;1,&gt;=0.8.7-&gt;mlflow) (1.15.0)\n",
       "Requirement already satisfied: websocket-client&gt;=0.32.0 in /databricks/python3/lib/python3.8/site-packages (from docker&lt;7,&gt;=4.0.0-&gt;mlflow) (1.8.0)\n",
       "Requirement already satisfied: blinker&gt;=1.6.2 in /databricks/python3/lib/python3.8/site-packages (from Flask&lt;3-&gt;mlflow) (1.8.2)\n",
       "Requirement already satisfied: Werkzeug&gt;=2.3.7 in /databricks/python3/lib/python3.8/site-packages (from Flask&lt;3-&gt;mlflow) (3.0.4)\n",
       "Requirement already satisfied: itsdangerous&gt;=2.1.2 in /databricks/python3/lib/python3.8/site-packages (from Flask&lt;3-&gt;mlflow) (2.2.0)\n",
       "Requirement already satisfied: gitdb&lt;5,&gt;=4.0.1 in /databricks/python3/lib/python3.8/site-packages (from gitpython&lt;4,&gt;=2.1.0-&gt;mlflow) (4.0.11)\n",
       "Requirement already satisfied: smmap&lt;6,&gt;=3.0.1 in /databricks/python3/lib/python3.8/site-packages (from gitdb&lt;5,&gt;=4.0.1-&gt;gitpython&lt;4,&gt;=2.1.0-&gt;mlflow) (5.0.1)\n",
       "Requirement already satisfied: setuptools&gt;=3.0 in /usr/local/lib/python3.8/dist-packages (from gunicorn&lt;21-&gt;mlflow) (52.0.0)\n",
       "Requirement already satisfied: zipp&gt;=0.5 in /databricks/python3/lib/python3.8/site-packages (from importlib-metadata!=4.7.0,&lt;7,&gt;=3.7.0-&gt;mlflow) (3.20.2)\n",
       "Requirement already satisfied: MarkupSafe&gt;=2.0 in /databricks/python3/lib/python3.8/site-packages (from Jinja2&lt;4,&gt;=2.11-&gt;mlflow) (2.1.5)\n",
       "Requirement already satisfied: cycler&gt;=0.10 in /databricks/python3/lib/python3.8/site-packages (from matplotlib&lt;4-&gt;mlflow) (0.10.0)\n",
       "Requirement already satisfied: pillow&gt;=6.2.0 in /databricks/python3/lib/python3.8/site-packages (from matplotlib&lt;4-&gt;mlflow) (8.2.0)\n",
       "Requirement already satisfied: pyparsing&gt;=2.2.1 in /databricks/python3/lib/python3.8/site-packages (from matplotlib&lt;4-&gt;mlflow) (2.4.7)\n",
       "Requirement already satisfied: kiwisolver&gt;=1.0.1 in /databricks/python3/lib/python3.8/site-packages (from matplotlib&lt;4-&gt;mlflow) (1.3.1)\n",
       "Requirement already satisfied: python-dateutil&gt;=2.7 in /databricks/python3/lib/python3.8/site-packages (from matplotlib&lt;4-&gt;mlflow) (2.8.1)\n",
       "Requirement already satisfied: certifi&gt;=2017.4.17 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.17.3-&gt;mlflow) (2020.12.5)\n",
       "Requirement already satisfied: idna&lt;4,&gt;=2.5 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.17.3-&gt;mlflow) (2.10)\n",
       "Requirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.17.3-&gt;mlflow) (3.4.0)\n",
       "Requirement already satisfied: joblib&gt;=0.11 in /databricks/python3/lib/python3.8/site-packages (from scikit-learn&lt;2-&gt;mlflow) (1.0.1)\n",
       "Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /databricks/python3/lib/python3.8/site-packages (from scikit-learn&lt;2-&gt;mlflow) (2.1.0)\n",
       "Requirement already satisfied: numba in /databricks/python3/lib/python3.8/site-packages (from shap&lt;1,&gt;=0.40-&gt;mlflow) (0.58.1)\n",
       "Requirement already satisfied: tqdm&gt;=4.27.0 in /databricks/python3/lib/python3.8/site-packages (from shap&lt;1,&gt;=0.40-&gt;mlflow) (4.66.5)\n",
       "Requirement already satisfied: slicer==0.0.7 in /databricks/python3/lib/python3.8/site-packages (from shap&lt;1,&gt;=0.40-&gt;mlflow) (0.0.7)\n",
       "Requirement already satisfied: greenlet!=0.4.17 in /databricks/python3/lib/python3.8/site-packages (from sqlalchemy&lt;3,&gt;=1.4.0-&gt;mlflow) (3.1.1)\n",
       "Requirement already satisfied: llvmlite&lt;0.42,&gt;=0.41.0dev0 in /databricks/python3/lib/python3.8/site-packages (from numba-&gt;shap&lt;1,&gt;=0.40-&gt;mlflow) (0.41.1)\n",
       "Python interpreter will be restarted.\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Python interpreter will be restarted.\nRequirement already satisfied: mlflow in /databricks/python3/lib/python3.8/site-packages (2.2.2)\nRequirement already satisfied: scipy&lt;2 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.6.2)\nRequirement already satisfied: requests&lt;3,&gt;=2.17.3 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (2.32.3)\nRequirement already satisfied: entrypoints&lt;1 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (0.3)\nRequirement already satisfied: markdown&lt;4,&gt;=3.3 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (3.7)\nRequirement already satisfied: cloudpickle&lt;3 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (2.2.1)\nRequirement already satisfied: scikit-learn&lt;2 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.0.2)\nRequirement already satisfied: shap&lt;1,&gt;=0.40 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (0.44.1)\nRequirement already satisfied: pytz&lt;2023 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (2020.5)\nRequirement already satisfied: pyyaml&lt;7,&gt;=5.1 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (6.0.2)\nRequirement already satisfied: docker&lt;7,&gt;=4.0.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (6.1.3)\nRequirement already satisfied: pandas&lt;3 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.4.4)\nRequirement already satisfied: querystring-parser&lt;2 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.2.4)\nRequirement already satisfied: numpy&lt;2 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.22.4)\nRequirement already satisfied: importlib-metadata!=4.7.0,&lt;7,&gt;=3.7.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (6.11.0)\nRequirement already satisfied: protobuf&lt;5,&gt;=3.12.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (3.17.2)\nRequirement already satisfied: pyarrow&lt;12,&gt;=4.0.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (4.0.0)\nRequirement already satisfied: Flask&lt;3 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (2.3.3)\nRequirement already satisfied: packaging&lt;24 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (23.2)\nRequirement already satisfied: databricks-cli&lt;1,&gt;=0.8.7 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (0.18.0)\nRequirement already satisfied: alembic&lt;2 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (1.13.3)\nRequirement already satisfied: sqlparse&lt;1,&gt;=0.4.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (0.5.1)\nRequirement already satisfied: matplotlib&lt;4 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (3.4.2)\nRequirement already satisfied: gunicorn&lt;21 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (20.1.0)\nRequirement already satisfied: Jinja2&lt;4,&gt;=2.11 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (3.1.4)\nRequirement already satisfied: gitpython&lt;4,&gt;=2.1.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (3.1.43)\nRequirement already satisfied: click&lt;9,&gt;=7.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (8.1.7)\nRequirement already satisfied: sqlalchemy&lt;3,&gt;=1.4.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow) (2.0.35)\nRequirement already satisfied: importlib-resources in /databricks/python3/lib/python3.8/site-packages (from alembic&lt;2-&gt;mlflow) (6.4.5)\nRequirement already satisfied: typing-extensions&gt;=4 in /databricks/python3/lib/python3.8/site-packages (from alembic&lt;2-&gt;mlflow) (4.12.2)\nRequirement already satisfied: Mako in /databricks/python3/lib/python3.8/site-packages (from alembic&lt;2-&gt;mlflow) (1.3.5)\nRequirement already satisfied: oauthlib&gt;=3.1.0 in /databricks/python3/lib/python3.8/site-packages (from databricks-cli&lt;1,&gt;=0.8.7-&gt;mlflow) (3.2.2)\nRequirement already satisfied: pyjwt&gt;=1.7.0 in /databricks/python3/lib/python3.8/site-packages (from databricks-cli&lt;1,&gt;=0.8.7-&gt;mlflow) (2.9.0)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.26.7 in /databricks/python3/lib/python3.8/site-packages (from databricks-cli&lt;1,&gt;=0.8.7-&gt;mlflow) (1.26.15)\nRequirement already satisfied: tabulate&gt;=0.7.7 in /databricks/python3/lib/python3.8/site-packages (from databricks-cli&lt;1,&gt;=0.8.7-&gt;mlflow) (0.9.0)\nRequirement already satisfied: six&gt;=1.10.0 in /databricks/python3/lib/python3.8/site-packages (from databricks-cli&lt;1,&gt;=0.8.7-&gt;mlflow) (1.15.0)\nRequirement already satisfied: websocket-client&gt;=0.32.0 in /databricks/python3/lib/python3.8/site-packages (from docker&lt;7,&gt;=4.0.0-&gt;mlflow) (1.8.0)\nRequirement already satisfied: blinker&gt;=1.6.2 in /databricks/python3/lib/python3.8/site-packages (from Flask&lt;3-&gt;mlflow) (1.8.2)\nRequirement already satisfied: Werkzeug&gt;=2.3.7 in /databricks/python3/lib/python3.8/site-packages (from Flask&lt;3-&gt;mlflow) (3.0.4)\nRequirement already satisfied: itsdangerous&gt;=2.1.2 in /databricks/python3/lib/python3.8/site-packages (from Flask&lt;3-&gt;mlflow) (2.2.0)\nRequirement already satisfied: gitdb&lt;5,&gt;=4.0.1 in /databricks/python3/lib/python3.8/site-packages (from gitpython&lt;4,&gt;=2.1.0-&gt;mlflow) (4.0.11)\nRequirement already satisfied: smmap&lt;6,&gt;=3.0.1 in /databricks/python3/lib/python3.8/site-packages (from gitdb&lt;5,&gt;=4.0.1-&gt;gitpython&lt;4,&gt;=2.1.0-&gt;mlflow) (5.0.1)\nRequirement already satisfied: setuptools&gt;=3.0 in /usr/local/lib/python3.8/dist-packages (from gunicorn&lt;21-&gt;mlflow) (52.0.0)\nRequirement already satisfied: zipp&gt;=0.5 in /databricks/python3/lib/python3.8/site-packages (from importlib-metadata!=4.7.0,&lt;7,&gt;=3.7.0-&gt;mlflow) (3.20.2)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /databricks/python3/lib/python3.8/site-packages (from Jinja2&lt;4,&gt;=2.11-&gt;mlflow) (2.1.5)\nRequirement already satisfied: cycler&gt;=0.10 in /databricks/python3/lib/python3.8/site-packages (from matplotlib&lt;4-&gt;mlflow) (0.10.0)\nRequirement already satisfied: pillow&gt;=6.2.0 in /databricks/python3/lib/python3.8/site-packages (from matplotlib&lt;4-&gt;mlflow) (8.2.0)\nRequirement already satisfied: pyparsing&gt;=2.2.1 in /databricks/python3/lib/python3.8/site-packages (from matplotlib&lt;4-&gt;mlflow) (2.4.7)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /databricks/python3/lib/python3.8/site-packages (from matplotlib&lt;4-&gt;mlflow) (1.3.1)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /databricks/python3/lib/python3.8/site-packages (from matplotlib&lt;4-&gt;mlflow) (2.8.1)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.17.3-&gt;mlflow) (2020.12.5)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.17.3-&gt;mlflow) (2.10)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.17.3-&gt;mlflow) (3.4.0)\nRequirement already satisfied: joblib&gt;=0.11 in /databricks/python3/lib/python3.8/site-packages (from scikit-learn&lt;2-&gt;mlflow) (1.0.1)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /databricks/python3/lib/python3.8/site-packages (from scikit-learn&lt;2-&gt;mlflow) (2.1.0)\nRequirement already satisfied: numba in /databricks/python3/lib/python3.8/site-packages (from shap&lt;1,&gt;=0.40-&gt;mlflow) (0.58.1)\nRequirement already satisfied: tqdm&gt;=4.27.0 in /databricks/python3/lib/python3.8/site-packages (from shap&lt;1,&gt;=0.40-&gt;mlflow) (4.66.5)\nRequirement already satisfied: slicer==0.0.7 in /databricks/python3/lib/python3.8/site-packages (from shap&lt;1,&gt;=0.40-&gt;mlflow) (0.0.7)\nRequirement already satisfied: greenlet!=0.4.17 in /databricks/python3/lib/python3.8/site-packages (from sqlalchemy&lt;3,&gt;=1.4.0-&gt;mlflow) (3.1.1)\nRequirement already satisfied: llvmlite&lt;0.42,&gt;=0.41.0dev0 in /databricks/python3/lib/python3.8/site-packages (from numba-&gt;shap&lt;1,&gt;=0.40-&gt;mlflow) (0.41.1)\nPython interpreter will be restarted.\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pip install mlflow\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcc10ce7-548b-41b4-a4fe-598457be0e8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33daa7f6-0f16-4f7a-8233-ad68bf89d3bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">sys.version_info(major=3, minor=8, micro=10, releaselevel=&#39;final&#39;, serial=0)\n",
       "Python version: 3.8.10\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">sys.version_info(major=3, minor=8, micro=10, releaselevel=&#39;final&#39;, serial=0)\nPython version: 3.8.10\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version_info)\n",
    "print(f\"Python version: {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afe309c6-c388-45f9-8cca-722637c2b6dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.widgets.dropdown(\"env_stage\", \"dev\", [\"dev\", \"prod\"], \"Pipeline stage\")\n",
    "dbutils.widgets.dropdown(\"exclude_pms\", \"False\", [\"True\", \"False\"], \"Exclude PMS\")\n",
    "dbutils.widgets.dropdown(\"target_type\", \"REVENUE\", [\"REVENUE\", \"ROOMS\"], \"Target Type\")\n",
    "dbutils.widgets.dropdown(\"is_usd_currency\", \"True\", [\"True\", \"False\"], \"Use USD currency\")\n",
    "dbutils.widgets.text(\"selected_hotels\", \"\", \"Hotels\")\n",
    "dbutils.widgets.text(\"lag_numbers\",\"1,7,14,28\", \"Lag Numbers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64c58489-de6f-47a8-8327-e2c1a65f8ce0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import os\n",
    "from autogluon.core.utils.loaders import load_pkl\n",
    "import logging\n",
    "import shutil\n",
    "import mlflow\n",
    "from mlflow import MlflowException\n",
    "import mlflow.pyfunc\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56fa25c5-8fcd-42dc-a144-881cf2aa7b49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ENV = getArgument(\"env_stage\")\n",
    "\n",
    "REPOPATH = \"/Workspace/Repos/manik@surge.global/phg-data-mlsys/src\"\n",
    "cluster_name = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterName\") \n",
    "\n",
    "if (ENV == \"dev\") and (\"dev\" in cluster_name):\n",
    "    print(f\"Loading phgml package from repo {REPOPATH}\")\n",
    "    sys.path.append(os.path.abspath(REPOPATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "598dc74f-5002-4d1a-9f62-b68a20e94a65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from phgml.models.model_wrapper import ModelWrapper\n",
    "from phgml.models.model_strategy import StrategyLGBM, StrategyAG\n",
    "from phgml.data.processing_distr_ca import remove_padded_cols\n",
    "from phgml.reporting.output_metrics import *\n",
    "from phgml.reporting.report_results import get_output_df, interpolated_fill # , correct_prediction_list\n",
    "from phgml.data.data_types import inference_output_schema\n",
    "from phgml.reporting.logging import get_dbx_logger\n",
    "from phgml.data.config import ForecastingHotelConfigProvider,EnvironmentConfig\n",
    "from phgml.utilities.task_utilities import str_to_bool, str_to_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ec3a086-c710-4ccd-b02b-a172cc545101",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from typing import Optional, Tuple, Union, List, Dict, Any, Callable\n",
    "from lightgbm import LGBMRegressor\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "__all__ = [\"BaseStrategy\", \"StrategyLGBM\", \"StrategyAG\", \"StrategyLGBMFarField\"]\n",
    "\n",
    "\n",
    "class BaseStrategy(ABC):\n",
    "    model_type = \"base_strategy\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        day_ahead: int,\n",
    "        quantile_levels: list,\n",
    "        cd_axis_targets: list,\n",
    "        path: str,\n",
    "        is_auto_reg: bool,\n",
    "        # verbose: int,\n",
    "    ):\n",
    "        self.quantile_levels = quantile_levels\n",
    "        self.day_ahead = day_ahead\n",
    "        self.cd_axis_targets = cd_axis_targets\n",
    "        self.target_prefix = self.cd_axis_targets[0][:2]\n",
    "        self.path = path\n",
    "        self.autoregressive_predictions = is_auto_reg\n",
    "        # self.verbose = verbose\n",
    "\n",
    "        self.objective = \"quantile\"\n",
    "        self.sub_predictors: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "    @abstractmethod\n",
    "    def _fit(self, train_data: pd.DataFrame) -> None:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _predict(self, test_data: pd.Series) -> Dict[float, List[npt.NDArray]]:\n",
    "        pass\n",
    "\n",
    "\n",
    "class StrategyLGBM(BaseStrategy):\n",
    "    model_type = \"LGBM\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        day_ahead: int,\n",
    "        quantile_levels: list,\n",
    "        cd_axis_targets: list,\n",
    "        path: str,\n",
    "        is_auto_reg: bool,\n",
    "        n_jobs: int = 1,\n",
    "        # verbose: int = -1,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            day_ahead=day_ahead,\n",
    "            quantile_levels=quantile_levels,\n",
    "            cd_axis_targets=cd_axis_targets,\n",
    "            path=path,\n",
    "            is_auto_reg=is_auto_reg,\n",
    "            # verbose=verbose,\n",
    "        )\n",
    "\n",
    "    def _fit(self, train_data: pd.DataFrame) -> None:\n",
    "        self.target_feature_dtypes = dict(train_data.dtypes)\n",
    "\n",
    "        # using lambda function to identify dropping cols, rather than using a if condition inside the for loop\n",
    "        drop_cols_func = lambda target_index: self.cd_axis_targets\n",
    "        if self.autoregressive_predictions:\n",
    "            drop_cols_func = lambda target_index: self.cd_axis_targets[target_index:]\n",
    "\n",
    "        for index, target_ in enumerate(self.cd_axis_targets):\n",
    "            x_data = train_data.drop(drop_cols_func(index), axis=1)\n",
    "            y_data = train_data[[target_]]\n",
    "\n",
    "            # if self.verbose != -1:\n",
    "            print(\n",
    "                \"\\t\\ttarget: \",\n",
    "                target_,\n",
    "                \" x_data_cols:\",\n",
    "                [col for col in x_data.columns if self.target_prefix in col][:6],\n",
    "                \" y_data_cols:\",\n",
    "                list(y_data.columns),\n",
    "            )\n",
    "\n",
    "            reg_objs = {}\n",
    "            for qtile in self.quantile_levels:\n",
    "                sub_predictor = LGBMRegressor(\n",
    "                    objective=self.objective, alpha=qtile, verbose=-1, n_jobs=self.n_jobs\n",
    "                )\n",
    "                sub_predictor.fit(x_data, y_data)\n",
    "                reg_objs[qtile] = sub_predictor\n",
    "            self.sub_predictors[target_] = {\n",
    "                \"predictors\": reg_objs,\n",
    "                \"targets\": list(y_data.columns),\n",
    "                \"features\": list(x_data.columns),\n",
    "            }\n",
    "\n",
    "    def _predict(self, test_data: pd.Series) -> Dict[float, List[npt.NDArray]]:\n",
    "\n",
    "        # making sure dtypes are the same as in training, and filtering out the target columns from the dtypes dict since in test data its not there.\n",
    "        needed_dtypes = {\n",
    "            col: col_dtype\n",
    "            for col, col_dtype in self.target_feature_dtypes.items()\n",
    "            if \"_tgt\" not in col\n",
    "        }\n",
    "        test_data_cpy = (\n",
    "            test_data.to_frame().T.reset_index(drop=True).copy().astype(needed_dtypes)\n",
    "        )\n",
    "\n",
    "        data = {qtile: test_data_cpy.copy() for qtile in self.quantile_levels}\n",
    "        for index, target_ in enumerate(self.cd_axis_targets):\n",
    "            feature_cols = self.sub_predictors[target_][\"features\"]\n",
    "            other_cols = [col for col in feature_cols if self.target_prefix not in col]\n",
    "\n",
    "            # if self.verbose != -1:\n",
    "            print(\n",
    "                \"\\t\\ttarget: \",\n",
    "                target_,\n",
    "                \" x_data_cols:\",\n",
    "                feature_cols[:6],\n",
    "                \" other_cols :\",\n",
    "                other_cols[:6],\n",
    "            )\n",
    "            for qtile in self.quantile_levels:\n",
    "                pred = self.sub_predictors[target_][\"predictors\"][qtile].predict(\n",
    "                    data[0.5][feature_cols]\n",
    "                )\n",
    "                data[qtile][target_] = pred\n",
    "                data[qtile] = data[qtile].sort_index(axis=1)\n",
    "\n",
    "        data = {\n",
    "            qtile: data[qtile][self.cd_axis_targets].to_numpy() for qtile in data.keys()\n",
    "        }\n",
    "        return data\n",
    "\n",
    "\n",
    "class StrategyAG(BaseStrategy):\n",
    "    model_type = \"AG\"\n",
    "    excluded_models = [\"NN_TORCH\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        day_ahead: int,\n",
    "        quantile_levels: list,\n",
    "        cd_axis_targets: list,\n",
    "        path: str,\n",
    "        is_auto_reg: bool,\n",
    "        # verbose: int = -1,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            day_ahead=day_ahead,\n",
    "            quantile_levels=quantile_levels,\n",
    "            cd_axis_targets=cd_axis_targets,\n",
    "            path=path,\n",
    "            is_auto_reg=is_auto_reg,\n",
    "            # verbose=verbose,\n",
    "        )\n",
    "        self.included_model_types = [\"GBM\"]\n",
    "\n",
    "    def _fit(self, train_data: pd.DataFrame, **kwargs) -> None:\n",
    "        self.target_feature_dtypes = dict(train_data.dtypes)\n",
    "\n",
    "        # using lambda function to identify dropping cols, rather than using a if condition inside the for loop\n",
    "        drop_cols_func = lambda target_index: self.cd_axis_targets\n",
    "        if self.autoregressive_predictions:\n",
    "            drop_cols_func = lambda target_index: self.cd_axis_targets[target_index:]\n",
    "\n",
    "        for index, target_ in enumerate(self.cd_axis_targets):\n",
    "            x_data = train_data.drop(drop_cols_func(index), axis=1)\n",
    "            y_data = train_data[[target_]]\n",
    "\n",
    "            # if self.verbose != -1:\n",
    "            print(\n",
    "                \"\\t\\ttarget: \",\n",
    "                target_,\n",
    "                \" x_data_cols:\",\n",
    "                [col for col in x_data.columns if self.target_prefix in col][:6],\n",
    "                \" y_data_cols:\",\n",
    "                list(y_data.columns),\n",
    "            )\n",
    "\n",
    "            path_i = self.path + f\"day{self.day_ahead}_{target_}\"\n",
    "\n",
    "            sub_predictor = TabularPredictor(\n",
    "                label=target_,\n",
    "                problem_type=self.objective,\n",
    "                path=path_i,\n",
    "                quantile_levels=self.quantile_levels,\n",
    "                verbosity=1,\n",
    "            )\n",
    "\n",
    "            cols_to_consider = [target_] + list(x_data.columns)\n",
    "\n",
    "            sub_predictor.fit(\n",
    "                train_data=train_data[cols_to_consider],\n",
    "                tuning_data=None,\n",
    "                excluded_model_types=self.excluded_models,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "            self.sub_predictors[target_] = {\n",
    "                \"predictors\": sub_predictor,\n",
    "                \"targets\": list(y_data.columns),\n",
    "                \"features\": list(x_data.columns),\n",
    "            }\n",
    "\n",
    "    def _predict(self, test_data: pd.Series) -> Dict[float, List[npt.NDArray]]:\n",
    "\n",
    "        # making sure dtypes are the same as in training, and filtering out the target columns from the dtypes dict since in test data its not there.\n",
    "        needed_dtypes = {\n",
    "            col: col_dtype\n",
    "            for col, col_dtype in self.target_feature_dtypes.items()\n",
    "            if \"_tgt\" not in col\n",
    "        }\n",
    "        test_data_cpy = (\n",
    "            test_data.to_frame().T.reset_index(drop=True).copy().astype(needed_dtypes)\n",
    "        )\n",
    "\n",
    "        data = {qtile: test_data_cpy.copy() for qtile in self.quantile_levels}\n",
    "        for index, target_ in enumerate(self.cd_axis_targets):\n",
    "            feature_cols = self.sub_predictors[target_][\"features\"]\n",
    "            other_cols = [col for col in feature_cols if self.target_prefix not in col]\n",
    "\n",
    "            # if self.verbose != -1:\n",
    "            print(\n",
    "                \"\\t\\ttarget: \",\n",
    "                target_,\n",
    "                \" x_data_cols:\",\n",
    "                feature_cols[:6],\n",
    "                \" other_cols :\",\n",
    "                other_cols[:6],\n",
    "            )\n",
    "            pred = self.sub_predictors[target_][\"predictors\"].predict(\n",
    "                data[0.5][feature_cols]\n",
    "            )\n",
    "            for index, qtile in enumerate(self.quantile_levels):\n",
    "\n",
    "                data[qtile][target_] = pred.iloc[:, index]\n",
    "                data[qtile] = data[qtile].sort_index(axis=1)\n",
    "\n",
    "        data = {\n",
    "            qtile: data[qtile][self.cd_axis_targets].to_numpy() for qtile in data.keys()\n",
    "        }\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5062d82-35e1-48ed-b79b-3b71cd567d24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n",
    "\n",
    "from phgml.models.base_model import BaseModel\n",
    "# from phgml.models.model_strategy import BaseStrategy\n",
    "# from phgml.models.model_strategy import StrategyLGBM, StrategyAG, StrategyLGBMFarField\n",
    "from mlflow import MlflowClient\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cloudpickle\n",
    "from sys import version_info\n",
    "import pickle\n",
    "import os\n",
    "import shutil\n",
    "from typing import Optional, Tuple, Union, List, Dict, Any, Callable\n",
    "import numpy.typing as npt\n",
    "import re\n",
    "\n",
    "class InnerException(Exception):\n",
    "    pass\n",
    "\n",
    "__all__ = [\"ModelWrapper\", \"ModelWrapperMlflowModel\", \"ModelWrapperFarField\"]\n",
    "\n",
    "PYTHON_VERSION = \"{major}.{minor}.{micro}\".format(\n",
    "    major=version_info.major, minor=version_info.minor, micro=version_info.micro\n",
    ")\n",
    "\n",
    "conda_env = {\n",
    "    \"channels\": [\"defaults\"],\n",
    "    \"dependencies\": [\n",
    "        \"python={}\".format(PYTHON_VERSION),\n",
    "        \"pip\",\n",
    "        {\n",
    "            \"pip\": [\n",
    "                \"mlflow\",\n",
    "                \"lightgbm\",\n",
    "                \"cloudpickle=={}\".format(cloudpickle.__version__),\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "    \"name\": \"model_wrapper_env\",\n",
    "}\n",
    "\n",
    "\n",
    "class ModelWrapper(BaseModel):\n",
    "    \"\"\"Custom class which wraps a model type to generate\n",
    "    predictions in a timeseries format.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cd_axis_max_lags: int,\n",
    "        static_cols: List[str],\n",
    "        model_strategy: BaseStrategy,\n",
    "        is_auto_reg: bool = False,\n",
    "        is_ca3_training: bool = True,\n",
    "        prediction_horizon: int = 28,\n",
    "        lag_numbers: List[int] = [1, 7, 14, 28],\n",
    "        quantiles: List[float] = [0.5],\n",
    "        mlflow_run_id: Optional[str] = None,\n",
    "        hotel_id: Optional[str] = None,\n",
    "        version: Optional[Union[str, int]] = None,\n",
    "        stage: Optional[str] = None,\n",
    "        target_type: str = \"REVENUE\",\n",
    "        exclude_pms: bool = False,\n",
    "        save_models: bool = True,\n",
    "        local_root_dir: Optional[str] = None,\n",
    "        model_type: str = \"MODELWRAPPER\",\n",
    "        model_name_prefix: Optional[str] = None,\n",
    "        meta_data: Dict[str, Any] = {},\n",
    "        n_cd_lags: Optional[int] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            model_type=model_strategy.model_type,\n",
    "            prediction_horizon=prediction_horizon,\n",
    "            lag_numbers=lag_numbers,\n",
    "            quantiles=quantiles,\n",
    "            mlflow_run_id=mlflow_run_id,\n",
    "            hotel_id=hotel_id,\n",
    "            version=version,\n",
    "            stage=stage,\n",
    "            target_type=target_type,\n",
    "            exclude_pms=exclude_pms,\n",
    "            save_models=save_models,\n",
    "            local_root_dir=local_root_dir,\n",
    "            model_name_prefix=model_name_prefix,\n",
    "            meta_data=meta_data,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.quantile_levels.sort()\n",
    "        self.cd_axis_max_lags = cd_axis_max_lags\n",
    "        self.sd_axis_lag_prefix = \"lag\"\n",
    "        self.static_cols = static_cols\n",
    "        self.n_cd_lags = n_cd_lags\n",
    "        self.target_suffix = \"_tgt\"\n",
    "        self.is_auto_reg = is_auto_reg\n",
    "\n",
    "        self.model_strategy = model_strategy\n",
    "        self.model_type = self.model_strategy.model_type\n",
    "        self.all_cd_cols = [\n",
    "            f\"{self.target_prefix}{i}\" for i in range(self.cd_axis_max_lags + 1)\n",
    "        ]\n",
    "        self.is_ca3_training = is_ca3_training\n",
    "\n",
    "        # initializing targets variables\n",
    "        self.target_cols: Dict[int, List[str]] = {}\n",
    "\n",
    "        # initializing feature variables\n",
    "        self.feature_cols: Dict[int, List[str]] = {}\n",
    "\n",
    "        self.envs = [\"dev\", \"qa\", \"prod\"]\n",
    "\n",
    "        if 0.5 not in self.quantile_levels:\n",
    "            raise ValueError(\n",
    "                \"median quantile (0.5) is not included in the quantile_levels. please ensure that its included\"\n",
    "            )\n",
    "\n",
    "    def save_model(self) -> None:\n",
    "        \"\"\"Saves the models in the local directory, which will then be logged as artifacts in MLflow\"\"\"\n",
    "        if os.path.exists(self.local_root):\n",
    "            self.clean()\n",
    "\n",
    "        os.makedirs(self.local_dir)\n",
    "        with open(self.local_path, \"wb\") as pkl_file:\n",
    "            pickle.dump(obj=self, file=pkl_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def change_current_env_tags(self, incoming_tags: Dict[str, str]):\n",
    "\n",
    "        env_model_tag_keys = set([f\"model_stage_{env}\" for env in self.envs])\n",
    "        incoming_tags_keys = set(incoming_tags.keys())\n",
    "\n",
    "        tags_detected = env_model_tag_keys.intersection(incoming_tags_keys)\n",
    "\n",
    "        if len(tags_detected) > 0:\n",
    "            client = MlflowClient()\n",
    "            all_registered_models_info = client.search_model_versions(\n",
    "                f\"name ='{self.get_model_name()}'\"\n",
    "            )\n",
    "            # sorting the model meta data list by version number of the considered model name in descending order\n",
    "            sorted_model_versions = sorted(\n",
    "                all_registered_models_info, key=lambda x: int(x.version), reverse=True\n",
    "            )\n",
    "\n",
    "            for version_meta in sorted_model_versions:\n",
    "                for env_tag in tags_detected:\n",
    "\n",
    "                    if (incoming_tags[env_tag] == \"yes\") and (\n",
    "                        version_meta.tags.get(env_tag) == \"yes\"\n",
    "                    ):\n",
    "                        client.set_model_version_tag(\n",
    "                            name=self.get_model_name(),\n",
    "                            version=str(version_meta.version),\n",
    "                            key=env_tag,\n",
    "                            value=\"no\",\n",
    "                        )\n",
    "\n",
    "    def log_models(self) -> None:\n",
    "        \"\"\"Carries out the mlflow model registry procedures\"\"\"\n",
    "        print(\"Starting model logging\")\n",
    "        self.save_model()\n",
    "\n",
    "        modelpath = self.get_model_log_path()\n",
    "        print(\"Logging model\")\n",
    "        mlflow.pyfunc.log_model(\n",
    "            artifact_path=self.get_model_log_path(),\n",
    "            python_model=ModelWrapperMlflowModel(),\n",
    "            artifacts=self.artifacts,\n",
    "            conda_env=conda_env,\n",
    "        )\n",
    "\n",
    "        # enforcing lower case for env based string keys and values\n",
    "        decap_meta_data = {}\n",
    "        for key, value in self.meta_data.items():\n",
    "            env_str_match = re.findall(pattern=f\"({'|'.join(self.envs)})\", string=key)\n",
    "            if len(env_str_match) > 0:\n",
    "                decap_meta_data[key.lower()] = (\n",
    "                    value.lower() if isinstance(value, str) else value\n",
    "                )\n",
    "            else:\n",
    "                decap_meta_data[key] = value\n",
    "\n",
    "        self.meta_data = decap_meta_data\n",
    "\n",
    "        self.change_current_env_tags(self.meta_data)\n",
    "\n",
    "        print(\"Registering model\")\n",
    "        result = mlflow.register_model(\n",
    "            self.get_model_register_path(),\n",
    "            self.get_model_name(),\n",
    "            tags=self.meta_data,\n",
    "        )\n",
    "\n",
    "    def clean(self) -> None:\n",
    "        if os.path.exists(self.local_root):\n",
    "            shutil.rmtree(self.local_root)\n",
    "\n",
    "    def load_pyfunc_model(\n",
    "        self, dst_path: Optional[str] = None, tag: Optional[Union[str, int]] = None\n",
    "    ) -> mlflow.pyfunc.PyFuncModel:\n",
    "        \"\"\"Load and return the pyfunc model from the MLFlow model repository\n",
    "\n",
    "        Args:\n",
    "            dst_path (str, optional): Destination path to save the loaded model.\n",
    "                                      If not provided the files will be saved in the local_root path.\n",
    "                                      Defaults to None.\n",
    "            tag (str, optional): Tag to specify the version or model stage to be loaded.\n",
    "                                 If not provided the latest model version will be loaded.\n",
    "                                Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            mlflow.pyfunc.model : pyfunc model\n",
    "        \"\"\"\n",
    "        # self.local_dir = dst_path\n",
    "        print(f\"Loading model {self.get_model_uri()}\")\n",
    "\n",
    "        if dst_path is not None:\n",
    "            self.local_root = dst_path\n",
    "\n",
    "        if os.path.exists(self.local_root):\n",
    "            self.clean()\n",
    "\n",
    "        os.mkdir(self.local_root)\n",
    "\n",
    "        model = mlflow.pyfunc.load_model(\n",
    "            self.get_model_uri(tag=tag), dst_path=self.local_root\n",
    "        )\n",
    "\n",
    "        self.run_id = model._model_meta.run_id\n",
    "\n",
    "        # following is a bit of a round about way to set local_dir\n",
    "        # having the run id in the directory name is a bit troublesome as the run id is not available to us when we create the autogluon object\n",
    "        # TODO make sure to remove the run id from the local_dir and include either or both task_type/exclude_pms\n",
    "        # TODO make sure to set the local_dir consistently for both training and inference tasks\n",
    "        # self.local_dir = \"/ag_models/\"\n",
    "\n",
    "        # if self.exclude_pms:\n",
    "        #     self.local_dir = f\"ag_models_{self.hotel_id}_{self.run_id}/\"\n",
    "\n",
    "        # os.rename(\"artifacts\",self.local_dir)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def get_filtered_data(\n",
    "        self, data: pd.DataFrame, day_ahead: int\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "        target_columns = self.all_cd_cols[:day_ahead]\n",
    "        target_columns = list(\n",
    "            map(lambda target: target + self.target_suffix, target_columns)\n",
    "        )\n",
    "\n",
    "        # original target order\n",
    "        target_columns_orig = target_columns.copy()\n",
    "        target_columns.reverse()\n",
    "\n",
    "        cd_axis_lag_columns = self.all_cd_cols[day_ahead:]\n",
    "        if self.n_cd_lags != None:\n",
    "            cd_axis_lag_columns = cd_axis_lag_columns[: self.n_cd_lags]\n",
    "\n",
    "        sd_axis_lag_columns = [\n",
    "            f\"{self.sd_axis_lag_prefix}{SD_lag}\"\n",
    "            for SD_lag in self.lag_numbers\n",
    "            if SD_lag > day_ahead\n",
    "        ]\n",
    "\n",
    "        # assigning target and feature variables corresponding to the particular day ahead. This will be retrieved through the attributes in the inference phase.\n",
    "        self.target_cols[day_ahead] = target_columns\n",
    "        self.feature_cols[day_ahead] = (\n",
    "            cd_axis_lag_columns + sd_axis_lag_columns + self.static_cols\n",
    "        )\n",
    "\n",
    "        if self.is_ca3_training:\n",
    "            # Condition helps us get the specific entry for the cancellation day index\n",
    "            condition = data[\"forecast_index\"] == (day_ahead - 1)\n",
    "            filt_data = data[condition].copy()\n",
    "        else:\n",
    "            filt_data = data.copy()\n",
    "\n",
    "        x_data = filt_data[self.feature_cols[day_ahead]]\n",
    "        y_data = filt_data[self.target_cols[day_ahead]]\n",
    "\n",
    "        return (\n",
    "            x_data,\n",
    "            y_data,\n",
    "            filt_data[target_columns_orig + self.feature_cols[day_ahead]],\n",
    "        )\n",
    "\n",
    "    def train_inner(self, train_data: pd.DataFrame, day_ahead: int):\n",
    "        x_train, y_train, xy_train = self.get_filtered_data(\n",
    "            data=train_data, day_ahead=day_ahead\n",
    "        )\n",
    "\n",
    "        reg_obj = self.model_strategy(\n",
    "            quantile_levels=self.quantile_levels,\n",
    "            day_ahead=day_ahead,\n",
    "            cd_axis_targets=self.target_cols[day_ahead],\n",
    "            path=self.local_dir,\n",
    "            is_auto_reg=self.is_auto_reg,\n",
    "        )  # type: ignore\n",
    "        reg_obj._fit(xy_train)\n",
    "\n",
    "        # self.models[day_ahead] = reg_obj\n",
    "\n",
    "        return day_ahead, reg_obj\n",
    "    \n",
    "    def predict_inner(self, train_data: pd.DataFrame, day_ahead: int):\n",
    "        test_idx = day_ahead - 1\n",
    "\n",
    "        try:\n",
    "            needed_test_data = test_data[test_data.day_ahead == day_ahead].iloc[0]\n",
    "        except IndexError as e:\n",
    "            days_str = \"days\" if day_ahead > 1 else \"day\"\n",
    "\n",
    "            error_msg = f\"Error when predicting {day_ahead} {days_str} ahead: {e}\"\n",
    "            raise ValueError(error_msg)\n",
    "\n",
    "        x_test = needed_test_data[self.feature_cols[day_ahead]]\n",
    "        y_test = needed_test_data[self.target_cols[day_ahead]]\n",
    "\n",
    "        predictor = self.models[day_ahead]\n",
    "        y_pred_dct = predictor._predict(x_test)\n",
    "\n",
    "        return y_pred_dct\n",
    "\n",
    "    def train(self, train_data: pd.DataFrame, n_threads: int) -> None:\n",
    "        \"\"\"\n",
    "        trains models for each day ahead quantile predictions and  relevant\n",
    "        to the specified prediction_horizon value and the specific quantile\n",
    "        levels.\n",
    "\n",
    "        parameters:\n",
    "            train_data = training data with the booking pace lags, stay date lags or\n",
    "                    other features such as date features.\n",
    "\n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        with ThreadPoolExecutor(max_workers=n_threads) as executor:\n",
    "            future_to_target = {executor.submit(\n",
    "                self.train_inner, train_data, day_ahead): day_ahead for day_ahead in range(1, self.prediction_horizon + 1)}\n",
    "            \n",
    "            for future in as_completed(future_to_target):\n",
    "                try:\n",
    "                    day_ahead, reg_obj = future.result()\n",
    "                except Exception as exc:\n",
    "                    print(exc)\n",
    "                else:\n",
    "                    self.models[day_ahead] = reg_obj\n",
    "\n",
    "\n",
    "        # for day_ahead in range(1, self.prediction_horizon + 1):\n",
    "        #     print(\"\\tday ahead: \", day_ahead)\n",
    "        #     x_train, y_train, xy_train = self.get_filtered_data(\n",
    "        #         data=train_data, day_ahead=day_ahead\n",
    "        #     )\n",
    "\n",
    "        #     reg_obj = self.model_strategy(\n",
    "        #         quantile_levels=self.quantile_levels,\n",
    "        #         day_ahead=day_ahead,\n",
    "        #         cd_axis_targets=self.target_cols[day_ahead],\n",
    "        #         path=self.local_dir,\n",
    "        #         is_auto_reg=self.is_auto_reg,\n",
    "        #     )  # type: ignore\n",
    "        #     reg_obj._fit(xy_train)\n",
    "\n",
    "        #     self.models[day_ahead] = reg_obj\n",
    "\n",
    "        # if self.do_save_models:\n",
    "        #     self.log_models()\n",
    "\n",
    "    def predict(\n",
    "        self, test_data: pd.DataFrame\n",
    "    ) -> Dict[Union[str, float], Union[List[npt.NDArray], List[pd.Series]]]:\n",
    "        \"\"\"generating quantile predictions for the test data provided. test_data\n",
    "        should be provided which aligns with the prediction_horizon. If\n",
    "        test_data has less rows than the prediction_horizon, then the length\n",
    "        of the test_data will be considered as the prediction horizon.\n",
    "\n",
    "        eg: if prediction_horizon= 28, ideally test_data should have 28 rows\n",
    "            which are relevant for 28 stay dates.\n",
    "\n",
    "        parameters:\n",
    "            test_data = test data which aligns with the prediction horizon.\n",
    "                        rows of test_data <= prediction_horizon.\n",
    "\n",
    "        Returns: Lists with actual values and corresponding predicted values along the booking axis leading upto the\n",
    "          relevant stay date ahead\n",
    "        \"\"\"\n",
    "        output_pred: Dict[\n",
    "            Union[str, float], Union[List[npt.NDArray], List[pd.Series]]\n",
    "        ] = {}\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=n_threads) as executor:\n",
    "            future_to_target = {executor.submit(\n",
    "                self.predict_inner, test_data, day_ahead): day_ahead for day_ahead in range(1, self.prediction_horizon + 1)}\n",
    "            \n",
    "            for future in as_completed(future_to_target):\n",
    "                try:\n",
    "                    y_pred_dct = future.result()\n",
    "                except Exception as exc:\n",
    "                    print(exc)\n",
    "                else:\n",
    "                    # self.models[day_ahead] = reg_obj\n",
    "                    if output_pred.get(\"y_test\") == None:\n",
    "                        output_pred[\"y_test\"] = [y_test]\n",
    "                    else:\n",
    "                        output_pred[\"y_test\"] += [y_test]\n",
    "\n",
    "                    for qtile in self.quantile_levels:\n",
    "                        if output_pred.get(qtile) == None:\n",
    "                            output_pred[qtile] = [y_pred_dct[qtile][0]]\n",
    "                        else:\n",
    "                            output_pred[qtile] += [y_pred_dct[qtile][0]]\n",
    "\n",
    "        return output_pred\n",
    "\n",
    "        # for day_ahead in range(1, self.prediction_horizon + 1):\n",
    "        #     test_idx = day_ahead - 1\n",
    "\n",
    "        #     try:\n",
    "        #         needed_test_data = test_data[test_data.day_ahead == day_ahead].iloc[0]\n",
    "        #     except IndexError as e:\n",
    "        #         days_str = \"days\" if day_ahead > 1 else \"day\"\n",
    "\n",
    "        #         print(f\"Error when predicting {day_ahead} {days_str} ahead\")\n",
    "        #         print(f\"Encountered error {e}\")\n",
    "        #         print(\"Skipping this row\")\n",
    "        #         continue\n",
    "\n",
    "        #     x_test = needed_test_data[self.feature_cols[day_ahead]]\n",
    "        #     y_test = needed_test_data[self.target_cols[day_ahead]]\n",
    "\n",
    "        #     predictor = self.models[day_ahead]\n",
    "        #     y_pred_dct = predictor._predict(x_test)\n",
    "\n",
    "        #     if output_pred.get(\"y_test\") == None:\n",
    "        #         output_pred[\"y_test\"] = [y_test]\n",
    "        #     else:\n",
    "        #         output_pred[\"y_test\"] += [y_test]\n",
    "\n",
    "        #     for qtile in self.quantile_levels:\n",
    "        #         if output_pred.get(qtile) == None:\n",
    "        #             output_pred[qtile] = [y_pred_dct[qtile][0]]\n",
    "        #         else:\n",
    "        #             output_pred[qtile] += [y_pred_dct[qtile][0]]\n",
    "\n",
    "        # return output_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07acac0a-4000-4232-8185-65294d99d27c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Disable adaptive query optimization\n",
    "# Adaptive query optimization groups together smaller tasks into a larger tasks.\n",
    "# This may result in limited parallelism if the parallel inference tasks are deemed to be too small by the query optimizer\n",
    "# We are diableing AQE here to circumevent this limitation on parallelism\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5be9323b-708d-47c7-9a79-0a625f6af830",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "REVENUE_COL = \"_reservationRevenuePerRoomUSD\"\n",
    "ROOMS_COL = \"_rooms\"\n",
    "PIPELINE = \"INFERENCE\"\n",
    "\n",
    "WITHOUT_PMS = str_to_bool(getArgument(\"exclude_pms\"))\n",
    "IS_USD_CURRENCY = str_to_bool(getArgument(\"is_usd_currency\"))\n",
    "TARGET_TYPE = getArgument(\"target_type\")\n",
    "selected_hotels = str_to_lst(getArgument(\"selected_hotels\"))\n",
    "LAG_NUMBERS = list(map(int,str_to_lst(getArgument('lag_numbers'))))\n",
    "\n",
    "### The start of the model data\n",
    "MODEL_START_DATE = pd.to_datetime(\"2018-10-01\")\n",
    "COVID_START_DATE = pd.to_datetime(\"2020-03-01\")\n",
    "COVID_END_DATE = pd.to_datetime(\"2021-08-01\")\n",
    "\n",
    "CALC_UNCERTAINTY = False\n",
    "# MODEL_TYPE = \"XGB\"  # Use \"AG\" to try out the auto gloun approach\n",
    "MODEL_TYPE = \"AG\"\n",
    "\n",
    "LEAD_WINDOW = 60\n",
    "\n",
    "ML_EXPERIMENT_ID = 1079527465953184\n",
    "\n",
    "if MODEL_TYPE == \"XGB\":\n",
    "    RUN_ID = \"92907cac187f4c8cadb63ff60a05d72e\"  # XGB Run\n",
    "elif CALC_UNCERTAINTY and (MODEL_TYPE == \"AG\"):\n",
    "    RUN_ID = \"9549361574484dc58fcf1b7d130541a0\"\n",
    "else:\n",
    "    RUN_ID = \"19dee6420aed45f29e956016c5ea6e8a\"\n",
    "\n",
    "\n",
    "lead_window_start_days = 14\n",
    "lead_window_end_days = 60\n",
    "prediction_horizon = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d707ac6-578b-4078-be83-7f0ee85fc8fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env_config = EnvironmentConfig(env=ENV, target=TARGET_TYPE, spark=spark, is_usd_currency=IS_USD_CURRENCY)\n",
    "forecasting_config_provider = ForecastingHotelConfigProvider(spark=spark,env=ENV)\n",
    "target_column = env_config.target_column\n",
    "schema = inference_output_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ee033c4-dca5-4987-845a-44648f1852f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">2024-10-07\n",
       "2024-11-04\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">2024-10-07\n2024-11-04\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# As a workaround for the bug PHG-2157\n",
    "PARTITION_DATE = spark.sql(\n",
    "    f\"select max(confirmationDate) from {env_config.source_data_table}\"\n",
    ").collect()[0][0]\n",
    "print(PARTITION_DATE)\n",
    "\n",
    "max_inference_length = spark.sql(f'select max(inference_prediction_length) from {forecasting_config_provider.config_table_name}').collect()[0][0]\n",
    "TEST_PARTIITON_END = PARTITION_DATE + pd.Timedelta(max_inference_length, \"D\")\n",
    "print(TEST_PARTIITON_END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "393437d7-ed06-41ba-bc65-d37283901994",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PARTITION_DATE = pd.to_datetime(\"2024-10-07\")\n",
    "# TEST_PARTIITON_END = pd.to_datetime(\"2024-11-04\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6746c673-c1d9-4d51-b5fd-0c6cc7c2d479",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger = get_dbx_logger(pipeline=PIPELINE,\n",
    "                        task_type=TARGET_TYPE,\n",
    "                        exclude_pms=WITHOUT_PMS)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26b666df-4ba3-41fd-b2ae-b1f11e4f7913",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def pyfunc_load_model_retry(model_uri, max_tries):\n",
    "    '''Retry mechanism for loading models from mlflow model registry to \n",
    "    handle the model loading error\n",
    "    '''\n",
    "    loop_len = max_tries+1\n",
    "    for i in range(loop_len):\n",
    "            try:\n",
    "                return mlflow.pyfunc.load_model(model_uri)\n",
    "            except Exception as e:\n",
    "                if i+1==loop_len:\n",
    "                    raise e\n",
    "                else:\n",
    "                    print(e)\n",
    "                    print(f'Retrying: attempt {i+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a93c7e83-8fe7-4388-9d76-613d901d3c8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def correct_prediction_list(y_med, y_test, y_upper, y_lower, target, available_rooms):\n",
    "    y_med_lst = []\n",
    "    y_upper_lst = []\n",
    "    y_lower_lst = []\n",
    "\n",
    "    for i, vals in enumerate(zip(y_med, y_test, y_upper, y_lower)):\n",
    "        (\n",
    "            y_med_corrected,\n",
    "            y_test_corrected,\n",
    "            y_upper_corrected,\n",
    "            y_lower_corrected,\n",
    "        ) = post_process_prediction(\n",
    "            y_med=vals[0],\n",
    "            y_test=vals[1],\n",
    "            y_upper=vals[2],\n",
    "            y_lower=vals[3],\n",
    "            target=target,\n",
    "            available_rooms=available_rooms,\n",
    "        )\n",
    "\n",
    "        y_med_lst.append(y_med_corrected)\n",
    "        y_upper_lst.append(y_upper_corrected)\n",
    "        y_lower_lst.append(y_lower_corrected)\n",
    "\n",
    "    return y_med_lst, y_upper_lst, y_lower_lst\n",
    "\n",
    "\n",
    "def post_process_prediction(y_med, y_test, y_upper, y_lower, target, available_rooms):\n",
    "    # This step corrects the dipping issue\n",
    "    (\n",
    "        y_med_corrected,\n",
    "        y_test_corrected,\n",
    "        y_upper_corrected,\n",
    "        y_lower_corrected,\n",
    "    ) = correct_dipping(y_med=y_med, y_test=y_test, y_upper=y_upper, y_lower=y_lower)\n",
    "\n",
    "    if target == \"ROOMS\":\n",
    "        # If the target type is ROOMS we can try to correct the max rooms capping issue\n",
    "        if available_rooms is None:\n",
    "            raise ValueError(\"The argument available_rooms must be provided\")\n",
    "\n",
    "        (\n",
    "            y_med_corrected,\n",
    "            y_test_corrected,\n",
    "            y_upper_corrected,\n",
    "            y_lower_corrected,\n",
    "        ) = correct_capping(\n",
    "            y_med=y_med_corrected,\n",
    "            y_test=y_test_corrected,\n",
    "            y_upper=y_upper_corrected,\n",
    "            y_lower=y_lower_corrected,\n",
    "            available_rooms=available_rooms,\n",
    "        )\n",
    "\n",
    "    return y_med_corrected, y_test, y_upper_corrected, y_lower_corrected\n",
    "\n",
    "\n",
    "def correct_capping(y_med, y_test, y_upper, y_lower, available_rooms):\n",
    "    \"\"\"\n",
    "    Transforms forecast predictions to be below the available_rooms.\n",
    "\n",
    "    Args:\n",
    "        y_med : raw mean/median predictions\n",
    "        y_test: actual values\n",
    "        y_upper : raw upper quantile predictions\n",
    "        y_lower : raw lower quantile predictions\n",
    "        available_rooms: available number of rooms for a the specific hotel\n",
    "\n",
    "    Returns corrected predicted values.\n",
    "    \"\"\"\n",
    "\n",
    "    y_med_corrected = np.where(y_med > available_rooms, available_rooms, y_med)\n",
    "    y_upper_corrected = np.where(y_upper > available_rooms, available_rooms, y_upper)\n",
    "    y_lower_corrected = np.where(y_lower > available_rooms, available_rooms, y_lower)\n",
    "\n",
    "    return y_med_corrected, y_test, y_upper_corrected, y_lower_corrected\n",
    "\n",
    "\n",
    "def correct_dipping(y_med, y_test, y_upper, y_lower):\n",
    "    \"\"\"This returns the adjusted predicted values such that,\n",
    "    1) median/mean predictions have a strict cumulative nature\n",
    "    2) upper and lower quantile predictions are restricted to be on either\n",
    "    side of the median/mean predictions.\n",
    "\n",
    "    Args:\n",
    "        y_med : raw mean/median predictions\n",
    "        y_test: actual values\n",
    "        y_upper : raw upper quantile predictions\n",
    "        y_lower : raw lower quantile predictions\n",
    "\n",
    "    Returns corrected predicted values.\n",
    "    \"\"\"\n",
    "    last_val = y_med[0]\n",
    "\n",
    "    # correcting predictions if they are lower than the last known actual value\n",
    "    y_med_corrected = np.where(y_med < last_val, last_val, y_med)\n",
    "    y_lower_corrected = y_lower #np.where(y_lower < last_val, last_val, y_lower)\n",
    "\n",
    "    delta = y_med_corrected - np.abs(y_med)\n",
    "\n",
    "    y_upper_corrected = y_upper + delta\n",
    "\n",
    "    # further correction of predictions along the booking axis to have the cumulative nature\n",
    "    for index in range(len(y_med)):\n",
    "        # adjusting median/mean predictions\n",
    "        if (y_med_corrected[index] < y_med_corrected[index - 1]) and (index > 0):\n",
    "            y_med_corrected[index] = y_med_corrected[index - 1]\n",
    "\n",
    "        # adjusting upper quantile predictions\n",
    "        if y_med_corrected[index] > y_upper_corrected[index]:\n",
    "            y_upper_corrected[index] = y_med_corrected[index]\n",
    "\n",
    "        # adjusting lower quantile predictions\n",
    "        if y_med_corrected[index] < y_lower_corrected[index]:\n",
    "            y_lower_corrected[index] = y_med_corrected[index]\n",
    "\n",
    "    return y_med_corrected, y_test, y_upper_corrected, y_lower_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "392ac16e-e8cc-4f19-ad30-0d53c37c6a07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prediction_wrapper(\n",
    "    target_type, run_id, exclude_pms,hotel_config_provider,model_cache_dir,environment\n",
    "):\n",
    "    def predict_distributed(data):\n",
    "        static_cols_ = ['year', 'quarter_of_year', 'month_of_year', 'week_of_year',\n",
    "                         'day_of_year', 'month_of_quarter', 'week_of_quarter', 'day_of_quarter',\n",
    "                           'week_of_month', 'day_of_month', 'holiday',\n",
    "                             'day_of_week_0', 'day_of_week_1', 'day_of_week_2', \n",
    "                             'day_of_week_3', 'day_of_week_4', 'day_of_week_5', 'day_of_week_6']\n",
    "\n",
    "        logger = get_dbx_logger(\"PHGML\")\n",
    "        \n",
    "        max_lead_window = 100\n",
    "        \n",
    "        hotel_id = data[\"HotelID\"].iloc[0]\n",
    "        hotel_config = hotel_config_provider.get_config(hotel_id)\n",
    "        model_type = hotel_config.inference_model_name\n",
    "\n",
    "        print(f\"Processing Hotel {hotel_id}\")\n",
    "        \n",
    "        if target_type == \"REVENUE\":\n",
    "            col_prefix = \"RV\"\n",
    "\n",
    "            if hotel_config.forecast_currency is None:\n",
    "                # If the target type is REVENUE, we should have a defined forecast_currency\n",
    "                raise ValueError(f\"Forecast currency cannot be None for target_type {target_type}\")\n",
    "            \n",
    "        elif target_type == \"ROOMS\":\n",
    "            col_prefix = \"RM\"\n",
    "        \n",
    "        data = remove_padded_cols(data,hotel_config.lead_window,max_lead_window,col_prefix)\n",
    "        \n",
    "        model_version = 1\n",
    "        model_stage = \"Staging\"\n",
    "        model_name = None\n",
    "\n",
    "        try:\n",
    "\n",
    "            if model_type == \"LIGHTGBM\":\n",
    "\n",
    "                model_obj = ModelWrapper(\n",
    "                                model_strategy=StrategyLGBM,\n",
    "                                prediction_horizon=hotel_config.inference_length,\n",
    "                                hotel_id=hotel_id,\n",
    "                                target_type=target_type,\n",
    "                                exclude_pms=exclude_pms,\n",
    "                                cd_axis_max_lags=99, \n",
    "                                static_cols =static_cols_,)\n",
    "                \n",
    "                model_obj.set_latest_model_version(model_stage = environment)\n",
    "\n",
    "                loaded_model = pyfunc_load_model_retry(model_obj.get_model_uri(), 6)\n",
    "                \n",
    "                loaded_model.unwrap_python_model().model_wrapper_model.prediction_horizon = hotel_config.inference_length\n",
    "                #during training time, the target variables are suffixed as '_tgt' to differentiate between target booking pace values and feature booking pace values. but while doing daily inferences,\n",
    "                # that distinction doesnt matter since we dont have the true values anyway, hence overriding the the target columns as below to avoid columns being not detected.\n",
    "                loaded_model.unwrap_python_model().model_wrapper_model.target_cols = {day_ahead:[ f\"{col_prefix}{j}\" for j in range(day_ahead)] for day_ahead in range(1,hotel_config.inference_length+1)}\n",
    "                    \n",
    "            elif model_type == \"AUTOGLUON\":\n",
    "\n",
    "                model_obj = ModelWrapper(\n",
    "                                model_strategy=StrategyAG,\n",
    "                                is_auto_reg=True,\n",
    "                                prediction_horizon=hotel_config.inference_length,\n",
    "                                hotel_id=hotel_id,\n",
    "                                target_type=target_type,\n",
    "                                exclude_pms=exclude_pms,\n",
    "                                cd_axis_max_lags=99, \n",
    "                                static_cols =static_cols_,)\n",
    "\n",
    "\n",
    "                model_obj.set_latest_model_version()\n",
    "            \n",
    "                pms = \"PMS\"\n",
    "                if exclude_pms:\n",
    "                    pms = \"NOPMS\"\n",
    "\n",
    "                #dbfs_dir = f\"/dbfs/mnt/models/forecasting/individual_hotels/{hotel_id}_{target_type}_{pms}/\"\n",
    "                dbfs_dir = f\"{model_cache_dir}{hotel_id}_{target_type}_{pms}\" \n",
    "                #f\"/dbfs/mnt/models/forecasting/dev_individual_hotels/{hotel_id}_{target_type}_{pms}/\"\n",
    "                local_dir = model_obj.local_root\n",
    "\n",
    "                if os.path.exists(local_dir):\n",
    "                    shutil.rmtree(local_dir)\n",
    "\n",
    "                # Copy cached model from blob storage to local dir\n",
    "                \n",
    "                shutil.copytree(dbfs_dir, local_dir)\n",
    "\n",
    "                # load model\n",
    "                loaded_model = load_pkl.load(path=model_obj.local_path)\n",
    "                loaded_model.prediction_horizon = model_obj.prediction_horizon\n",
    "\n",
    "            model_version = int(model_obj.version)\n",
    "            model_name = [\n",
    "                model_obj.get_model_name()\n",
    "                for step in range(1, hotel_config.inference_length + 1)\n",
    "            ]\n",
    "            model_metadata = model_obj.get_remote_model_metadata()\n",
    "            logger.info(\"Using model version {model_version}\")\n",
    "\n",
    "            logger.info(f\"Inference length of model: {model_metadata.get('inference_length','NOT_FOUND')}\")\n",
    "            logger.info(f\"Last trained date: {model_metadata.get('last_trained_date','NOT_FOUND')}\")           \n",
    "\n",
    "            output_dct = loaded_model.predict(data)\n",
    "            y_pred_raw, y_test, y_upper_raw, y_lower_raw = output_dct[0.5], output_dct['y_test'], output_dct[0.9], output_dct[0.1]\n",
    "\n",
    "            y_pred_interpolated = [interpolated_fill(day_ahead_array) for day_ahead_array in y_pred_raw]\n",
    "            \n",
    "            y_pred, y_upper, y_lower = correct_prediction_list(\n",
    "                y_pred_interpolated, y_test, y_upper_raw, y_lower_raw,target_type,available_rooms = hotel_config.available_rooms\n",
    "            )\n",
    "\n",
    "            data[\"status\"] = \"complete\"\n",
    "            data[\"message\"] = f\"Successfully processed {hotel_id}\"\n",
    "\n",
    "            output_df = get_output_df(\n",
    "                y_pred=y_pred,\n",
    "                y_true=y_test,\n",
    "                run_id=run_id,\n",
    "                hotel_id=hotel_id,\n",
    "                data=data.sort_values('day_ahead'),\n",
    "                model_name=model_name,\n",
    "                model_version=model_version,\n",
    "                pms_sync_off=exclude_pms,\n",
    "                forecast_currency=hotel_config.forecast_currency,\n",
    "                prediction_horizon=hotel_config.inference_length,\n",
    "                y_upper=y_upper,\n",
    "                y_lower=y_lower,\n",
    "                y_med_raw=y_pred_raw,\n",
    "                y_upper_raw=y_upper_raw,\n",
    "                y_lower_raw=y_lower_raw,\n",
    "            )\n",
    "\n",
    "            output_df[\"status\"] = \"complete\"\n",
    "            output_df[\"message\"] = f\"Successfully processed {hotel_id}\"\n",
    "\n",
    "        except MlflowException as e:\n",
    "            if \"RESOURCE_DOES_NOT_EXIST\" in e.message:\n",
    "                print(\n",
    "                        f\"Model {model_obj.get_model_name()} was not  found in the model registry. Skipping this model...\"\n",
    "                    )\n",
    "            else:\n",
    "                print(\"An MLFlowException occured\")\n",
    "                print(e)\n",
    "\n",
    "            empty = pd.DataFrame(\n",
    "                {\n",
    "                    \"HotelID\": [hotel_id],\n",
    "                    \"run_id\": [run_id],\n",
    "                    \"stay_date\": [pd.Timestamp(\"1900-01-01\")],\n",
    "                    \"booking_date\": [pd.Timestamp(\"1900-01-01\")],\n",
    "                    \"model_version\": [0],\n",
    "                    \"timestamp\": [pd.Timestamp(\"1900-01-01\")],\n",
    "                    \"pms_sync_off\": [exclude_pms],\n",
    "                    \"forecast_currency\":[hotel_config.forecast_currency],\n",
    "                    \"day_index\": [0],\n",
    "                    \"y_med\": [0],\n",
    "                    \"model_name\": [\"\"],\n",
    "                    \"y_upper\": [0],\n",
    "                    \"y_lower\": [0],\n",
    "                    \"y_med_raw\": [0],\n",
    "                    \"y_upper_raw\": [0],\n",
    "                    \"y_lower_raw\": [0],\n",
    "                    \"status\": \"incomplete\",\n",
    "                    \"message\": e.message,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            return empty\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Hotel {hotel_id} encountered an error \")\n",
    "            raise e\n",
    "        finally:\n",
    "            if model_type == \"AUTOGLUON\":\n",
    "                model_obj.clean()\n",
    "\n",
    "        return output_df\n",
    "\n",
    "    return predict_distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd4e379e-9deb-44f9-aba6-2828ed3837ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env_config.preprocess_intermediate_table = \"test_preprocess_intermediate_table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7f11cb8-635f-42c2-beb0-cfa98e216874",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">24/10/10/ 18:15:41 UTC:PHGML-INFERENCE-REVENUE-PMS-INFO-Read preprocessing data\n",
       "Read preprocessing data\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">24/10/10/ 18:15:41 UTC:PHGML-INFERENCE-REVENUE-PMS-INFO-Read preprocessing data\nRead preprocessing data\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger.info(\"Read preprocessing data\")\n",
    "df = spark.sql(\n",
    "    f\"select * from {env_config.preprocess_intermediate_table}\"\n",
    ").withColumn(\"status\", lit(\"incomplete\"))\n",
    "\n",
    "# df = df.filter(df.HotelID=='63662')\n",
    "df = df.withColumn(\"_StayDates\", to_timestamp(\"_StayDates\", \"yyyy-MM-dd\")).orderBy([\"HotelID\", \"_StayDates\"])\n",
    "\n",
    "df = df.withColumn('partition_date', lit(str(PARTITION_DATE)))\n",
    "df = df.withColumn(\"day_ahead\", datediff(col(\"_StayDates\"), to_timestamp('partition_date', \"yyyy-MM-dd\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e6bc518-b638-4e62-b4de-241a909b1864",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DEBUG\n",
    "# output = debug_prediction(df,MODEL_TYPE, TARGET_TYPE, ML_EXPERIMENT_ID, RUN_ID, WITHOUT_PMS, CALC_UNCERTAINTY,forecasting_config_provider,model_cache_dir=env_config.model_cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5aade37-19d4-43c1-8053-d7e120c7bd65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">24/10/10/ 18:15:41 UTC:PHGML-INFERENCE-REVENUE-PMS-INFO-Starting parallell processing\n",
       "Starting parallell processing\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">24/10/10/ 18:15:41 UTC:PHGML-INFERENCE-REVENUE-PMS-INFO-Starting parallell processing\nStarting parallell processing\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Group the data by hotel id and execute the inferences in parallel\n",
    "logger.info(\"Starting parallell processing\")\n",
    "output_df = df.groupby(\"HotelID\").applyInPandas(\n",
    "    prediction_wrapper(\n",
    "        target_type=TARGET_TYPE, \n",
    "        run_id=RUN_ID, \n",
    "        exclude_pms=WITHOUT_PMS, \n",
    "        hotel_config_provider=forecasting_config_provider,\n",
    "        model_cache_dir=env_config.model_cache_dir,\n",
    "        environment=ENV\n",
    "    ),\n",
    "    schema,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed7d276f-7476-4ec5-b1b1-407f4f1ade8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# logger.info(\"Drop intermediate results table if it exists\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {env_config.inference_intermediate_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e8c4bcd-915f-4807-97ee-3819a5450ab1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env_config.inference_intermediate_table = \"test_inference_intermediate_table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80eb5498-147f-41da-b043-d94178e59cf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">24/10/10/ 18:15:41 UTC:PHGML-INFERENCE-REVENUE-PMS-INFO-Writing inference results to temporary table test_inference_intermediate_table\n",
       "Writing inference results to temporary table test_inference_intermediate_table\n",
       "24/10/10/ 18:19:23 UTC:PHGML-INFERENCE-REVENUE-PMS-INFO-Time elapsed 222.3319234129999\n",
       "Time elapsed 222.3319234129999\n",
       "24/10/10/ 18:19:23 UTC:PHGML-INFERENCE-REVENUE-PMS-INFO-Time elapsed in minutes 3.705532056883332\n",
       "Time elapsed in minutes 3.705532056883332\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">24/10/10/ 18:15:41 UTC:PHGML-INFERENCE-REVENUE-PMS-INFO-Writing inference results to temporary table test_inference_intermediate_table\nWriting inference results to temporary table test_inference_intermediate_table\n24/10/10/ 18:19:23 UTC:PHGML-INFERENCE-REVENUE-PMS-INFO-Time elapsed 222.3319234129999\nTime elapsed 222.3319234129999\n24/10/10/ 18:19:23 UTC:PHGML-INFERENCE-REVENUE-PMS-INFO-Time elapsed in minutes 3.705532056883332\nTime elapsed in minutes 3.705532056883332\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger.info(\n",
    "    f\"Writing inference results to temporary table {env_config.inference_intermediate_table}\"\n",
    ")\n",
    "start_time_temp = time.perf_counter()\n",
    "(\n",
    "    output_df.write.mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(env_config.inference_intermediate_table)\n",
    ")\n",
    "elapsed_time_temp = time.perf_counter() - start_time_temp\n",
    "logger.info(f\"Time elapsed {elapsed_time_temp}\")\n",
    "logger.info(f\"Time elapsed in minutes {elapsed_time_temp/60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e778322-6569-414e-a48f-379500537368",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">24/10/10/ 18:19:29 UTC:PHGML-INFERENCE-REVENUE-PMS-INFO-43 out of 43 hotels processed succussfully\n",
       "43 out of 43 hotels processed succussfully\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">24/10/10/ 18:19:29 UTC:PHGML-INFERENCE-REVENUE-PMS-INFO-43 out of 43 hotels processed succussfully\n43 out of 43 hotels processed succussfully\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "meta_columns = [\"HotelID\", \"run_id\", \"timestamp\", \"pms_sync_off\", \"status\", \"message\"]\n",
    "results_table = spark.sql(f\"select * from {env_config.inference_intermediate_table}\")\n",
    "output_meta = results_table.select(meta_columns).toPandas()\n",
    "\n",
    "num_completed = output_meta[output_meta[\"status\"] == \"complete\"][\"HotelID\"].nunique()\n",
    "total = output_meta[\"HotelID\"].nunique()\n",
    "logger.info(f\"{num_completed} out of {total} hotels processed succussfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7165f8c3-e2b7-4fce-b65a-5000188237b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "incomplete = output_meta[~(output_meta[\"status\"] == \"complete\")]\n",
    "\n",
    "for row in incomplete.itertuples():\n",
    "    logger.error(\n",
    "        f\"Error encountered when processing hotel {row.HotelID}: {row.message}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f690b047-e988-4d79-85c0-5a31da964165",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_df = results_table.filter(results_table.status == \"complete\").drop(\n",
    "    \"status\", \"message\"\n",
    ")\n",
    "\n",
    "#Drop forecast currency if TARGET_TYPE is ROOMS\n",
    "if TARGET_TYPE == \"ROOMS\":\n",
    "    output_df = output_df.drop(\"forecast_currency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "429bc7a9-70ca-4e16-9b5d-0629f6e2480d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env_config.inference_output_table = \"test_inference_output_table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ce7a835-9951-4468-bba7-b51fd51024bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">24/10/10/ 18:19:30 UTC:PHGML-INFERENCE-REVENUE-PMS-INFO-Writing completed results to table\n",
       "Writing completed results to table\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">24/10/10/ 18:19:30 UTC:PHGML-INFERENCE-REVENUE-PMS-INFO-Writing completed results to table\nWriting completed results to table\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger.info(\"Writing completed results to table\")\n",
    "file_format = \"delta\"\n",
    "\n",
    "(\n",
    "    output_df.write.format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .partitionBy(\"HotelID\")\n",
    "    # .option(\"path\", env_config.inference_output_table_blob)\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(env_config.inference_output_table)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85c6364a-a576-4b10-8316-0cc42361c14e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">24/10/10/ 18:19:36 UTC:PHGML-INFERENCE-REVENUE-PMS-INFO-Time elapsed 260.29532507999966\n",
       "Time elapsed 260.29532507999966\n",
       "24/10/10/ 18:19:36 UTC:PHGML-INFERENCE-REVENUE-PMS-INFO-Time elapsed in minutes 4.338255417999994\n",
       "Time elapsed in minutes 4.338255417999994\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">24/10/10/ 18:19:36 UTC:PHGML-INFERENCE-REVENUE-PMS-INFO-Time elapsed 260.29532507999966\nTime elapsed 260.29532507999966\n24/10/10/ 18:19:36 UTC:PHGML-INFERENCE-REVENUE-PMS-INFO-Time elapsed in minutes 4.338255417999994\nTime elapsed in minutes 4.338255417999994\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "elapsed_time = time.perf_counter() - start_time\n",
    "logger.info(f\"Time elapsed {elapsed_time}\")\n",
    "logger.info(f\"Time elapsed in minutes {elapsed_time/60}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "mod_forecast_inference_distributed_v2",
   "widgets": {
    "env_stage": {
     "currentValue": "dev",
     "nuid": "52458213-982d-4c83-9d32-c0fbccca2763",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dev",
      "label": "Pipeline stage",
      "name": "env_stage",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "dev",
        "prod"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "dev",
      "label": "Pipeline stage",
      "name": "env_stage",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "dev",
        "prod"
       ]
      }
     }
    },
    "exclude_pms": {
     "currentValue": "False",
     "nuid": "f9f908da-bea1-4591-a4a0-e6e50dde3cb6",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "False",
      "label": "Exclude PMS",
      "name": "exclude_pms",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "True",
        "False"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "False",
      "label": "Exclude PMS",
      "name": "exclude_pms",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "True",
        "False"
       ]
      }
     }
    },
    "is_usd_currency": {
     "currentValue": "True",
     "nuid": "7d5dc722-0b9e-4093-8205-7d3b592cb9d1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "True",
      "label": "Use USD currency",
      "name": "is_usd_currency",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "True",
        "False"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "True",
      "label": "Use USD currency",
      "name": "is_usd_currency",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "True",
        "False"
       ]
      }
     }
    },
    "lag_numbers": {
     "currentValue": "1,7,14,28",
     "nuid": "4ddfc541-da5d-4742-8d21-54aee71a8c80",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "1,7,14,28",
      "label": "Lag Numbers",
      "name": "lag_numbers",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "1,7,14,28",
      "label": "Lag Numbers",
      "name": "lag_numbers",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "selected_hotels": {
     "currentValue": "",
     "nuid": "8e69e311-6913-404f-9f85-8ebf9742afbd",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Hotels",
      "name": "selected_hotels",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Hotels",
      "name": "selected_hotels",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "target_type": {
     "currentValue": "REVENUE",
     "nuid": "96a2866e-a29a-4d94-9653-3322e4430b05",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "REVENUE",
      "label": "Target Type",
      "name": "target_type",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "REVENUE",
        "ROOMS"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "REVENUE",
      "label": "Target Type",
      "name": "target_type",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "REVENUE",
        "ROOMS"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}