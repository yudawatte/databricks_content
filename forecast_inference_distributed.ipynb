{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3fab728a-f2a8-4443-837a-291eecd22cad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9af78a23-6982-4d9f-8697-55e9a79c67aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.dropdown(\"env_stage\", \"dev\", [\"dev\", \"prod\", \"qa\",\"dev_synxis_2_0\", \"prod_synxis_2_0\", \"qa_synxis_2_0\"], \"Pipeline stage\")\n",
    "dbutils.widgets.dropdown(\"exclude_pms\", \"False\", [\"True\", \"False\"], \"Exclude PMS\")\n",
    "dbutils.widgets.dropdown(\"target_type\", \"REVENUE\", [\"REVENUE\", \"ROOMS\"], \"Target Type\")\n",
    "dbutils.widgets.dropdown(\"is_usd_currency\", \"True\", [\"True\", \"False\"], \"Use USD currency\")\n",
    "dbutils.widgets.text(\"selected_hotels\", \"\", \"Hotels\")\n",
    "dbutils.widgets.text(\"lag_numbers\",\"1,7,14,28\", \"Lag Numbers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed321aa1-88ba-4522-8920-18bb746d9d73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import os\n",
    "from autogluon.core.utils.loaders import load_pkl\n",
    "import logging\n",
    "import shutil\n",
    "import mlflow\n",
    "from mlflow import MlflowException\n",
    "import mlflow.pyfunc\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "098c1f61-cdb9-48fb-a264-6f83b412d22a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "params = {}\n",
    "params[\"ENV\"] = getArgument(\"env_stage\")\n",
    "params[\"CLUSTER_NAME\"] = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterName\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a7bd921-e890-40e8-ac23-fb13a79e01e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from phgml.models.model_wrapper import ModelWrapper\n",
    "from phgml.models.model_strategy import StrategyLGBM, StrategyAG\n",
    "from phgml.data.processing_distr_ca import remove_padded_cols\n",
    "from phgml.reporting.output_metrics import *\n",
    "from phgml.reporting.report_results import get_output_df, correct_prediction_list, interpolated_fill\n",
    "from phgml.data.data_types import inference_output_schema\n",
    "from phgml.reporting.logging import get_dbx_logger\n",
    "from phgml.data.config import ForecastingHotelConfigProvider,EnvironmentConfig\n",
    "from phgml.utilities.task_utilities import str_to_lst, str_to_bool, model_wrapper_attr_sync\n",
    "from phgml.reporting.logging import get_logging_path, get_logging_filename, get_dbx_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2663a9dc-5236-40e6-a3fb-f45879b5b173",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Disable adaptive query optimization\n",
    "# Adaptive query optimization groups together smaller tasks into a larger tasks.\n",
    "# This may result in limited parallelism if the parallel inference tasks are deemed to be too small by the query optimizer\n",
    "# We are diableing AQE here to circumevent this limitation on parallelism\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90537527-e323-47cc-9d55-6265cca468c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "params[\"REVENUE_COL\"] = \"_reservationRevenuePerRoomUSD\"\n",
    "params[\"ROOMS_COL\"] = \"_rooms\"\n",
    "params[\"PIPELINE\"] = \"INFERENCE\"\n",
    "\n",
    "params[\"WITHOUT_PMS\"] = str_to_bool(getArgument(\"exclude_pms\"))\n",
    "params[\"IS_USD_CURRENCY\"] = str_to_bool(getArgument(\"is_usd_currency\"))\n",
    "params[\"TARGET_TYPE\"] = getArgument(\"target_type\")\n",
    "selected_hotels = str_to_lst(getArgument(\"selected_hotels\"))\n",
    "params[\"LAG_NUMBERS\"] = list(map(int,str_to_lst(getArgument('lag_numbers'))))\n",
    "\n",
    "### The start of the model data\n",
    "params[\"MODEL_START_DATE\"] = pd.to_datetime(\"2018-10-01\")\n",
    "params[\"COVID_START_DATE\"] = pd.to_datetime(\"2020-03-01\")\n",
    "params[\"COVID_END_DATE\"] = pd.to_datetime(\"2021-08-01\")\n",
    "\n",
    "params[\"CALC_UNCERTAINTY\"] = False\n",
    "# MODEL_TYPE = \"XGB\"  # Use \"AG\" to try out the auto gloun approach\n",
    "params[\"MODEL_TYPE\"] = \"AG\"\n",
    "params[\"LEAD_WINDOW\"] = 60\n",
    "params[\"ML_EXPERIMENT_ID\"] = 1079527465953184\n",
    "\n",
    "params[\"LOG_ROOT\"] = '/dbfs/mnt/extractionlogs/synxis'\n",
    "if \"synxis_2_0\" in params[\"ENV\"]:\n",
    "    params[\"LOG_ROOT\"] = '/dbfs/mnt/extractionlogs/synxis_2_0'\n",
    "\n",
    "if params[\"MODEL_TYPE\"] == \"XGB\":\n",
    "    params[\"RUN_ID\"] = \"92907cac187f4c8cadb63ff60a05d72e\"  # XGB Run\n",
    "elif params[\"CALC_UNCERTAINTY\"] and (params[\"MODEL_TYPE\"] == \"AG\"):\n",
    "    params[\"RUN_ID\"] = \"9549361574484dc58fcf1b7d130541a0\"\n",
    "else:\n",
    "    params[\"RUN_ID\"] = \"19dee6420aed45f29e956016c5ea6e8a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20b3ae86-e63a-4f07-bb24-e3523b7c4e7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Config data relevant to this pipeline\n",
    "env_config = EnvironmentConfig(env=params[\"ENV\"], target=params[\"TARGET_TYPE\"], spark=spark, is_usd_currency=params[\"IS_USD_CURRENCY\"])\n",
    "forecasting_config_provider = ForecastingHotelConfigProvider(spark=spark,env=params[\"ENV\"])\n",
    "params[\"TARGET_COLUMN\"] = env_config.target_column\n",
    "\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "        \n",
    "logging.root.setLevel(logging.INFO)\n",
    "\n",
    "processing_timestamp = datetime.now(timezone.utc)\n",
    "TIMESTAMP = pd.to_datetime(processing_timestamp)\n",
    "\n",
    "logfile_path = get_logging_path(params[\"LOG_ROOT\"],processing_timestamp)\n",
    "if not os.path.exists(logfile_path):\n",
    "    os.makedirs(logfile_path)\n",
    "\n",
    "pms = \"PMS\"\n",
    "if params[\"WITHOUT_PMS\"]:\n",
    "    pms = \"NOPMS\"\n",
    "        \n",
    "log_file_name = get_logging_filename(\n",
    "    logfile_path,\n",
    "    params[\"PIPELINE\"],\n",
    "    params[\"TARGET_TYPE\"],\n",
    "    pms,\n",
    "    processing_timestamp)\n",
    "\n",
    "logger = logging.getLogger(f\"preprocess-{params['TARGET_TYPE']}-{pms}\")\n",
    "\n",
    "file_handler = logging.FileHandler(log_file_name)\n",
    "file_format = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "file_handler.setFormatter(file_format)\n",
    "\n",
    "logger.addHandler(file_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "693ac818-b495-4f7d-bd0f-bb95d4d63bde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = inference_output_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81d07e48-3eb8-4e84-b570-201b04619662",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# As a workaround for the bug PHG-2157\n",
    "params[\"PARTITION_DATE\"] = spark.sql(\n",
    "    f\"select max(confirmationDate) from {env_config.source_data_table}\"\n",
    ").collect()[0][0]\n",
    "\n",
    "print(f\"Partition date: {params['PARTITION_DATE']}\")\n",
    "\n",
    "max_inference_length = spark.sql(f'select max(inference_prediction_length) from {forecasting_config_provider.config_table_name}').collect()[0][0]\n",
    "params[\"TEST_PARTIITON_END\"] = params[\"PARTITION_DATE\"] + pd.Timedelta(max_inference_length, \"D\")\n",
    "print(f\"Partition end date: {params['TEST_PARTIITON_END']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b84d6db-9d15-4e1b-9a44-a38d1508dcd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def pyfunc_load_model_retry(model_uri, max_tries):\n",
    "    '''Retry mechanism for loading models from mlflow model registry to \n",
    "    handle the model loading error\n",
    "    '''\n",
    "    loop_len = max_tries+1\n",
    "    for i in range(loop_len):\n",
    "        try:\n",
    "            return mlflow.pyfunc.load_model(model_uri)\n",
    "        except Exception as e:\n",
    "            if i+1==loop_len:\n",
    "                raise e\n",
    "            else:\n",
    "                print(e)\n",
    "                print(f'Retrying: attempt {i+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22b1d1c0-c17d-4a36-a7c3-1baa1c611abb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def prediction_wrapper(\n",
    "    target_type, run_id, exclude_pms,hotel_config_provider,model_cache_dir,environment,infer_timestamp\n",
    "):\n",
    "    def predict_distributed(data):\n",
    "        static_cols_ = ['year', 'quarter_of_year', 'month_of_year', 'week_of_year', 'day_of_year', 'month_of_quarter', 'week_of_quarter', 'day_of_quarter', 'week_of_month', 'day_of_month', 'holiday', 'day_of_week_0', 'day_of_week_1', 'day_of_week_2', 'day_of_week_3', 'day_of_week_4', 'day_of_week_5', 'day_of_week_6']\n",
    "\n",
    "        logger = get_dbx_logger(\"PHGML\")\n",
    "        \n",
    "        max_lead_window = 100\n",
    "        \n",
    "        hotel_id = data[\"HotelID\"].iloc[0]\n",
    "        hotel_config = hotel_config_provider.get_config(hotel_id)\n",
    "        model_type = hotel_config.inference_model_name\n",
    "\n",
    "        print(f\"Processing Hotel {hotel_id}\")\n",
    "        \n",
    "        if target_type == \"REVENUE\":\n",
    "            col_prefix = \"RV\"\n",
    "\n",
    "            if hotel_config.forecast_currency is None:\n",
    "                # If the target type is REVENUE, we should have a defined forecast_currency\n",
    "                raise ValueError(f\"Forecast currency cannot be None for target_type {target_type}\")\n",
    "            \n",
    "        elif target_type == \"ROOMS\":\n",
    "            col_prefix = \"RM\"\n",
    "        \n",
    "        data = remove_padded_cols(data,hotel_config.lead_window,max_lead_window,col_prefix)\n",
    "        \n",
    "        model_version = 1\n",
    "        model_stage = \"Staging\"\n",
    "        model_name = None\n",
    "\n",
    "        try:\n",
    "            if data['_StayDates'].isna().any():\n",
    "                # Raise an exception for empty data\n",
    "                raise ValueError(\"The input data is empty and cannot proceed with prediction\")\n",
    "\n",
    "            if model_type == \"LIGHTGBM\":\n",
    "\n",
    "                model_obj = ModelWrapper(\n",
    "                                model_strategy=StrategyLGBM,\n",
    "                                prediction_horizon=hotel_config.inference_length,\n",
    "                                hotel_id=hotel_id,\n",
    "                                target_type=target_type,\n",
    "                                exclude_pms=exclude_pms,\n",
    "                                cd_axis_max_lags=99, \n",
    "                                static_cols =static_cols_,)\n",
    "                \n",
    "                model_obj.set_latest_model_version(model_stage = environment)\n",
    "\n",
    "                loaded_model = pyfunc_load_model_retry(model_obj.get_model_uri(), 6)\n",
    "                \n",
    "                loaded_model.unwrap_python_model().model_wrapper_model.prediction_horizon = hotel_config.inference_length\n",
    "                #during training time, the target variables are suffixed as '_tgt' to differentiate between target booking pace values and feature booking pace values. but while doing daily inferences,\n",
    "                # that distinction doesnt matter since we dont have the true values anyway, hence overriding the the target columns as below to avoid columns being not detected.\n",
    "                loaded_model.unwrap_python_model().model_wrapper_model.target_cols = {day_ahead:[ f\"{col_prefix}{j}\" for j in range(day_ahead)] for day_ahead in range(1,hotel_config.inference_length+1)}\n",
    "\n",
    "                # syncing attributes if the class implementation has extra attributes added\n",
    "                model_wrapper_attr_sync(model_wrapper_instance=loaded_model.unwrap_python_model().model_wrapper_model)\n",
    "                    \n",
    "            elif model_type == \"AUTOGLUON\":\n",
    "\n",
    "                model_obj = ModelWrapper(\n",
    "                                model_strategy=StrategyAG,\n",
    "                                is_auto_reg=True,\n",
    "                                prediction_horizon=hotel_config.inference_length,\n",
    "                                hotel_id=hotel_id,\n",
    "                                target_type=target_type,\n",
    "                                exclude_pms=exclude_pms,\n",
    "                                cd_axis_max_lags=99, \n",
    "                                static_cols =static_cols_,)\n",
    "\n",
    "\n",
    "                model_obj.set_latest_model_version()\n",
    "            \n",
    "                pms = \"PMS\"\n",
    "                if exclude_pms:\n",
    "                    pms = \"NOPMS\"\n",
    "\n",
    "                dbfs_dir = f\"{model_cache_dir}{hotel_id}_{target_type}_{pms}\" \n",
    "                local_dir = model_obj.local_root\n",
    "\n",
    "                if os.path.exists(local_dir):\n",
    "                    shutil.rmtree(local_dir)\n",
    "\n",
    "                # Copy cached model from blob storage to local dir                \n",
    "                shutil.copytree(dbfs_dir, local_dir)\n",
    "\n",
    "                # load model\n",
    "                loaded_model = load_pkl.load(path=model_obj.local_path)\n",
    "                loaded_model.prediction_horizon = model_obj.prediction_horizon\n",
    "\n",
    "            model_version = int(model_obj.version)\n",
    "            model_name = [\n",
    "                model_obj.get_model_name()\n",
    "                for step in range(1, hotel_config.inference_length + 1)\n",
    "            ]\n",
    "            model_metadata = model_obj.get_remote_model_metadata()\n",
    "            logger.info(\"Using model version {model_version}\")\n",
    "\n",
    "            logger.info(f\"Inference length of model: {model_metadata.get('inference_length','NOT_FOUND')}\")\n",
    "            logger.info(f\"Last trained date: {model_metadata.get('last_trained_date','NOT_FOUND')}\")           \n",
    "\n",
    "            output_dct = loaded_model.predict(data)\n",
    "\n",
    "            y_test, y_lower_raw, y_pred_raw, y_upper_raw = output_dct['y_test'], output_dct[0.1], output_dct[0.5], output_dct[0.9]\n",
    "\n",
    "            y_pred_interpolated = [interpolated_fill(day_ahead_array) for day_ahead_array in y_pred_raw]\n",
    "            \n",
    "            y_pred, y_upper, y_lower = correct_prediction_list(\n",
    "                y_pred_interpolated, y_test, y_upper_raw, y_lower_raw,target_type,available_rooms = hotel_config.available_rooms\n",
    "            )\n",
    "\n",
    "        \n",
    "            data[\"status\"] = \"complete\"\n",
    "            data[\"message\"] = f\"Successfully processed {hotel_id}\"\n",
    "\n",
    "            output_df = get_output_df(\n",
    "                y_pred=y_pred,\n",
    "                y_true=y_test,\n",
    "                run_id=run_id,\n",
    "                hotel_id=hotel_id,\n",
    "                data=data.sort_values('day_ahead'),\n",
    "                model_name=model_name,\n",
    "                model_version=model_version,\n",
    "                pms_sync_off=exclude_pms,\n",
    "                forecast_currency=hotel_config.forecast_currency,\n",
    "                prediction_horizon=hotel_config.inference_length,\n",
    "                y_upper=y_upper,\n",
    "                y_lower=y_lower,\n",
    "                y_med_raw=y_pred_raw,\n",
    "                y_upper_raw=y_upper_raw,\n",
    "                y_lower_raw=y_lower_raw,\n",
    "                timestamp=infer_timestamp\n",
    "            )\n",
    "\n",
    "            output_df[\"status\"] = \"complete\"\n",
    "            output_df[\"message\"] = f\"Successfully processed {hotel_id}\"\n",
    "\n",
    "        except (ValueError, MlflowException) as e:\n",
    "            # Check for specific error cases\n",
    "            if isinstance(e, ValueError):\n",
    "                message = str(e)\n",
    "                print(f\"ValueError occurred: {message}\")\n",
    "                \n",
    "            elif isinstance(e, MlflowException):\n",
    "                if \"RESOURCE_DOES_NOT_EXIST\" in e.message:\n",
    "                    print(f\"Model {model_obj.get_model_name()} was not found in the model registry. Skipping this model...\")\n",
    "                else:\n",
    "                    print(\"An MLFlowException occurred\")\n",
    "                    print(e)\n",
    "                message = e.message\n",
    "\n",
    "            empty = pd.DataFrame(\n",
    "                {\n",
    "                    \"HotelID\": [hotel_id],\n",
    "                    \"run_id\": [run_id],\n",
    "                    \"stay_date\": [pd.Timestamp(\"1900-01-01\")],\n",
    "                    \"booking_date\": [pd.Timestamp(\"1900-01-01\")],\n",
    "                    \"model_version\": [0],\n",
    "                    \"timestamp\": [pd.Timestamp(\"1900-01-01\")],\n",
    "                    \"pms_sync_off\": [exclude_pms],\n",
    "                    \"forecast_currency\":[hotel_config.forecast_currency],\n",
    "                    \"day_index\": [0],\n",
    "                    \"y_med\": [0],\n",
    "                    \"model_name\": [\"\"],\n",
    "                    \"y_upper\": [0],\n",
    "                    \"y_lower\": [0],\n",
    "                    \"y_med_raw\": [0],\n",
    "                    \"y_upper_raw\": [0],\n",
    "                    \"y_lower_raw\": [0],\n",
    "                    \"status\": \"incomplete\",\n",
    "                    \"message\": message,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            return empty\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Hotel {hotel_id} encountered an error \")\n",
    "            raise e\n",
    "        finally:\n",
    "            if model_type == \"AUTOGLUON\":\n",
    "                model_obj.clean()\n",
    "\n",
    "        return output_df\n",
    "    \n",
    "    return predict_distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38e4459f-fe4a-4648-90d3-3c768975ff7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logger.info(\"Read preprocessing data\")\n",
    "df = spark.sql(\n",
    "    f\"select * from {env_config.preprocess_intermediate_table}\"\n",
    ").withColumn(\"status\", lit(\"incomplete\"))\n",
    "df = df.withColumn(\"_StayDates\", to_timestamp(\"_StayDates\", \"yyyy-MM-dd\")).orderBy([\"HotelID\", \"_StayDates\"])\n",
    "\n",
    "df = df.withColumn('partition_date', lit(str(params[\"PARTITION_DATE\"])))\n",
    "df = df.withColumn(\"day_ahead\", datediff(col(\"_StayDates\"), to_timestamp('partition_date', \"yyyy-MM-dd\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f658f9ca-a312-491c-a66a-3bfec0e0119d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Group the data by hotel id and execute the inferences in parallel\n",
    "logger.info(\"Starting parallell processing\")\n",
    "output_df = df.groupby(\"HotelID\").applyInPandas(\n",
    "    prediction_wrapper(\n",
    "        target_type=params[\"TARGET_TYPE\"], \n",
    "        run_id=params[\"RUN_ID\"], \n",
    "        exclude_pms=params[\"WITHOUT_PMS\"], \n",
    "        hotel_config_provider=forecasting_config_provider,\n",
    "        model_cache_dir=env_config.model_cache_dir,\n",
    "        environment=params[\"ENV\"],\n",
    "        infer_timestamp=TIMESTAMP\n",
    "    ),\n",
    "    schema,\n",
    ")\n",
    "\n",
    "logger.info(\n",
    "    f\"Writing inference results to temporary table {env_config.inference_intermediate_table}\"\n",
    ")\n",
    "(\n",
    "    output_df.write.mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(env_config.inference_intermediate_table)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e6a27ce-17b9-4c06-9d71-a87562f793ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "meta_columns = [\"HotelID\", \"run_id\", \"timestamp\", \"pms_sync_off\", \"status\", \"message\"]\n",
    "results_table = spark.sql(f\"select * from {env_config.inference_intermediate_table}\")\n",
    "output_meta = results_table.select(meta_columns).toPandas()\n",
    "\n",
    "num_completed = output_meta[output_meta[\"status\"] == \"complete\"][\"HotelID\"].nunique()\n",
    "total = output_meta[\"HotelID\"].nunique()\n",
    "logger.info(f\"{num_completed} out of {total} hotels processed succussfully\")\n",
    "print(f\"{num_completed} out of {total} hotels processed succussfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c2483c5-c061-4840-aa64-2897c6e74b7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "incomplete = output_meta[~(output_meta[\"status\"] == \"complete\")]\n",
    "\n",
    "for row in incomplete.itertuples():\n",
    "    logger.error(\n",
    "        f\"Error encountered when processing hotel {row.HotelID}: {row.message}\"\n",
    "    )\n",
    "    print( f\"Error encountered when processing hotel {row.HotelID}: {row.message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba49719c-6126-42e2-af67-126cb3cf5a54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "output_df = results_table.filter(results_table.status == \"complete\").drop(\n",
    "    \"status\", \"message\"\n",
    ")\n",
    "\n",
    "#Drop forecast currency if TARGET_TYPE is ROOMS\n",
    "if params[\"TARGET_TYPE\"] == \"ROOMS\":\n",
    "    output_df = output_df.drop(\"forecast_currency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c764d0a6-156b-4079-8569-d059bd1fd393",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logger.info(\"Writing completed results to table\")\n",
    "file_format = \"delta\"\n",
    "\n",
    "(\n",
    "    output_df.write.format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .partitionBy(\"HotelID\")\n",
    "    .option(\"path\", env_config.inference_output_table_blob)\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(env_config.inference_output_table)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba7dc677-c0e1-48ac-aa69-73d2c8b26634",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "elapsed_time = time.perf_counter() - start_time\n",
    "logger.info(f\"Time elapsed {elapsed_time}\")\n",
    "logger.info(f\"Time elapsed in minutes {elapsed_time/60}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "forecast_inference_distributed",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}