{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "919e0ab2-5679-4de5-a2af-7c1a3eb719ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.widgets.dropdown(\"env_stage\", \"dev\", [\"dev\", \"prod\"], \"Pipeline stage\")\n",
    "dbutils.widgets.dropdown(\"exclude_pms\", \"False\", [\"True\", \"False\"], \"Exclude PMS\")\n",
    "dbutils.widgets.dropdown(\"target_type\", \"REVENUE\", [\"REVENUE\", \"ROOMS\"], \"Target Type\")\n",
    "dbutils.widgets.dropdown(\"is_usd_currency\", \"True\", [\"True\", \"False\"], \"Use USD currency\")\n",
    "dbutils.widgets.text(\"lag_numbers\",\"1,7,14,28\", \"Lag Numbers\")\n",
    "dbutils.widgets.text(\"model_tags\",\"\", \"Model Tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c540e505-6904-429b-bb18-2f68d93e8bba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Python interpreter will be restarted.\n",
       "Requirement already satisfied: mlflow==2.2.2 in /databricks/python3/lib/python3.8/site-packages (2.2.2)\n",
       "Requirement already satisfied: scipy&lt;2 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (1.6.2)\n",
       "Requirement already satisfied: requests&lt;3,&gt;=2.17.3 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (2.32.3)\n",
       "Requirement already satisfied: entrypoints&lt;1 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (0.3)\n",
       "Requirement already satisfied: markdown&lt;4,&gt;=3.3 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (3.7)\n",
       "Requirement already satisfied: cloudpickle&lt;3 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (2.2.1)\n",
       "Requirement already satisfied: scikit-learn&lt;2 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (1.0.2)\n",
       "Requirement already satisfied: shap&lt;1,&gt;=0.40 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (0.44.1)\n",
       "Requirement already satisfied: pytz&lt;2023 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (2020.5)\n",
       "Requirement already satisfied: pyyaml&lt;7,&gt;=5.1 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (6.0.2)\n",
       "Requirement already satisfied: docker&lt;7,&gt;=4.0.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (6.1.3)\n",
       "Requirement already satisfied: pandas&lt;3 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (1.4.4)\n",
       "Requirement already satisfied: querystring-parser&lt;2 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (1.2.4)\n",
       "Requirement already satisfied: numpy&lt;2 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (1.22.4)\n",
       "Requirement already satisfied: importlib-metadata!=4.7.0,&lt;7,&gt;=3.7.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (6.11.0)\n",
       "Requirement already satisfied: protobuf&lt;5,&gt;=3.12.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (3.17.2)\n",
       "Requirement already satisfied: pyarrow&lt;12,&gt;=4.0.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (4.0.0)\n",
       "Requirement already satisfied: Flask&lt;3 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (2.3.3)\n",
       "Requirement already satisfied: packaging&lt;24 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (23.2)\n",
       "Requirement already satisfied: databricks-cli&lt;1,&gt;=0.8.7 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (0.18.0)\n",
       "Requirement already satisfied: alembic&lt;2 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (1.13.3)\n",
       "Requirement already satisfied: sqlparse&lt;1,&gt;=0.4.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (0.5.1)\n",
       "Requirement already satisfied: matplotlib&lt;4 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (3.4.2)\n",
       "Requirement already satisfied: gunicorn&lt;21 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (20.1.0)\n",
       "Requirement already satisfied: Jinja2&lt;4,&gt;=2.11 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (3.1.4)\n",
       "Requirement already satisfied: gitpython&lt;4,&gt;=2.1.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (3.1.43)\n",
       "Requirement already satisfied: click&lt;9,&gt;=7.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (8.1.7)\n",
       "Requirement already satisfied: sqlalchemy&lt;3,&gt;=1.4.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (2.0.35)\n",
       "Requirement already satisfied: importlib-resources in /databricks/python3/lib/python3.8/site-packages (from alembic&lt;2-&gt;mlflow==2.2.2) (6.4.5)\n",
       "Requirement already satisfied: typing-extensions&gt;=4 in /databricks/python3/lib/python3.8/site-packages (from alembic&lt;2-&gt;mlflow==2.2.2) (4.12.2)\n",
       "Requirement already satisfied: Mako in /databricks/python3/lib/python3.8/site-packages (from alembic&lt;2-&gt;mlflow==2.2.2) (1.3.5)\n",
       "Requirement already satisfied: oauthlib&gt;=3.1.0 in /databricks/python3/lib/python3.8/site-packages (from databricks-cli&lt;1,&gt;=0.8.7-&gt;mlflow==2.2.2) (3.2.2)\n",
       "Requirement already satisfied: pyjwt&gt;=1.7.0 in /databricks/python3/lib/python3.8/site-packages (from databricks-cli&lt;1,&gt;=0.8.7-&gt;mlflow==2.2.2) (2.9.0)\n",
       "Requirement already satisfied: urllib3&lt;3,&gt;=1.26.7 in /databricks/python3/lib/python3.8/site-packages (from databricks-cli&lt;1,&gt;=0.8.7-&gt;mlflow==2.2.2) (1.26.15)\n",
       "Requirement already satisfied: tabulate&gt;=0.7.7 in /databricks/python3/lib/python3.8/site-packages (from databricks-cli&lt;1,&gt;=0.8.7-&gt;mlflow==2.2.2) (0.9.0)\n",
       "Requirement already satisfied: six&gt;=1.10.0 in /databricks/python3/lib/python3.8/site-packages (from databricks-cli&lt;1,&gt;=0.8.7-&gt;mlflow==2.2.2) (1.15.0)\n",
       "Requirement already satisfied: websocket-client&gt;=0.32.0 in /databricks/python3/lib/python3.8/site-packages (from docker&lt;7,&gt;=4.0.0-&gt;mlflow==2.2.2) (1.8.0)\n",
       "Requirement already satisfied: blinker&gt;=1.6.2 in /databricks/python3/lib/python3.8/site-packages (from Flask&lt;3-&gt;mlflow==2.2.2) (1.8.2)\n",
       "Requirement already satisfied: Werkzeug&gt;=2.3.7 in /databricks/python3/lib/python3.8/site-packages (from Flask&lt;3-&gt;mlflow==2.2.2) (3.0.4)\n",
       "Requirement already satisfied: itsdangerous&gt;=2.1.2 in /databricks/python3/lib/python3.8/site-packages (from Flask&lt;3-&gt;mlflow==2.2.2) (2.2.0)\n",
       "Requirement already satisfied: gitdb&lt;5,&gt;=4.0.1 in /databricks/python3/lib/python3.8/site-packages (from gitpython&lt;4,&gt;=2.1.0-&gt;mlflow==2.2.2) (4.0.11)\n",
       "Requirement already satisfied: smmap&lt;6,&gt;=3.0.1 in /databricks/python3/lib/python3.8/site-packages (from gitdb&lt;5,&gt;=4.0.1-&gt;gitpython&lt;4,&gt;=2.1.0-&gt;mlflow==2.2.2) (5.0.1)\n",
       "Requirement already satisfied: setuptools&gt;=3.0 in /usr/local/lib/python3.8/dist-packages (from gunicorn&lt;21-&gt;mlflow==2.2.2) (52.0.0)\n",
       "Requirement already satisfied: zipp&gt;=0.5 in /databricks/python3/lib/python3.8/site-packages (from importlib-metadata!=4.7.0,&lt;7,&gt;=3.7.0-&gt;mlflow==2.2.2) (3.20.2)\n",
       "Requirement already satisfied: MarkupSafe&gt;=2.0 in /databricks/python3/lib/python3.8/site-packages (from Jinja2&lt;4,&gt;=2.11-&gt;mlflow==2.2.2) (2.1.5)\n",
       "Requirement already satisfied: cycler&gt;=0.10 in /databricks/python3/lib/python3.8/site-packages (from matplotlib&lt;4-&gt;mlflow==2.2.2) (0.10.0)\n",
       "Requirement already satisfied: pillow&gt;=6.2.0 in /databricks/python3/lib/python3.8/site-packages (from matplotlib&lt;4-&gt;mlflow==2.2.2) (8.2.0)\n",
       "Requirement already satisfied: pyparsing&gt;=2.2.1 in /databricks/python3/lib/python3.8/site-packages (from matplotlib&lt;4-&gt;mlflow==2.2.2) (2.4.7)\n",
       "Requirement already satisfied: kiwisolver&gt;=1.0.1 in /databricks/python3/lib/python3.8/site-packages (from matplotlib&lt;4-&gt;mlflow==2.2.2) (1.3.1)\n",
       "Requirement already satisfied: python-dateutil&gt;=2.7 in /databricks/python3/lib/python3.8/site-packages (from matplotlib&lt;4-&gt;mlflow==2.2.2) (2.8.1)\n",
       "Requirement already satisfied: certifi&gt;=2017.4.17 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.17.3-&gt;mlflow==2.2.2) (2020.12.5)\n",
       "Requirement already satisfied: idna&lt;4,&gt;=2.5 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.17.3-&gt;mlflow==2.2.2) (2.10)\n",
       "Requirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.17.3-&gt;mlflow==2.2.2) (3.3.2)\n",
       "Requirement already satisfied: joblib&gt;=0.11 in /databricks/python3/lib/python3.8/site-packages (from scikit-learn&lt;2-&gt;mlflow==2.2.2) (1.0.1)\n",
       "Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /databricks/python3/lib/python3.8/site-packages (from scikit-learn&lt;2-&gt;mlflow==2.2.2) (2.1.0)\n",
       "Requirement already satisfied: numba in /databricks/python3/lib/python3.8/site-packages (from shap&lt;1,&gt;=0.40-&gt;mlflow==2.2.2) (0.58.1)\n",
       "Requirement already satisfied: tqdm&gt;=4.27.0 in /databricks/python3/lib/python3.8/site-packages (from shap&lt;1,&gt;=0.40-&gt;mlflow==2.2.2) (4.66.5)\n",
       "Requirement already satisfied: slicer==0.0.7 in /databricks/python3/lib/python3.8/site-packages (from shap&lt;1,&gt;=0.40-&gt;mlflow==2.2.2) (0.0.7)\n",
       "Requirement already satisfied: greenlet!=0.4.17 in /databricks/python3/lib/python3.8/site-packages (from sqlalchemy&lt;3,&gt;=1.4.0-&gt;mlflow==2.2.2) (3.1.1)\n",
       "Requirement already satisfied: llvmlite&lt;0.42,&gt;=0.41.0dev0 in /databricks/python3/lib/python3.8/site-packages (from numba-&gt;shap&lt;1,&gt;=0.40-&gt;mlflow==2.2.2) (0.41.1)\n",
       "Python interpreter will be restarted.\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Python interpreter will be restarted.\nRequirement already satisfied: mlflow==2.2.2 in /databricks/python3/lib/python3.8/site-packages (2.2.2)\nRequirement already satisfied: scipy&lt;2 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (1.6.2)\nRequirement already satisfied: requests&lt;3,&gt;=2.17.3 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (2.32.3)\nRequirement already satisfied: entrypoints&lt;1 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (0.3)\nRequirement already satisfied: markdown&lt;4,&gt;=3.3 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (3.7)\nRequirement already satisfied: cloudpickle&lt;3 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (2.2.1)\nRequirement already satisfied: scikit-learn&lt;2 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (1.0.2)\nRequirement already satisfied: shap&lt;1,&gt;=0.40 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (0.44.1)\nRequirement already satisfied: pytz&lt;2023 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (2020.5)\nRequirement already satisfied: pyyaml&lt;7,&gt;=5.1 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (6.0.2)\nRequirement already satisfied: docker&lt;7,&gt;=4.0.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (6.1.3)\nRequirement already satisfied: pandas&lt;3 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (1.4.4)\nRequirement already satisfied: querystring-parser&lt;2 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (1.2.4)\nRequirement already satisfied: numpy&lt;2 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (1.22.4)\nRequirement already satisfied: importlib-metadata!=4.7.0,&lt;7,&gt;=3.7.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (6.11.0)\nRequirement already satisfied: protobuf&lt;5,&gt;=3.12.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (3.17.2)\nRequirement already satisfied: pyarrow&lt;12,&gt;=4.0.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (4.0.0)\nRequirement already satisfied: Flask&lt;3 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (2.3.3)\nRequirement already satisfied: packaging&lt;24 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (23.2)\nRequirement already satisfied: databricks-cli&lt;1,&gt;=0.8.7 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (0.18.0)\nRequirement already satisfied: alembic&lt;2 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (1.13.3)\nRequirement already satisfied: sqlparse&lt;1,&gt;=0.4.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (0.5.1)\nRequirement already satisfied: matplotlib&lt;4 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (3.4.2)\nRequirement already satisfied: gunicorn&lt;21 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (20.1.0)\nRequirement already satisfied: Jinja2&lt;4,&gt;=2.11 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (3.1.4)\nRequirement already satisfied: gitpython&lt;4,&gt;=2.1.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (3.1.43)\nRequirement already satisfied: click&lt;9,&gt;=7.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (8.1.7)\nRequirement already satisfied: sqlalchemy&lt;3,&gt;=1.4.0 in /databricks/python3/lib/python3.8/site-packages (from mlflow==2.2.2) (2.0.35)\nRequirement already satisfied: importlib-resources in /databricks/python3/lib/python3.8/site-packages (from alembic&lt;2-&gt;mlflow==2.2.2) (6.4.5)\nRequirement already satisfied: typing-extensions&gt;=4 in /databricks/python3/lib/python3.8/site-packages (from alembic&lt;2-&gt;mlflow==2.2.2) (4.12.2)\nRequirement already satisfied: Mako in /databricks/python3/lib/python3.8/site-packages (from alembic&lt;2-&gt;mlflow==2.2.2) (1.3.5)\nRequirement already satisfied: oauthlib&gt;=3.1.0 in /databricks/python3/lib/python3.8/site-packages (from databricks-cli&lt;1,&gt;=0.8.7-&gt;mlflow==2.2.2) (3.2.2)\nRequirement already satisfied: pyjwt&gt;=1.7.0 in /databricks/python3/lib/python3.8/site-packages (from databricks-cli&lt;1,&gt;=0.8.7-&gt;mlflow==2.2.2) (2.9.0)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.26.7 in /databricks/python3/lib/python3.8/site-packages (from databricks-cli&lt;1,&gt;=0.8.7-&gt;mlflow==2.2.2) (1.26.15)\nRequirement already satisfied: tabulate&gt;=0.7.7 in /databricks/python3/lib/python3.8/site-packages (from databricks-cli&lt;1,&gt;=0.8.7-&gt;mlflow==2.2.2) (0.9.0)\nRequirement already satisfied: six&gt;=1.10.0 in /databricks/python3/lib/python3.8/site-packages (from databricks-cli&lt;1,&gt;=0.8.7-&gt;mlflow==2.2.2) (1.15.0)\nRequirement already satisfied: websocket-client&gt;=0.32.0 in /databricks/python3/lib/python3.8/site-packages (from docker&lt;7,&gt;=4.0.0-&gt;mlflow==2.2.2) (1.8.0)\nRequirement already satisfied: blinker&gt;=1.6.2 in /databricks/python3/lib/python3.8/site-packages (from Flask&lt;3-&gt;mlflow==2.2.2) (1.8.2)\nRequirement already satisfied: Werkzeug&gt;=2.3.7 in /databricks/python3/lib/python3.8/site-packages (from Flask&lt;3-&gt;mlflow==2.2.2) (3.0.4)\nRequirement already satisfied: itsdangerous&gt;=2.1.2 in /databricks/python3/lib/python3.8/site-packages (from Flask&lt;3-&gt;mlflow==2.2.2) (2.2.0)\nRequirement already satisfied: gitdb&lt;5,&gt;=4.0.1 in /databricks/python3/lib/python3.8/site-packages (from gitpython&lt;4,&gt;=2.1.0-&gt;mlflow==2.2.2) (4.0.11)\nRequirement already satisfied: smmap&lt;6,&gt;=3.0.1 in /databricks/python3/lib/python3.8/site-packages (from gitdb&lt;5,&gt;=4.0.1-&gt;gitpython&lt;4,&gt;=2.1.0-&gt;mlflow==2.2.2) (5.0.1)\nRequirement already satisfied: setuptools&gt;=3.0 in /usr/local/lib/python3.8/dist-packages (from gunicorn&lt;21-&gt;mlflow==2.2.2) (52.0.0)\nRequirement already satisfied: zipp&gt;=0.5 in /databricks/python3/lib/python3.8/site-packages (from importlib-metadata!=4.7.0,&lt;7,&gt;=3.7.0-&gt;mlflow==2.2.2) (3.20.2)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /databricks/python3/lib/python3.8/site-packages (from Jinja2&lt;4,&gt;=2.11-&gt;mlflow==2.2.2) (2.1.5)\nRequirement already satisfied: cycler&gt;=0.10 in /databricks/python3/lib/python3.8/site-packages (from matplotlib&lt;4-&gt;mlflow==2.2.2) (0.10.0)\nRequirement already satisfied: pillow&gt;=6.2.0 in /databricks/python3/lib/python3.8/site-packages (from matplotlib&lt;4-&gt;mlflow==2.2.2) (8.2.0)\nRequirement already satisfied: pyparsing&gt;=2.2.1 in /databricks/python3/lib/python3.8/site-packages (from matplotlib&lt;4-&gt;mlflow==2.2.2) (2.4.7)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /databricks/python3/lib/python3.8/site-packages (from matplotlib&lt;4-&gt;mlflow==2.2.2) (1.3.1)\nRequirement already satisfied: python-dateutil&gt;=2.7 in /databricks/python3/lib/python3.8/site-packages (from matplotlib&lt;4-&gt;mlflow==2.2.2) (2.8.1)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.17.3-&gt;mlflow==2.2.2) (2020.12.5)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.17.3-&gt;mlflow==2.2.2) (2.10)\nRequirement already satisfied: charset-normalizer&lt;4,&gt;=2 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3,&gt;=2.17.3-&gt;mlflow==2.2.2) (3.3.2)\nRequirement already satisfied: joblib&gt;=0.11 in /databricks/python3/lib/python3.8/site-packages (from scikit-learn&lt;2-&gt;mlflow==2.2.2) (1.0.1)\nRequirement already satisfied: threadpoolctl&gt;=2.0.0 in /databricks/python3/lib/python3.8/site-packages (from scikit-learn&lt;2-&gt;mlflow==2.2.2) (2.1.0)\nRequirement already satisfied: numba in /databricks/python3/lib/python3.8/site-packages (from shap&lt;1,&gt;=0.40-&gt;mlflow==2.2.2) (0.58.1)\nRequirement already satisfied: tqdm&gt;=4.27.0 in /databricks/python3/lib/python3.8/site-packages (from shap&lt;1,&gt;=0.40-&gt;mlflow==2.2.2) (4.66.5)\nRequirement already satisfied: slicer==0.0.7 in /databricks/python3/lib/python3.8/site-packages (from shap&lt;1,&gt;=0.40-&gt;mlflow==2.2.2) (0.0.7)\nRequirement already satisfied: greenlet!=0.4.17 in /databricks/python3/lib/python3.8/site-packages (from sqlalchemy&lt;3,&gt;=1.4.0-&gt;mlflow==2.2.2) (3.1.1)\nRequirement already satisfied: llvmlite&lt;0.42,&gt;=0.41.0dev0 in /databricks/python3/lib/python3.8/site-packages (from numba-&gt;shap&lt;1,&gt;=0.40-&gt;mlflow==2.2.2) (0.41.1)\nPython interpreter will be restarted.\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pip install mlflow==2.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "314fc74e-4d61-411c-a627-3c2e94147841",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0f5d98e-ec41-4905-aede-1e12ee194ad5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import *\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import os\n",
    "from sys import version_info\n",
    "import cloudpickle\n",
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "import logging\n",
    "import warnings\n",
    "from mlflow import MlflowException\n",
    "from mlflow.client import MlflowClient\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e7c2ed1-aa68-4ada-873e-1e0a25a433a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sys.path.append(os.path.abspath('/Workspace/Repos/manik@surge.global/phg-data-mlsys/src'))\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# start_time = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b15d5110-dd01-404f-9777-1cf3f49b955e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ENV = getArgument(\"env_stage\")\n",
    "\n",
    "REPOPATH = \"/Workspace/Repos/manik@surge.global/phg-data-mlsys/src\"\n",
    "\n",
    "cluster_name = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterName\")\n",
    "\n",
    "if (ENV == \"dev\") and (\"dev\" in cluster_name):\n",
    "    print(f\"Loading phgml package from repo {REPOPATH}\")\n",
    "    sys.path.append(os.path.abspath(REPOPATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c83010d-d1ad-406e-9484-d306db4fb9c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from phgml.models.xgboost_model import XGBMultiStepPredictor\n",
    "from phgml.models.autogluon_model import AutoGluonModel, AGMlflowModel\n",
    "from phgml.models.lightgbm_model import LightGBMModel, LGBMMlflowModel\n",
    "# from phgml.pipeline.training import train_wrapper\n",
    "from phgml.data.processing_distr_ca import (\n",
    "    filter_train_data,\n",
    "    filter_test_data,\n",
    "    remove_padded_cols,\n",
    ")\n",
    "from phgml.reporting.output_metrics import *\n",
    "from phgml.data.data_types import (\n",
    "    revenue_preprocessed_schema,\n",
    "    rooms_preprocessed_schema,\n",
    "    training_output_schema,\n",
    ")\n",
    "from phgml.reporting.logging import get_logging_path, get_logging_filename, get_dbx_logger\n",
    "from phgml.reporting.report_results import get_output_df, correct_prediction_list\n",
    "from phgml.data.config import EnvironmentConfig, ForecastingHotelConfigProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be04703d-398d-4ca8-a715-fa9c44161699",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Disable adaptrive query optimization\n",
    "# Adaptive query optimization groups together smaller tasks into a larger tasks.\n",
    "# This may result in limited parallelism if the parallel inference tasks are deemed to be too small by the query optimizer\n",
    "# We are diableing AQE here to circumevent this limitation on parallelism\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc294607-1c49-4fba-8683-df3974b499b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def str_to_lst(value):\n",
    "    if value == \"\":\n",
    "        return []\n",
    "    elif \",\" in value:\n",
    "        hotels = value.split(\",\")\n",
    "        return hotels\n",
    "\n",
    "    return [value]\n",
    "\n",
    "\n",
    "def str_to_bool(value):\n",
    "    FALSE_VALUES = [\"false\", \"no\", \"0\"]\n",
    "    TRUE_VALUES = [\"true\", \"yes\", \"1\"]\n",
    "    lvalue = str(value).lower()\n",
    "    if lvalue in (FALSE_VALUES):\n",
    "        return False\n",
    "    if lvalue in (TRUE_VALUES):\n",
    "        return True\n",
    "    raise Exception(\n",
    "        \"String value should be one of {}, but got '{}'.\".format(\n",
    "            FALSE_VALUES + TRUE_VALUES, value\n",
    "        )\n",
    "    )\n",
    "\n",
    "def get_model_tags(model_tags_str):\n",
    "    ''' A Validation for the model tag text through databricks utility'''\n",
    "    valid_pattern = r'(\\w+:\\w+)'\n",
    "    invalid_pattern = r'[^:,\\w\\s]'\n",
    "    str_cpy=model_tags_str.replace(\" \", '')\n",
    "\n",
    "    not_allowed_symbols = re.findall(pattern=invalid_pattern, string=str_cpy)\n",
    "    if len(not_allowed_symbols)>0:\n",
    "        raise ValueError('''Unwanted characters detected. Allowed characters are colon(:), comma(,), word characters and white space characters\n",
    "                            Please specify key values pairs as key1:value1,key2:value2\n",
    "                        ''')\n",
    "    else:\n",
    "        matching_pairs = re.findall(pattern=valid_pattern, string=str_cpy)\n",
    "\n",
    "        for matching_str in matching_pairs:\n",
    "            str_cpy = str_cpy.replace(matching_str,'')\n",
    "        str_cpy = str_cpy.replace(',','')\n",
    "        \n",
    "        if len(str_cpy)>0:\n",
    "            raise ValueError('''unmatched string components detected. please check the specified string\n",
    "                             Please specify key values pairs as key1:value1,key2:value2\n",
    "                             ''')\n",
    "        \n",
    "    return {key:value for key,value in map(lambda x: x.split(':'),matching_pairs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bc22507-599d-476e-943d-4a75a7217baf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "log_root = \"/dbfs/mnt/extractionlogs/synxis\"\n",
    "processing_timestamp = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7aa193f6-ca13-4259-b87b-83d82f3979fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">model tags dict:  {}\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">model tags dict:  {}\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "REVENUE_COL = \"_reservationRevenuePerRoomUSD\"\n",
    "ROOMS_COL = \"rooms\"\n",
    "PIPELINE = \"TRAINING\"\n",
    "\n",
    "WITHOUT_PMS = str_to_bool(getArgument(\"exclude_pms\"))\n",
    "IS_USD_CURRENCY = str_to_bool(getArgument(\"is_usd_currency\"))\n",
    "TARGET_TYPE = getArgument(\"target_type\")\n",
    "\n",
    "MODEL_TAGS_DCT = get_model_tags(getArgument(\"model_tags\"))\n",
    "print('model tags dict: ',MODEL_TAGS_DCT)\n",
    "\n",
    "### The start of the model data\n",
    "MODEL_START_DATE = pd.to_datetime(\"2018-10-01\")\n",
    "\n",
    "COVID_START_DATE = pd.to_datetime(\"2020-03-01\")\n",
    "COVID_END_DATE = pd.to_datetime(\"2021-08-01\")\n",
    "\n",
    "CALC_UNCERTAINTY = True\n",
    "# MODEL_TYPE = \"XGB\"  # Use \"AG\" to try out the auto gloun approach\n",
    "MODEL_TYPE = \"AG\"\n",
    "\n",
    "LEAD_WINDOW = 60\n",
    "PREDICTION_HORIZON = 30\n",
    "\n",
    "ML_EXPERIMENT_ID = 609933091443417\n",
    "\n",
    "lead_window_start_days = 14\n",
    "lead_window_end_days = 60\n",
    "prediction_horizon = 14\n",
    "LAG_NUMBERS = list(map(int,str_to_lst(getArgument('lag_numbers'))))\n",
    "\n",
    "\n",
    "SAVE_MODEL = False\n",
    "SAVE_METRICS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d33109f-b434-4efa-ba34-8dc89ea9f7ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Config data relevant to this pipeline\n",
    "env_config = EnvironmentConfig(env=ENV, target=TARGET_TYPE, spark=spark, is_usd_currency=IS_USD_CURRENCY)\n",
    "forecasting_config_provider = ForecastingHotelConfigProvider(spark=spark,env=ENV)\n",
    "target_column = env_config.target_column\n",
    "schema = training_output_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df4e0dea-b891-4e71-afa7-deb5386071fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger= get_dbx_logger(\n",
    "    pipeline=PIPELINE,\n",
    "    task_type=TARGET_TYPE,\n",
    "    exclude_pms=WITHOUT_PMS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebd76218-9157-4cde-aa25-143be3dd81ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger.info(f\"Processing data for target type: {TARGET_TYPE} : {target_column}\")\n",
    "logger.info(f\"Excluding PMS data? {WITHOUT_PMS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a227e9f6-13f5-4f24-bc82-3a4349e8b783",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Removed forecast training wrapper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa815752-d33c-4289-b9e0-08acb8d0932f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "logger.info(f\"Loading data from testing_data.pp_ff_preprocess_rv\")\n",
    "df = spark.sql(f\"select * from testing_data.pp_ff_preprocess_rv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aeab7d2d-36ab-4b27-85d6-439fa5d38cfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if df.count() <= 0:\n",
    "    logger.error(\"The loaded training dataset is empty.\")\n",
    "    logger.info(\"Terminting the pipeline execution\")\n",
    "    raise Exception(\"The loaded training dataset is empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e66055d-16cc-4034-b95a-4782cba71af0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "from typing import Optional, Tuple, Union, List, Dict, Any, Callable\n",
    "from lightgbm import LGBMRegressor\n",
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "__all__ = [\"BaseStrategy\", \"StrategyLGBM\", \"StrategyAG\", \"StrategyLGBMFarField\"]\n",
    "\n",
    "\n",
    "class BaseStrategy(ABC):\n",
    "    model_type = \"base_strategy\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        day_ahead: int,\n",
    "        quantile_levels: list,\n",
    "        cd_axis_targets: list,\n",
    "        path: str,\n",
    "        is_auto_reg: bool,\n",
    "        # verbose: int,\n",
    "    ):\n",
    "        self.quantile_levels = quantile_levels\n",
    "        self.day_ahead = day_ahead\n",
    "        self.cd_axis_targets = cd_axis_targets\n",
    "        self.target_prefix = self.cd_axis_targets[0][:2]\n",
    "        self.path = path\n",
    "        self.autoregressive_predictions = is_auto_reg\n",
    "        # self.verbose = verbose\n",
    "\n",
    "        self.objective = \"quantile\"\n",
    "        self.sub_predictors: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "    @abstractmethod\n",
    "    def _fit(self, train_data: pd.DataFrame) -> None:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _predict(self, test_data: pd.Series) -> Dict[float, List[npt.NDArray]]:\n",
    "        pass\n",
    "\n",
    "\n",
    "class StrategyLGBM(BaseStrategy):\n",
    "    model_type = \"LGBM\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        day_ahead: int,\n",
    "        quantile_levels: list,\n",
    "        cd_axis_targets: list,\n",
    "        path: str,\n",
    "        is_auto_reg: bool,\n",
    "        n_jobs: int = -1,\n",
    "        # verbose: int = -1,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            day_ahead=day_ahead,\n",
    "            quantile_levels=quantile_levels,\n",
    "            cd_axis_targets=cd_axis_targets,\n",
    "            path=path,\n",
    "            is_auto_reg=is_auto_reg,\n",
    "            # verbose=verbose,\n",
    "        )\n",
    "        # self.n_jobs = None #n_jobs\n",
    "\n",
    "    def _fit(self, train_data: pd.DataFrame) -> None:\n",
    "        self.target_feature_dtypes = dict(train_data.dtypes)\n",
    "\n",
    "        # using lambda function to identify dropping cols, rather than using a if condition inside the for loop\n",
    "        drop_cols_func = lambda target_index: self.cd_axis_targets\n",
    "        if self.autoregressive_predictions:\n",
    "            drop_cols_func = lambda target_index: self.cd_axis_targets[target_index:]\n",
    "\n",
    "        for index, target_ in enumerate(self.cd_axis_targets):\n",
    "            x_data = train_data.drop(drop_cols_func(index), axis=1)\n",
    "            y_data = train_data[[target_]]\n",
    "\n",
    "            # if self.verbose != -1:\n",
    "            print(\n",
    "                \"\\t\\ttarget: \",\n",
    "                target_,\n",
    "                \" x_data_cols:\",\n",
    "                [col for col in x_data.columns if self.target_prefix in col][:6],\n",
    "                \" y_data_cols:\",\n",
    "                list(y_data.columns),\n",
    "            )\n",
    "\n",
    "            reg_objs = {}\n",
    "            for qtile in self.quantile_levels:\n",
    "                sub_predictor = LGBMRegressor(\n",
    "                    objective=self.objective, alpha=qtile, verbose=-1, n_jobs=self.n_jobs\n",
    "                )\n",
    "                sub_predictor.fit(x_data, y_data)\n",
    "                reg_objs[qtile] = sub_predictor\n",
    "            self.sub_predictors[target_] = {\n",
    "                \"predictors\": reg_objs,\n",
    "                \"targets\": list(y_data.columns),\n",
    "                \"features\": list(x_data.columns),\n",
    "            }\n",
    "\n",
    "    def _predict(self, test_data: pd.Series) -> Dict[float, List[npt.NDArray]]:\n",
    "\n",
    "        # making sure dtypes are the same as in training, and filtering out the target columns from the dtypes dict since in test data its not there.\n",
    "        needed_dtypes = {\n",
    "            col: col_dtype\n",
    "            for col, col_dtype in self.target_feature_dtypes.items()\n",
    "            if \"_tgt\" not in col\n",
    "        }\n",
    "        test_data_cpy = (\n",
    "            test_data.to_frame().T.reset_index(drop=True).copy().astype(needed_dtypes)\n",
    "        )\n",
    "\n",
    "        data = {qtile: test_data_cpy.copy() for qtile in self.quantile_levels}\n",
    "        for index, target_ in enumerate(self.cd_axis_targets):\n",
    "            feature_cols = self.sub_predictors[target_][\"features\"]\n",
    "            other_cols = [col for col in feature_cols if self.target_prefix not in col]\n",
    "\n",
    "            # if self.verbose != -1:\n",
    "            print(\n",
    "                \"\\t\\ttarget: \",\n",
    "                target_,\n",
    "                \" x_data_cols:\",\n",
    "                feature_cols[:6],\n",
    "                \" other_cols :\",\n",
    "                other_cols[:6],\n",
    "            )\n",
    "            for qtile in self.quantile_levels:\n",
    "                pred = self.sub_predictors[target_][\"predictors\"][qtile].predict(\n",
    "                    data[0.5][feature_cols]\n",
    "                )\n",
    "                data[qtile][target_] = pred\n",
    "                data[qtile] = data[qtile].sort_index(axis=1)\n",
    "\n",
    "        data = {\n",
    "            qtile: data[qtile][self.cd_axis_targets].to_numpy() for qtile in data.keys()\n",
    "        }\n",
    "        return data\n",
    "\n",
    "\n",
    "class StrategyAG(BaseStrategy):\n",
    "    model_type = \"AG\"\n",
    "    excluded_models = [\"NN_TORCH\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        day_ahead: int,\n",
    "        quantile_levels: list,\n",
    "        cd_axis_targets: list,\n",
    "        path: str,\n",
    "        is_auto_reg: bool,\n",
    "        # verbose: int = -1,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            day_ahead=day_ahead,\n",
    "            quantile_levels=quantile_levels,\n",
    "            cd_axis_targets=cd_axis_targets,\n",
    "            path=path,\n",
    "            is_auto_reg=is_auto_reg,\n",
    "            # verbose=verbose,\n",
    "        )\n",
    "        self.included_model_types = [\"GBM\"]\n",
    "\n",
    "    def _fit(self, train_data: pd.DataFrame, **kwargs) -> None:\n",
    "        self.target_feature_dtypes = dict(train_data.dtypes)\n",
    "\n",
    "        # using lambda function to identify dropping cols, rather than using a if condition inside the for loop\n",
    "        drop_cols_func = lambda target_index: self.cd_axis_targets\n",
    "        if self.autoregressive_predictions:\n",
    "            drop_cols_func = lambda target_index: self.cd_axis_targets[target_index:]\n",
    "\n",
    "        for index, target_ in enumerate(self.cd_axis_targets):\n",
    "            x_data = train_data.drop(drop_cols_func(index), axis=1)\n",
    "            y_data = train_data[[target_]]\n",
    "\n",
    "            # if self.verbose != -1:\n",
    "            print(\n",
    "                \"\\t\\ttarget: \",\n",
    "                target_,\n",
    "                \" x_data_cols:\",\n",
    "                [col for col in x_data.columns if self.target_prefix in col][:6],\n",
    "                \" y_data_cols:\",\n",
    "                list(y_data.columns),\n",
    "            )\n",
    "\n",
    "            path_i = self.path + f\"day{self.day_ahead}_{target_}\"\n",
    "\n",
    "            sub_predictor = TabularPredictor(\n",
    "                label=target_,\n",
    "                problem_type=self.objective,\n",
    "                path=path_i,\n",
    "                quantile_levels=self.quantile_levels,\n",
    "                verbosity=1,\n",
    "            )\n",
    "\n",
    "            cols_to_consider = [target_] + list(x_data.columns)\n",
    "\n",
    "            sub_predictor.fit(\n",
    "                train_data=train_data[cols_to_consider],\n",
    "                tuning_data=None,\n",
    "                excluded_model_types=self.excluded_models,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "            self.sub_predictors[target_] = {\n",
    "                \"predictors\": sub_predictor,\n",
    "                \"targets\": list(y_data.columns),\n",
    "                \"features\": list(x_data.columns),\n",
    "            }\n",
    "\n",
    "    def _predict(self, test_data: pd.Series) -> Dict[float, List[npt.NDArray]]:\n",
    "\n",
    "        # making sure dtypes are the same as in training, and filtering out the target columns from the dtypes dict since in test data its not there.\n",
    "        needed_dtypes = {\n",
    "            col: col_dtype\n",
    "            for col, col_dtype in self.target_feature_dtypes.items()\n",
    "            if \"_tgt\" not in col\n",
    "        }\n",
    "        test_data_cpy = (\n",
    "            test_data.to_frame().T.reset_index(drop=True).copy().astype(needed_dtypes)\n",
    "        )\n",
    "\n",
    "        data = {qtile: test_data_cpy.copy() for qtile in self.quantile_levels}\n",
    "        for index, target_ in enumerate(self.cd_axis_targets):\n",
    "            feature_cols = self.sub_predictors[target_][\"features\"]\n",
    "            other_cols = [col for col in feature_cols if self.target_prefix not in col]\n",
    "\n",
    "            # if self.verbose != -1:\n",
    "            print(\n",
    "                \"\\t\\ttarget: \",\n",
    "                target_,\n",
    "                \" x_data_cols:\",\n",
    "                feature_cols[:6],\n",
    "                \" other_cols :\",\n",
    "                other_cols[:6],\n",
    "            )\n",
    "            pred = self.sub_predictors[target_][\"predictors\"].predict(\n",
    "                data[0.5][feature_cols]\n",
    "            )\n",
    "            for index, qtile in enumerate(self.quantile_levels):\n",
    "\n",
    "                data[qtile][target_] = pred.iloc[:, index]\n",
    "                data[qtile] = data[qtile].sort_index(axis=1)\n",
    "\n",
    "        data = {\n",
    "            qtile: data[qtile][self.cd_axis_targets].to_numpy() for qtile in data.keys()\n",
    "        }\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a04c33bf-f3d7-418b-88ca-44c6047ae89b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from phgml.models.base_model import BaseModel\n",
    "# from phgml.models.model_strategy import BaseStrategy\n",
    "# from phgml.models.model_strategy import StrategyLGBM, StrategyAG, StrategyLGBMFarField\n",
    "from mlflow import MlflowClient\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cloudpickle\n",
    "from sys import version_info\n",
    "import pickle\n",
    "import os\n",
    "import shutil\n",
    "from typing import Optional, Tuple, Union, List, Dict, Any, Callable\n",
    "import numpy.typing as npt\n",
    "import re\n",
    "\n",
    "__all__ = [\"ModelWrapper\", \"ModelWrapperMlflowModel\", \"ModelWrapperFarField\"]\n",
    "\n",
    "PYTHON_VERSION = \"{major}.{minor}.{micro}\".format(\n",
    "    major=version_info.major, minor=version_info.minor, micro=version_info.micro\n",
    ")\n",
    "\n",
    "conda_env = {\n",
    "    \"channels\": [\"defaults\"],\n",
    "    \"dependencies\": [\n",
    "        \"python={}\".format(PYTHON_VERSION),\n",
    "        \"pip\",\n",
    "        {\n",
    "            \"pip\": [\n",
    "                \"mlflow\",\n",
    "                \"lightgbm\",\n",
    "                \"cloudpickle=={}\".format(cloudpickle.__version__),\n",
    "            ],\n",
    "        },\n",
    "    ],\n",
    "    \"name\": \"model_wrapper_env\",\n",
    "}\n",
    "\n",
    "\n",
    "class ModelWrapper(BaseModel):\n",
    "    \"\"\"Custom class which wraps a model type to generate\n",
    "    predictions in a timeseries format.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        cd_axis_max_lags: int,\n",
    "        static_cols: List[str],\n",
    "        model_strategy: BaseStrategy,\n",
    "        is_auto_reg: bool = False,\n",
    "        is_ca3_training: bool = True,\n",
    "        prediction_horizon: int = 28,\n",
    "        lag_numbers: List[int] = [1, 7, 14, 28],\n",
    "        quantiles: List[float] = [0.5],\n",
    "        mlflow_run_id: Optional[str] = None,\n",
    "        hotel_id: Optional[str] = None,\n",
    "        version: Optional[Union[str, int]] = None,\n",
    "        stage: Optional[str] = None,\n",
    "        target_type: str = \"REVENUE\",\n",
    "        exclude_pms: bool = False,\n",
    "        save_models: bool = True,\n",
    "        local_root_dir: Optional[str] = None,\n",
    "        model_type: str = \"MODELWRAPPER\",\n",
    "        model_name_prefix: Optional[str] = None,\n",
    "        meta_data: Dict[str, Any] = {},\n",
    "        n_cd_lags: Optional[int] = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            model_type=model_strategy.model_type,\n",
    "            prediction_horizon=prediction_horizon,\n",
    "            lag_numbers=lag_numbers,\n",
    "            quantiles=quantiles,\n",
    "            mlflow_run_id=mlflow_run_id,\n",
    "            hotel_id=hotel_id,\n",
    "            version=version,\n",
    "            stage=stage,\n",
    "            target_type=target_type,\n",
    "            exclude_pms=exclude_pms,\n",
    "            save_models=save_models,\n",
    "            local_root_dir=local_root_dir,\n",
    "            model_name_prefix=model_name_prefix,\n",
    "            meta_data=meta_data,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.quantile_levels.sort()\n",
    "        self.cd_axis_max_lags = cd_axis_max_lags\n",
    "        self.sd_axis_lag_prefix = \"lag\"\n",
    "        self.static_cols = static_cols\n",
    "        self.n_cd_lags = n_cd_lags\n",
    "        self.target_suffix = \"_tgt\"\n",
    "        self.is_auto_reg = is_auto_reg\n",
    "\n",
    "        self.model_strategy = model_strategy\n",
    "        self.model_type = self.model_strategy.model_type\n",
    "        self.all_cd_cols = [\n",
    "            f\"{self.target_prefix}{i}\" for i in range(self.cd_axis_max_lags + 1)\n",
    "        ]\n",
    "        self.is_ca3_training = is_ca3_training\n",
    "\n",
    "        # initializing targets variables\n",
    "        self.target_cols: Dict[int, List[str]] = {}\n",
    "\n",
    "        # initializing feature variables\n",
    "        self.feature_cols: Dict[int, List[str]] = {}\n",
    "\n",
    "        self.envs = [\"dev\", \"qa\", \"prod\"]\n",
    "\n",
    "        if 0.5 not in self.quantile_levels:\n",
    "            raise ValueError(\n",
    "                \"median quantile (0.5) is not included in the quantile_levels. please ensure that its included\"\n",
    "            )\n",
    "\n",
    "    def save_model(self) -> None:\n",
    "        \"\"\"Saves the models in the local directory, which will then be logged as artifacts in MLflow\"\"\"\n",
    "        if os.path.exists(self.local_root):\n",
    "            self.clean()\n",
    "\n",
    "        os.makedirs(self.local_dir)\n",
    "        with open(self.local_path, \"wb\") as pkl_file:\n",
    "            pickle.dump(obj=self, file=pkl_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def change_current_env_tags(self, incoming_tags: Dict[str, str]):\n",
    "\n",
    "        env_model_tag_keys = set([f\"model_stage_{env}\" for env in self.envs])\n",
    "        incoming_tags_keys = set(incoming_tags.keys())\n",
    "\n",
    "        tags_detected = env_model_tag_keys.intersection(incoming_tags_keys)\n",
    "\n",
    "        if len(tags_detected) > 0:\n",
    "            client = MlflowClient()\n",
    "            all_registered_models_info = client.search_model_versions(\n",
    "                f\"name ='{self.get_model_name()}'\"\n",
    "            )\n",
    "            # sorting the model meta data list by version number of the considered model name in descending order\n",
    "            sorted_model_versions = sorted(\n",
    "                all_registered_models_info, key=lambda x: int(x.version), reverse=True\n",
    "            )\n",
    "\n",
    "            for version_meta in sorted_model_versions:\n",
    "                for env_tag in tags_detected:\n",
    "\n",
    "                    if (incoming_tags[env_tag] == \"yes\") and (\n",
    "                        version_meta.tags.get(env_tag) == \"yes\"\n",
    "                    ):\n",
    "                        client.set_model_version_tag(\n",
    "                            name=self.get_model_name(),\n",
    "                            version=str(version_meta.version),\n",
    "                            key=env_tag,\n",
    "                            value=\"no\",\n",
    "                        )\n",
    "\n",
    "    def log_models(self) -> None:\n",
    "        \"\"\"Carries out the mlflow model registry procedures\"\"\"\n",
    "        print(\"Starting model logging\")\n",
    "        self.save_model()\n",
    "\n",
    "        modelpath = self.get_model_log_path()\n",
    "        print(\"Logging model\")\n",
    "        mlflow.pyfunc.log_model(\n",
    "            artifact_path=self.get_model_log_path(),\n",
    "            python_model=ModelWrapperMlflowModel(),\n",
    "            artifacts=self.artifacts,\n",
    "            conda_env=conda_env,\n",
    "        )\n",
    "\n",
    "        # enforcing lower case for env based string keys and values\n",
    "        decap_meta_data = {}\n",
    "        for key, value in self.meta_data.items():\n",
    "            env_str_match = re.findall(pattern=f\"({'|'.join(self.envs)})\", string=key)\n",
    "            if len(env_str_match) > 0:\n",
    "                decap_meta_data[key.lower()] = (\n",
    "                    value.lower() if isinstance(value, str) else value\n",
    "                )\n",
    "            else:\n",
    "                decap_meta_data[key] = value\n",
    "\n",
    "        self.meta_data = decap_meta_data\n",
    "\n",
    "        self.change_current_env_tags(self.meta_data)\n",
    "\n",
    "        print(\"Registering model\")\n",
    "        result = mlflow.register_model(\n",
    "            self.get_model_register_path(),\n",
    "            self.get_model_name(),\n",
    "            tags=self.meta_data,\n",
    "        )\n",
    "\n",
    "    def clean(self) -> None:\n",
    "        if os.path.exists(self.local_root):\n",
    "            shutil.rmtree(self.local_root)\n",
    "\n",
    "    def load_pyfunc_model(\n",
    "        self, dst_path: Optional[str] = None, tag: Optional[Union[str, int]] = None\n",
    "    ) -> mlflow.pyfunc.PyFuncModel:\n",
    "        \"\"\"Load and return the pyfunc model from the MLFlow model repository\n",
    "\n",
    "        Args:\n",
    "            dst_path (str, optional): Destination path to save the loaded model.\n",
    "                                      If not provided the files will be saved in the local_root path.\n",
    "                                      Defaults to None.\n",
    "            tag (str, optional): Tag to specify the version or model stage to be loaded.\n",
    "                                 If not provided the latest model version will be loaded.\n",
    "                                Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            mlflow.pyfunc.model : pyfunc model\n",
    "        \"\"\"\n",
    "        # self.local_dir = dst_path\n",
    "        print(f\"Loading model {self.get_model_uri()}\")\n",
    "\n",
    "        if dst_path is not None:\n",
    "            self.local_root = dst_path\n",
    "\n",
    "        if os.path.exists(self.local_root):\n",
    "            self.clean()\n",
    "\n",
    "        os.mkdir(self.local_root)\n",
    "\n",
    "        model = mlflow.pyfunc.load_model(\n",
    "            self.get_model_uri(tag=tag), dst_path=self.local_root\n",
    "        )\n",
    "\n",
    "        self.run_id = model._model_meta.run_id\n",
    "\n",
    "        # following is a bit of a round about way to set local_dir\n",
    "        # having the run id in the directory name is a bit troublesome as the run id is not available to us when we create the autogluon object\n",
    "        # TODO make sure to remove the run id from the local_dir and include either or both task_type/exclude_pms\n",
    "        # TODO make sure to set the local_dir consistently for both training and inference tasks\n",
    "        # self.local_dir = \"/ag_models/\"\n",
    "\n",
    "        # if self.exclude_pms:\n",
    "        #     self.local_dir = f\"ag_models_{self.hotel_id}_{self.run_id}/\"\n",
    "\n",
    "        # os.rename(\"artifacts\",self.local_dir)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def get_filtered_data(\n",
    "        self, data: pd.DataFrame, day_ahead: int\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "        target_columns = self.all_cd_cols[:day_ahead]\n",
    "        target_columns = list(\n",
    "            map(lambda target: target + self.target_suffix, target_columns)\n",
    "        )\n",
    "\n",
    "        # original target order\n",
    "        target_columns_orig = target_columns.copy()\n",
    "        target_columns.reverse()\n",
    "\n",
    "        cd_axis_lag_columns = self.all_cd_cols[day_ahead:]\n",
    "        if self.n_cd_lags != None:\n",
    "            cd_axis_lag_columns = cd_axis_lag_columns[: self.n_cd_lags]\n",
    "\n",
    "        sd_axis_lag_columns = [\n",
    "            f\"{self.sd_axis_lag_prefix}{SD_lag}\"\n",
    "            for SD_lag in self.lag_numbers\n",
    "            if SD_lag > day_ahead\n",
    "        ]\n",
    "\n",
    "        # assigning target and feature variables corresponding to the particular day ahead. This will be retrieved through the attributes in the inference phase.\n",
    "        self.target_cols[day_ahead] = target_columns\n",
    "        self.feature_cols[day_ahead] = (\n",
    "            cd_axis_lag_columns + sd_axis_lag_columns + self.static_cols\n",
    "        )\n",
    "\n",
    "        if self.is_ca3_training:\n",
    "            # Condition helps us get the specific entry for the cancellation day index\n",
    "            condition = data[\"forecast_index\"] == (day_ahead - 1)\n",
    "            filt_data = data[condition].copy()\n",
    "        else:\n",
    "            filt_data = data.copy()\n",
    "\n",
    "        x_data = filt_data[self.feature_cols[day_ahead]]\n",
    "        y_data = filt_data[self.target_cols[day_ahead]]\n",
    "\n",
    "        return (\n",
    "            x_data,\n",
    "            y_data,\n",
    "            filt_data[target_columns_orig + self.feature_cols[day_ahead]],\n",
    "        )\n",
    "\n",
    "    def train_inner(self, train_data: pd.DataFrame, day_ahead: int):\n",
    "        x_train, y_train, xy_train = self.get_filtered_data(\n",
    "            data=train_data, day_ahead=day_ahead\n",
    "        )\n",
    "\n",
    "        reg_obj = self.model_strategy(\n",
    "            quantile_levels=self.quantile_levels,\n",
    "            day_ahead=day_ahead,\n",
    "            cd_axis_targets=self.target_cols[day_ahead],\n",
    "            path=self.local_dir,\n",
    "            is_auto_reg=self.is_auto_reg,\n",
    "        )  # type: ignore\n",
    "        reg_obj._fit(xy_train)\n",
    "\n",
    "        # self.models[day_ahead] = reg_obj\n",
    "\n",
    "        return day_ahead, reg_obj\n",
    "    \n",
    "    def predict_inner(self, train_data: pd.DataFrame, day_ahead: int):\n",
    "        test_idx = day_ahead - 1\n",
    "\n",
    "        try:\n",
    "            needed_test_data = test_data[test_data.day_ahead == day_ahead].iloc[0]\n",
    "        except IndexError as e:\n",
    "            days_str = \"days\" if day_ahead > 1 else \"day\"\n",
    "\n",
    "            print(f\"Error when predicting {day_ahead} {days_str} ahead\")\n",
    "            print(f\"Encountered error {e}\")\n",
    "            print(\"Skipping this row\")\n",
    "            continue\n",
    "\n",
    "        x_test = needed_test_data[self.feature_cols[day_ahead]]\n",
    "        y_test = needed_test_data[self.target_cols[day_ahead]]\n",
    "\n",
    "        predictor = self.models[day_ahead]\n",
    "        y_pred_dct = predictor._predict(x_test)\n",
    "\n",
    "        return y_pred_dct\n",
    "\n",
    "    def train(self, train_data: pd.DataFrame, n_threads: int) -> None:\n",
    "        \"\"\"\n",
    "        trains models for each day ahead quantile predictions and  relevant\n",
    "        to the specified prediction_horizon value and the specific quantile\n",
    "        levels.\n",
    "\n",
    "        parameters:\n",
    "            train_data = training data with the booking pace lags, stay date lags or\n",
    "                    other features such as date features.\n",
    "\n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        with ThreadPoolExecutor(max_workers=n_threads) as executor:\n",
    "            future_to_target = {executor.submit(\n",
    "                self.train_inner, train_data, day_ahead): day_ahead for day_ahead in range(1, self.prediction_horizon + 1)}\n",
    "            \n",
    "            for future in as_completed(future_to_target):\n",
    "                try:\n",
    "                    day_ahead, reg_obj = future.result()\n",
    "                except Exception as exc:\n",
    "                    print(exc)\n",
    "                else:\n",
    "                    self.models[day_ahead] = reg_obj\n",
    "\n",
    "\n",
    "        # for day_ahead in range(1, self.prediction_horizon + 1):\n",
    "        #     print(\"\\tday ahead: \", day_ahead)\n",
    "        #     x_train, y_train, xy_train = self.get_filtered_data(\n",
    "        #         data=train_data, day_ahead=day_ahead\n",
    "        #     )\n",
    "\n",
    "        #     reg_obj = self.model_strategy(\n",
    "        #         quantile_levels=self.quantile_levels,\n",
    "        #         day_ahead=day_ahead,\n",
    "        #         cd_axis_targets=self.target_cols[day_ahead],\n",
    "        #         path=self.local_dir,\n",
    "        #         is_auto_reg=self.is_auto_reg,\n",
    "        #     )  # type: ignore\n",
    "        #     reg_obj._fit(xy_train)\n",
    "\n",
    "        #     self.models[day_ahead] = reg_obj\n",
    "\n",
    "        # if self.do_save_models:\n",
    "        #     self.log_models()\n",
    "\n",
    "    def predict(\n",
    "        self, test_data: pd.DataFrame\n",
    "    ) -> Dict[Union[str, float], Union[List[npt.NDArray], List[pd.Series]]]:\n",
    "        \"\"\"generating quantile predictions for the test data provided. test_data\n",
    "        should be provided which aligns with the prediction_horizon. If\n",
    "        test_data has less rows than the prediction_horizon, then the length\n",
    "        of the test_data will be considered as the prediction horizon.\n",
    "\n",
    "        eg: if prediction_horizon= 28, ideally test_data should have 28 rows\n",
    "            which are relevant for 28 stay dates.\n",
    "\n",
    "        parameters:\n",
    "            test_data = test data which aligns with the prediction horizon.\n",
    "                        rows of test_data <= prediction_horizon.\n",
    "\n",
    "        Returns: Lists with actual values and corresponding predicted values along the booking axis leading upto the\n",
    "          relevant stay date ahead\n",
    "        \"\"\"\n",
    "        output_pred: Dict[\n",
    "            Union[str, float], Union[List[npt.NDArray], List[pd.Series]]\n",
    "        ] = {}\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=n_threads) as executor:\n",
    "            future_to_target = {executor.submit(\n",
    "                self.predict_inner, test_data, day_ahead): day_ahead for day_ahead in range(1, self.prediction_horizon + 1)}\n",
    "            \n",
    "            for future in as_completed(future_to_target):\n",
    "                try:\n",
    "                    y_pred_dct = future.result()\n",
    "                except Exception as exc:\n",
    "                    print(exc)\n",
    "                else:\n",
    "                    # self.models[day_ahead] = reg_obj\n",
    "                    if output_pred.get(\"y_test\") == None:\n",
    "                        output_pred[\"y_test\"] = [y_test]\n",
    "                    else:\n",
    "                        output_pred[\"y_test\"] += [y_test]\n",
    "\n",
    "                    for qtile in self.quantile_levels:\n",
    "                        if output_pred.get(qtile) == None:\n",
    "                            output_pred[qtile] = [y_pred_dct[qtile][0]]\n",
    "                        else:\n",
    "                            output_pred[qtile] += [y_pred_dct[qtile][0]]\n",
    "\n",
    "        return output_pred\n",
    "\n",
    "        # for day_ahead in range(1, self.prediction_horizon + 1):\n",
    "        #     test_idx = day_ahead - 1\n",
    "\n",
    "        #     try:\n",
    "        #         needed_test_data = test_data[test_data.day_ahead == day_ahead].iloc[0]\n",
    "        #     except IndexError as e:\n",
    "        #         days_str = \"days\" if day_ahead > 1 else \"day\"\n",
    "\n",
    "        #         print(f\"Error when predicting {day_ahead} {days_str} ahead\")\n",
    "        #         print(f\"Encountered error {e}\")\n",
    "        #         print(\"Skipping this row\")\n",
    "        #         continue\n",
    "\n",
    "        #     x_test = needed_test_data[self.feature_cols[day_ahead]]\n",
    "        #     y_test = needed_test_data[self.target_cols[day_ahead]]\n",
    "\n",
    "        #     predictor = self.models[day_ahead]\n",
    "        #     y_pred_dct = predictor._predict(x_test)\n",
    "\n",
    "        #     if output_pred.get(\"y_test\") == None:\n",
    "        #         output_pred[\"y_test\"] = [y_test]\n",
    "        #     else:\n",
    "        #         output_pred[\"y_test\"] += [y_test]\n",
    "\n",
    "        #     for qtile in self.quantile_levels:\n",
    "        #         if output_pred.get(qtile) == None:\n",
    "        #             output_pred[qtile] = [y_pred_dct[qtile][0]]\n",
    "        #         else:\n",
    "        #             output_pred[qtile] += [y_pred_dct[qtile][0]]\n",
    "\n",
    "        # return output_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a7931c2-83fa-42e4-889f-8654b2c7e499",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train_wrapper(\n",
    "    target_type: str,\n",
    "    ml_experiment_id: str,\n",
    "    exclude_pms: bool,\n",
    "    calc_uncertainty: bool,\n",
    "    hotel_config_provider: ForecastingHotelConfigProvider,\n",
    "    processing_timestamp: datetime,\n",
    "    save_models: bool,\n",
    "    save_metrics: bool,\n",
    "    lag_numbers: List[int],\n",
    "    model_tags: dict = None,\n",
    "    n_threads: int = 4,\n",
    ") -> Callable:\n",
    "    def train_data_models(df):\n",
    "        static_cols_ = [\n",
    "            \"year\",\n",
    "            \"quarter_of_year\",\n",
    "            \"month_of_year\",\n",
    "            \"week_of_year\",\n",
    "            \"day_of_year\",\n",
    "            \"month_of_quarter\",\n",
    "            \"week_of_quarter\",\n",
    "            \"day_of_quarter\",\n",
    "            \"week_of_month\",\n",
    "            \"day_of_month\",\n",
    "            \"holiday\",\n",
    "            \"day_of_week_0\",\n",
    "            \"day_of_week_1\",\n",
    "            \"day_of_week_2\",\n",
    "            \"day_of_week_3\",\n",
    "            \"day_of_week_4\",\n",
    "            \"day_of_week_5\",\n",
    "            \"day_of_week_6\",\n",
    "        ]\n",
    "\n",
    "        logger = get_dbx_logger(\"PHGML\")\n",
    "\n",
    "        trainer = None\n",
    "        hotel_id = df[\"HotelID\"].iloc[0]\n",
    "        hotel_config = hotel_config_provider.get_config(hotel_id)\n",
    "        model_type = hotel_config.training_model_name\n",
    "\n",
    "        max_lead_window = 100\n",
    "\n",
    "        if target_type == \"REVENUE\":\n",
    "            col_prefix = \"RV\"\n",
    "        elif target_type == \"ROOMS\":\n",
    "            col_prefix = \"RM\"\n",
    "\n",
    "        df = remove_padded_cols(\n",
    "            df, hotel_config.lead_window, max_lead_window, col_prefix\n",
    "        )\n",
    "\n",
    "        test_partition_end = df[\"_StayDates\"].max()\n",
    "        test_partition_start = test_partition_end - pd.Timedelta(\n",
    "            hotel_config.training_length, \"D\"\n",
    "        )\n",
    "        metadata_dict = {\n",
    "            \"last_trained_date\": str(\n",
    "                test_partition_start - pd.Timedelta(hotel_config.training_length, \"D\")\n",
    "            ),\n",
    "            \"training_length\": hotel_config.training_length,\n",
    "            \"inference_length\": hotel_config.inference_length,\n",
    "        }\n",
    "        metadata_dict.update(model_tags)\n",
    "\n",
    "        logger.debug(f\"{hotel_id}:Filter train data\")\n",
    "        dftrain = filter_train_data(df, test_partition_start)\n",
    "\n",
    "        logger.debug(f\"{hotel_id}:Filter test data\")\n",
    "        dftest = filter_test_data(\n",
    "            df,\n",
    "            test_partition_start=test_partition_start,\n",
    "            test_partition_end=test_partition_end,\n",
    "        )\n",
    "        dftest[\"day_ahead\"] = (dftest[\"_StayDates\"] - test_partition_start).dt.days\n",
    "        dftest = dftest[dftest.forecast_index == (dftest.day_ahead - 1)]\n",
    "\n",
    "        model_version = 1\n",
    "        model_stage = \"Staging\"\n",
    "        model_name = None\n",
    "\n",
    "        pms = \"PMS\"\n",
    "        if exclude_pms:\n",
    "            pms = \"NOPMS\"\n",
    "\n",
    "        with mlflow.start_run(\n",
    "            experiment_id=ml_experiment_id,\n",
    "            run_name=f\"{model_type}-{target_type}-{pms}-{hotel_id}-{hotel_config.hotel_name}\",\n",
    "        ) as run:\n",
    "            run_id = run.info.run_id\n",
    "\n",
    "            if model_type == \"AUTOGLUON\":\n",
    "                trainer = ModelWrapper(\n",
    "                    model_strategy=StrategyAG,\n",
    "                    is_auto_reg=True,\n",
    "                    prediction_horizon=hotel_config.training_length,\n",
    "                    mlflow_run_id=run_id,\n",
    "                    hotel_id=hotel_id,\n",
    "                    save_models=save_models,\n",
    "                    target_type=target_type,\n",
    "                    exclude_pms=exclude_pms,\n",
    "                    meta_data=metadata_dict,\n",
    "                    cd_axis_max_lags=99,\n",
    "                    static_cols=static_cols_,\n",
    "                    quantiles=[0.1, 0.5, 0.9],\n",
    "                )\n",
    "            elif model_type == \"LIGHTGBM\":\n",
    "                trainer = ModelWrapper(\n",
    "                    model_strategy=StrategyLGBM,\n",
    "                    prediction_horizon=hotel_config.training_length,\n",
    "                    mlflow_run_id=run_id,\n",
    "                    hotel_id=hotel_id,\n",
    "                    save_models=save_models,\n",
    "                    target_type=target_type,\n",
    "                    exclude_pms=exclude_pms,\n",
    "                    meta_data=metadata_dict,\n",
    "                    cd_axis_max_lags=99,\n",
    "                    static_cols=static_cols_,\n",
    "                    quantiles=[0.1, 0.5, 0.9],\n",
    "                )\n",
    "\n",
    "            model_name = trainer.get_model_name()\n",
    "            output_df = pd.DataFrame()\n",
    "            try:\n",
    "                logger.info(f\"{hotel_id}:Training model\")\n",
    "                trainer.train(dftrain, n_threads)\n",
    "\n",
    "                logger.info(f\"{hotel_id}:Completed training\")\n",
    "                logger.info(f\"{hotel_id}:Start prediction\")\n",
    "\n",
    "                output_dct = trainer.predict(dftest)\n",
    "                y_pred_lst, y_test_lst, y_upper_lst, y_lower_lst = (\n",
    "                    output_dct[0.5],\n",
    "                    output_dct[\"y_test\"],\n",
    "                    output_dct[0.9],\n",
    "                    output_dct[0.1],\n",
    "                )\n",
    "\n",
    "                predicted_stays = [i[-1] for i in y_pred_lst]\n",
    "                observed_stays = [i.values[-1] for i in y_test_lst]\n",
    "                if calc_uncertainty:\n",
    "                    upper_stays = [i[-1] for i in y_upper_lst]\n",
    "                    lower_stays = [i[-1] for i in y_lower_lst]\n",
    "\n",
    "                if save_metrics:\n",
    "                    log_metrics_stays(observed_stays, predicted_stays, trainer)\n",
    "\n",
    "                report_metrics_stays(observed_stays, predicted_stays)\n",
    "\n",
    "                del dftrain\n",
    "                del dftest\n",
    "\n",
    "                output_df = pd.DataFrame(\n",
    "                    {\n",
    "                        \"HotelID\": [hotel_id],\n",
    "                        \"run_id\": [run_id],\n",
    "                        \"model_version\": [model_version],\n",
    "                        \"timestamp\": [processing_timestamp],\n",
    "                        \"pms_sync_off\": [exclude_pms],\n",
    "                        \"model_name\": [model_name],\n",
    "                        \"status\": \"complete\",\n",
    "                        \"message\": f\"Successfully trained {hotel_id}\",\n",
    "                    }\n",
    "                )\n",
    "            except Exception as e:\n",
    "                empty = pd.DataFrame(\n",
    "                    {\n",
    "                        \"HotelID\": [hotel_id],\n",
    "                        \"run_id\": [run_id],\n",
    "                        \"model_version\": [model_version],\n",
    "                        \"timestamp\": [pd.Timestamp(\"1900-01-01\")],\n",
    "                        \"pms_sync_off\": [exclude_pms],\n",
    "                        \"model_name\": [model_name],\n",
    "                        \"status\": \"incomplete\",\n",
    "                        \"message\": str(e),\n",
    "                    }\n",
    "                )\n",
    "                return empty\n",
    "\n",
    "            finally:\n",
    "                if (model_type == \"AUTOGLUON\") and (trainer is not None):\n",
    "                    trainer.clean()\n",
    "\n",
    "        return output_df\n",
    "\n",
    "    return train_data_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc50dfe1-054a-4eae-b626-97b1885dbb34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[19]: (False, False)</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[19]: (False, False)</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "SAVE_MODEL, SAVE_METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7898952-ce14-496c-bc4c-a0655a983f0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Group the data by hotel id and execute the trainings in parallel\n",
    "logger.info(\"Starting parallel training\")\n",
    "\n",
    "output_df = df.groupby(\"HotelID\").applyInPandas(\n",
    "    train_wrapper(\n",
    "        target_type=TARGET_TYPE,\n",
    "        ml_experiment_id=ML_EXPERIMENT_ID,\n",
    "        exclude_pms=WITHOUT_PMS,\n",
    "        calc_uncertainty=CALC_UNCERTAINTY,\n",
    "        hotel_config_provider=forecasting_config_provider,\n",
    "        processing_timestamp=processing_timestamp,\n",
    "        save_models=SAVE_MODEL,\n",
    "        save_metrics=SAVE_METRICS,\n",
    "        lag_numbers=LAG_NUMBERS,\n",
    "        model_tags=MODEL_TAGS_DCT\n",
    "    ),\n",
    "    schema,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a0e475e-20ab-4cf9-8057-75646f3b0736",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Model training time: 16.982523202896118\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Model training time: 16.982523202896118\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = time.time()\n",
    "output_df = output_df.toPandas()\n",
    "\n",
    "\n",
    "elapsed_time = time.perf_counter() - start\n",
    "print(\"Model training time: \" + time.strftime(\"%H:%M:%S.{}\".format(str(elapsed_time % 1)[2:])[:15], time.gmtime(elapsed_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7925a669-1071-4be0-adcf-5fc3fcb2704f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf4934c4-20e5-42fb-9f86-1cdeb4f5f052",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for index, row in output_df.iterrows():\n",
    "    if row.status == \"complete\":\n",
    "        logger.info(f\"{row.message}\")\n",
    "    else:\n",
    "        logger.error(\n",
    "            f\"Error encountered when training hotel {row.HotelID}: {row.message}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e1bb8a2-d00f-4384-96d7-6c9592f2a037",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "client = MlflowClient()\n",
    "completed = output_df[output_df[\"status\"]==\"complete\"]\n",
    "\n",
    "outputs_list = []\n",
    "for n,g in completed.groupby([\"HotelID\",\"model_name\"]):\n",
    "    hotel_id = n[0]\n",
    "    model_name = n[1]\n",
    "    hotel_config = forecasting_config_provider.get_config(hotel_id)\n",
    "\n",
    "    mv = client.get_latest_versions(name=model_name)[0]\n",
    "    print(mv)\n",
    "    arts = client.list_artifacts(mv.run_id,path=f\"forecasting/{hotel_id}/models/{model_name}/artifacts\")\n",
    "    \n",
    "    outputs_list.append({\"hotel_id\":hotel_id,\n",
    "                         \"model_name\":model_name,\n",
    "                         \"creation_time\":datetime.datetime.fromtimestamp(mv.creation_timestamp/1e3),\n",
    "                         \"last_update\":datetime.datetime.fromtimestamp(mv.last_updated_timestamp/1e3),\n",
    "                         \"version\":mv.version,\n",
    "                         \"target\":TARGET_TYPE,\n",
    "                         \"exclude_pms\":WITHOUT_PMS,\n",
    "                         \"config_train_length\":hotel_config.training_length,\n",
    "                         \"config_infer_length\":hotel_config.inference_length,\n",
    "                         \"num_model_steps\":len(arts)-1})\n",
    "    \n",
    "    print(f\"Hotel: {hotel_id} target_type:{TARGET_TYPE} exclude_pms:{WITHOUT_PMS} : {len(arts)-1}\")\n",
    "\n",
    "completed_df = pd.DataFrame(outputs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79288c01-4ced-43d7-9400-926a5237bbe8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "completed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7485c5c8-4b24-4891-8bd4-a8036ed91e08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logger.info(\"Model training completed.\")\n",
    "\n",
    "# elapsed_time = time.perf_counter() - start_time\n",
    "# logger.info(f\"Time elapsed {elapsed_time}\")\n",
    "# logger.info(f\"Time elapsed in minutes {elapsed_time/60}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "forecast_training_modified",
   "widgets": {
    "env_stage": {
     "currentValue": "dev",
     "nuid": "884805fe-5fa8-4adb-8645-f8c6e2f159f0",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dev",
      "label": "Pipeline stage",
      "name": "env_stage",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "dev",
        "prod"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "dev",
      "label": "Pipeline stage",
      "name": "env_stage",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "dev",
        "prod"
       ]
      }
     }
    },
    "exclude_pms": {
     "currentValue": "False",
     "nuid": "cd78b0ea-ba81-47d8-aec5-cd65c2f11fd3",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "False",
      "label": "Exclude PMS",
      "name": "exclude_pms",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "True",
        "False"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "False",
      "label": "Exclude PMS",
      "name": "exclude_pms",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "True",
        "False"
       ]
      }
     }
    },
    "is_usd_currency": {
     "currentValue": "True",
     "nuid": "c640f9e0-3e8e-4c76-bf5e-86482bad1424",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "True",
      "label": "Use USD currency",
      "name": "is_usd_currency",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "True",
        "False"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "True",
      "label": "Use USD currency",
      "name": "is_usd_currency",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "True",
        "False"
       ]
      }
     }
    },
    "lag_numbers": {
     "currentValue": "1,7,14,28",
     "nuid": "06e293fc-9626-4847-a90f-7c5b455cfc1b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "1,7,14,28",
      "label": "Lag Numbers",
      "name": "lag_numbers",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "1,7,14,28",
      "label": "Lag Numbers",
      "name": "lag_numbers",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "model_tags": {
     "currentValue": "",
     "nuid": "9c88777a-dc04-46fa-9693-a59e3ae337bf",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Model Tags",
      "name": "model_tags",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Model Tags",
      "name": "model_tags",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "target_type": {
     "currentValue": "REVENUE",
     "nuid": "0aeb62ab-9eae-4ebc-a5f4-799d90c039b9",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "REVENUE",
      "label": "Target Type",
      "name": "target_type",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "REVENUE",
        "ROOMS"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "REVENUE",
      "label": "Target Type",
      "name": "target_type",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": null,
       "choices": [
        "REVENUE",
        "ROOMS"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}